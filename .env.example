# Environment Configuration Template for RAG-MCP
# Copy this file to .env and update with your settings

# =============================================================================
# vLLM Server Configuration
# =============================================================================

# URL of the vLLM server
# For local testing (WSL2): http://localhost:8000
# For remote GPU server: http://your-server-ip:8000
VLLM_SERVER_URL=http://localhost:8000

# Model name being served by vLLM
# This should match the model you started vLLM with
VLLM_MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.3

# Request timeout in seconds
# Increase if queries are timing out
VLLM_TIMEOUT=30

# Temperature for LLM sampling (0.0 - 1.0)
# Lower = more deterministic, Higher = more random
# Recommended: 0.1 for tool selection
VLLM_TEMPERATURE=0.1

# Maximum tokens in LLM response
# Tool selection only needs ~20-50 tokens
VLLM_MAX_TOKENS=100

# =============================================================================
# Data Paths (Optional - for future use)
# =============================================================================

# Path to tools JSON file
# TOOLS_FILE=data/tools/mcp_tools.json

# Path to queries JSON file
# QUERIES_FILE=data/queries/test_queries.json

# Path to save FAISS index
# INDEX_PATH=data/indexes/tools_v1.index

# =============================================================================
# Embedding Model Configuration (Optional - for future use)
# =============================================================================

# Embedding model for indexing
# EMBEDDING_MODEL=all-MiniLM-L6-v2

# =============================================================================
# Experiment Configuration (Optional - for future use)
# =============================================================================

# Top-k value for retrieval
# TOP_K=3

# Results output directory
# RESULTS_DIR=data/results
