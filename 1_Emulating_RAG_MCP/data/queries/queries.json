{
  "count": 208,
  "queries": [
    {
      "query_id": "google_maps_weather_data_national_parks_000",
      "query": "You are planning a 3-day camping expedition to Yosemite National Park departing from San Jose, CA. Produce a detailed, self-contained itinerary that includes: 1) the top three campgrounds in Yosemite NP that have at least three amenities (e.g., showers, potable water, Wi-Fi), are not under any active alerts, and are open during the trip; 2) the operating hours of the nearest visitor center to your primary campground; 3) turn-by-turn driving directions from San Jose, CA to the primary campground and then to the visitor center; 4) travel distances and durations for all three campgrounds and the visitor center; 5) the elevation of the primary campground; 6) the upcoming 3-day weather forecast for Yosemite National Park; and 7) the nearest grocery store or convenience store within 5 km of the primary campground for resupply. Format your output as a structured JSON with sections: \"selected_campgrounds\" (name, parkCode, amenities, distance_m, duration_s), \"primary_itinerary\" (campground_name, visitor_center_name, visitor_center_hours, directions_to_campground[], directions_to_center[]), \"campground_elevation_meters\", \"weather_forecast_3_days\" (date, high_temp, low_temp, conditions), and \"nearest_resupply\" (name, distance_m, rating).",
      "category": "three_server_combinations",
      "ground_truth_tool": "Google Maps+Weather Data+National Parks",
      "servers": [
        "Google Maps",
        "Weather Data",
        "National Parks"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "DEX Paprika",
        "FruityVice",
        "Huge Icons",
        "Math MCP",
        "Metropolitan Museum",
        "NixOS",
        "Scientific Computing",
        "Unit Converter"
      ],
      "combination_name": "Travel Planning Suite"
    },
    {
      "query_id": "google_maps_weather_data_national_parks_000",
      "query": "Hey there—I’m gearing up for a quick three-day camping getaway to Yosemite from San Jose and, to be honest, I’m feeling a bit swamped by all the options and details. I’d love to zero in on the three best campgrounds that actually have real comforts—think showers, drinking water, maybe even Wi-Fi—are definitely open on my dates and aren’t under any alerts or closures right now. \n\nOnce I’ve got that shortlist, can you help me figure out roughly how far and how long it takes to drive from San Jose to each of those spots? I’m planning to settle into one as my “base camp,” so for that primary site it’d be great to know the nearest visitor center’s hours and exactly how to get there—like turn-by-turn directions, plus the distance and travel time. Also, what’s the elevation at that main campground? \n\nSince I want to pack smart, I really need a solid three-day weather outlook for Yosemite—nothing vague, just the highs, lows and general conditions for the next few days. And, just in case I run out of snacks or cooking supplies, is there a grocery or convenience store within about five kilometers of that first campground? \n\nI can’t just wing this trip, so any real numbers or solid reference points you can dig up would be awesome—no vague guesses, please. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Google Maps+Weather Data+National Parks",
      "servers": [
        "Google Maps",
        "Weather Data",
        "National Parks"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "DEX Paprika",
        "FruityVice",
        "Huge Icons",
        "Math MCP",
        "Metropolitan Museum",
        "NixOS",
        "Scientific Computing",
        "Unit Converter"
      ],
      "combination_name": "Travel Planning Suite"
    },
    {
      "query_id": "google_maps_weather_data_national_parks_001",
      "query": "Plan a 7-day multi-park hiking and camping itinerary starting and ending in Denver, CO. You must:\n1. Geocode “Denver, CO” to get its coordinates.\n2. Search for up to 10 national parks in Colorado (CO), Utah (UT), or Wyoming (WY) that offer both hiking and camping.\n3. Retrieve detailed information for each park, including geographic coordinates.\n4. Compute driving distances and durations from Denver to each park and select the three nearest parks by driving time.\n5. For each of the three selected parks:\n   a. Fetch current alerts, visitor center hours, campgrounds and upcoming events for the next 7 days.\n   b. Reverse-geocode the park’s coordinates to find the nearest town or landmark.\n   c. Obtain the current weather and a 7-day forecast for that town.\n   d. Search within a 20 km radius of the park coordinates for hotels.\n6. If any selected park has a hazardous alert or forecast precipitation probability over 50% on its planned visit day, reorder the park sequence to minimize weather and alert risk.\n7. Build a 3-leg round-trip driving route starting and ending in Denver, visiting each park in final order. For each leg:\n   a. Compute distance and duration.\n   b. Retrieve detailed turn-by-turn driving directions.\n   c. Sample elevation at the leg’s origin and destination points.\n8. Produce a daily schedule listing: park name, visit date, park description, alert summary, visitor center hours, campground availability, events, weather summary, nearest lodging options, driving time and distance, elevation change, and route directions.\n\nOutput as a JSON object with an array of 7 day-by-day itinerary entries.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Google Maps+Weather Data+National Parks",
      "servers": [
        "Google Maps",
        "Weather Data",
        "National Parks"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Car Price Evaluator",
        "Huge Icons",
        "Movie Recommender",
        "NASA Data",
        "OSINT Intelligence",
        "Paper Search",
        "Reddit",
        "Scientific Computing"
      ],
      "combination_name": "Travel Planning Suite"
    },
    {
      "query_id": "google_maps_weather_data_national_parks_001",
      "query": "I’m trying to plan a week-long hiking and camping loop that starts and ends in Denver, and I’m hoping you can really nerd out with me on the details. I want to hit a few of the best parks in Colorado, Utah or Wyoming that have both solid trails and campgrounds, then narrow it down to the three closest ones by drive time so I’m not losing half my day on the road. From there, I’d love a day-by-day agenda for the next seven days that not only tells me which park I’m at and when, but also flags any active alerts or if there’s more than a 50% chance of rain that day (so we could switch things around if it looks dicey). \n\nOn top of that, I need to know what the visitor center hours are, where I can actually secure a campsite or catch an event, plus a quick weather snapshot each morning and night. If there’s a nearby town or landmark, I want to know about hotels in, say, a 20 km radius too—just in case I decide to splurge one night. And for each driving leg, could you give me the distance, drive time, a rough idea of elevation change, and turn-by-turn directions? I really need actual numbers backed up by real data—no hand-wavy guesses—because I’m sharing this with friends who expect concrete facts. Thanks!",
      "category": "three_server_combinations",
      "ground_truth_tool": "Google Maps+Weather Data+National Parks",
      "servers": [
        "Google Maps",
        "Weather Data",
        "National Parks"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Car Price Evaluator",
        "Huge Icons",
        "Movie Recommender",
        "NASA Data",
        "OSINT Intelligence",
        "Paper Search",
        "Reddit",
        "Scientific Computing"
      ],
      "combination_name": "Travel Planning Suite"
    },
    {
      "query_id": "hugging_face_paper_search_wikipedia_001",
      "query": "You are a research engineer tasked with identifying and validating the current state-of-the-art text classification model for the AG News dataset, and cross-checking academic performance claims. Follow these steps without asking for more information:\n\n1. Use Hugging Face:search-datasets with query=\"ag news\", limit=5 to find the official AG News dataset ID.\n2. Call Hugging Face:get-dataset-info on the returned dataset_id to confirm it has exactly 4 classes and is in English. If it does not, abort with an error.\n3. Search for pre-trained text classification models fine-tuned on AG News:\n   • Use Hugging Face:search-models with query=\"ag news\", tags=\"text-classification\", author=\"\" (empty for no filter), limit=5.\n   • From the search results, select the top three models sorted by their reported evaluation metric F1 score (assume metadata contains a key \"f1\").\n4. For each of the three model_ids, call Hugging Face:get-model-info to retrieve architecture, model size, license, and the reported F1 score in the model card. Discard any model whose license is not an OSI-approved open-source license; if fewer than two remain, expand search-models to limit=10 and repeat selection.\n5. Identify the single Hugging Face model with the highest reported F1 score – call this Model_HF and record its model_id and F1_HF.\n6. Perform an academic literature search: use Paper Search:search_arxiv with query=\"AG News classification performance\", max_results=5 and select among returned metadata the paper published in the past 3 months with the highest reported F1 (you may inspect each metadata for a \"published\" date and \"f1\" field). Record its arxiv_id and F1_arxiv.\n7. Download and read the paper:\n   • Call Paper Search:download_arxiv with paper_id=arxiv_id.\n   • Then call Paper Search:read_arxiv_paper with the same paper_id to extract full text. Parse the extracted text to confirm the F1_arxiv value.\n8. Fetch definitions of evaluation metrics:\n   • Use Wikipedia:get_summary on title=\"F1 score\" to retrieve an overview of what a micro-averaged F1 score is.\n   • Use Wikipedia:extract_key_facts on title=\"F1 score\", topic_within_article=\"calculation formula\", count=3 to get the formula and key facts about how F1 is computed from precision and recall.\n9. Decision point – compare F1_arxiv vs. F1_HF:\n   • If F1_arxiv > F1_HF + 0.05 (5 percentage points), recommend adopting the academic approach (summarize the key architecture change from the paper) and outline a plan to fine-tune Model_HF using that method.\n   • Otherwise, recommend deploying Model_HF as is and note that no recent paper outperforms it by more than 5%. Include a brief summary of both performances and link model_id with arxiv_id for traceability.\n\nProvide your final output as a JSON object with:\n  • \"selected_model\": model_id and F1_HF\n  • \"paper_reference\": arxiv_id and F1_arxiv\n  • \"metric_definition_summary\": the summary and key facts from Wikipedia\n  • \"recommendation\": clear next steps as per the decision point above.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Hugging Face+Paper Search+Wikipedia",
      "servers": [
        "Hugging Face",
        "Paper Search",
        "Wikipedia"
      ],
      "distraction_servers": [
        "BioMCP",
        "Car Price Evaluator",
        "Context7",
        "DEX Paprika",
        "Game Trends",
        "Google Maps",
        "Movie Recommender",
        "NixOS",
        "Scientific Computing",
        "Weather Data"
      ],
      "combination_name": "AI Research Hub"
    },
    {
      "query_id": "hugging_face_paper_search_wikipedia_001",
      "query": "I’m working on a project where I need to pick the very best news‐article classifier out there right now—specifically the one built for that 4-category news dataset (world, sports, business, tech). My boss wants me to find a publicly available, open-source model that has the highest F1 score, and then see if any fresh paper from the last three months has pushed the bar another 5 percentage points higher. \n\nIf a recent research write-up really beats the community model by at least 5 points in F1, I’d like to know what architectural tweak or training trick they used so I can apply it to the top model we found. If not, we’ll just roll with that open-source champion as is. Also, I need a quick, plain-English refresher on what a micro-averaged F1 score actually means and how it’s calculated—got to explain it clearly to stakeholders. \n\nCould you dig into this for me, pull together the model ID and its reported F1, track down any paper from roughly the past three months with its own F1, compare them, and then recommend next steps? Really need solid numbers and clear references so I’m not just guessing. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Hugging Face+Paper Search+Wikipedia",
      "servers": [
        "Hugging Face",
        "Paper Search",
        "Wikipedia"
      ],
      "distraction_servers": [
        "BioMCP",
        "Car Price Evaluator",
        "Context7",
        "DEX Paprika",
        "Game Trends",
        "Google Maps",
        "Movie Recommender",
        "NixOS",
        "Scientific Computing",
        "Weather Data"
      ],
      "combination_name": "AI Research Hub"
    },
    {
      "query_id": "hugging_face_paper_search_wikipedia_003",
      "query": "You are asked to identify and evaluate the best open-source German→English machine translation model on Hugging Face Hub, cross-validate its reported performance with the primary research paper on arXiv, and enrich your findings with background from Wikipedia and live demos on Hugging Face Spaces. Specifically:\n\n1. Search Hugging Face models with query=\"translation\", tags=\"translation\", author=\"Helsinki-NLP\", limit=5 to retrieve the top 5 German→English translation models.\n2. For each returned model_id, fetch detailed metadata (including published BLEU scores) via get-model-info.\n3. Compare BLEU scores and select the model with the highest BLEU. If the top BLEU < 30, flag that performance is below industry benchmark; otherwise proceed.\n4. Use the selected model’s name in a Paper Search (arXiv) query: query=\"<model_name> BLEU German English translation\", max_results=3. Choose the first result.\n5. Download the chosen arXiv paper PDF and extract its full text. From the text, identify the section discussing evaluation methodology and record the exact reported BLEU score and test set used.\n6. Search Wikipedia for \"Machine translation\" (limit=5), get the summary of the article, then extract key facts (count=3) focused on \"evaluation metrics\" within that article.\n7. Cross-validate: compare the BLEU score and methodology from the arXiv paper with the score reported on Hugging Face. Note any discrepancies in reported test sets or preprocessing steps.\n8. Search Hugging Face Spaces with query=\"<selected_model_id>\", sdk=\"gradio\", limit=3. For each returned space_id, fetch its info and record which language pairs are supported in the demo and whether real-time translation latency is mentioned.\n9. Compile a structured report in JSON with the following fields:\n   - selected_model_id\n   - reported_bleu_hf\n   - reported_bleu_paper\n   - test_set_paper\n   - hf_vs_paper_discrepancy_note\n   - wikipedia_evaluation_facts (array of 3 facts)\n   - demo_spaces (array of objects: {space_id, supported_pairs, latency_mentioned})\n\nAll steps must use only the specified tools and the given concrete values. No external APIs or additional input are allowed.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Hugging Face+Paper Search+Wikipedia",
      "servers": [
        "Hugging Face",
        "Paper Search",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "Math MCP",
        "Metropolitan Museum",
        "NASA Data",
        "National Parks",
        "OKX Exchange",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "AI Research Hub"
    },
    {
      "query_id": "hugging_face_paper_search_wikipedia_003",
      "query": "Hey, I’m working on a little side project where I need a solid German→English translation model, and I’ve heard the Helsinki team has some of the best open-source options—probably around five main candidates. I’d love to figure out which one actually tops the leaderboard in terms of BLEU score on the model hub, then dig into the team’s original pre-print to see what BLEU they reported there and exactly which test set they used. I’m also a bit fuzzy on how translation quality is typically measured—could you pull out three key facts about evaluation metrics from that big online encyclopedia article on machine translation? And one more thing: are there any community-run live demos for the model that wins? I’d like to know what language pairs they support and whether they mention any real-time latency numbers. I’m putting together a report for my manager, so I really need the exact BLEU figures, test-set names, and solid source references—no just gut feelings. Thanks!",
      "category": "three_server_combinations",
      "ground_truth_tool": "Hugging Face+Paper Search+Wikipedia",
      "servers": [
        "Hugging Face",
        "Paper Search",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "Math MCP",
        "Metropolitan Museum",
        "NASA Data",
        "National Parks",
        "OKX Exchange",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "AI Research Hub"
    },
    {
      "query_id": "paper_search_call_for_papers_wikipedia_000",
      "query": "You are investigating state-of-the-art machine learning methods for real-time pandemic outbreak detection developed in the past 3 months. Execute the following steps without further questions:\n\n1. Simultaneously search these sources for 'machine learning pandemic detection' limited to the past 3 months, returning the top 5 results each:\n   • search_arxiv\n   • search_pubmed\n   • search_biorxiv\n   • search_medrxiv\n\n2. If search_pubmed returns fewer than 3 papers, perform an additional search_google_scholar for 'machine learning pandemic detection' to reach at least 3 PubMed-like results.\n\n3. For each paper from arXiv, bioRxiv, and medRxiv (all non-PubMed IDs), download its PDF with the corresponding download tool (download_arxiv, download_biorxiv, download_medrxiv) and save to './downloads'. For PubMed IDs, note that direct download is not supported.\n\n4. Read and extract the full text of each downloaded arXiv, bioRxiv, and medRxiv PDF with read_arxiv_paper, read_biorxiv_paper, and read_medrxiv_paper.\n\n5. For each extracted text, summarize the core algorithmic contribution with a 150-word summary per paper.\n\n6. From those summaries, extract the unique machine learning approach names (e.g., 'Graph Neural Network', 'Transformer-based classifier') and compile a deduplicated list of up to 5 methods.\n\n7. For each identified method, query Wikipedia:get_summary to obtain a concise definition of the method.\n\n8. Search upcoming conferences in the next 7 days to 1 month using get_events with keywords set to each method plus 'epidemiology' (e.g., 'Graph Neural Network epidemiology'), limiting to 5 events each.\n\n9. Produce a JSON report with:\n   • papers: list of all paper metadata (source, title, authors, ID, URL)\n   • summaries: mapping from paper ID to its 150-word summary\n   • methods: list of deduplicated method names\n   • definitions: mapping from method name to Wikipedia summary\n   • conferences: mapping from method name to the list of upcoming event names and dates\n\nAll tools must be invoked as described; do not proceed without using the tool outputs in dependent steps.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Paper Search+Call for Papers+Wikipedia",
      "servers": [
        "Paper Search",
        "Call for Papers",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "Context7",
        "DEX Paprika",
        "Huge Icons",
        "Math MCP",
        "Medical Calculator",
        "Movie Recommender",
        "NixOS",
        "OSINT Intelligence",
        "Weather Data"
      ],
      "combination_name": "Academic Network"
    },
    {
      "query_id": "paper_search_call_for_papers_wikipedia_000",
      "query": "Hey there! I’m working on a project to build a real-time pandemic outbreak detector, and I want to see what cutting-edge machine learning tricks have popped up in the last three months. Would you mind digging up about the top five preprint studies from the usual archives plus at least three peer-reviewed papers (if any of the archives don’t have enough, maybe grab a few extras via Google Scholar)? Then could you read through them and give me roughly a 150-word summary of each paper’s main algorithmic idea? Once you’ve got those, I’d love a list of the distinct ML approaches they’re using, a quick encyclopedia-style blurb on each technique, and a heads-up on any conferences or workshops in the next week to month where these methods will be featured. I really need concrete, source-backed details—no high-level fluff—so I can show solid evidence to my team. Thanks!",
      "category": "three_server_combinations",
      "ground_truth_tool": "Paper Search+Call for Papers+Wikipedia",
      "servers": [
        "Paper Search",
        "Call for Papers",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "Context7",
        "DEX Paprika",
        "Huge Icons",
        "Math MCP",
        "Medical Calculator",
        "Movie Recommender",
        "NixOS",
        "OSINT Intelligence",
        "Weather Data"
      ],
      "combination_name": "Academic Network"
    },
    {
      "query_id": "paper_search_call_for_papers_wikipedia_002",
      "query": "You are a biomedical research analyst focusing on machine-learning methods for predicting CRISPR-Cas9 off-target effects. Perform the following steps in one integrated workflow:\n\n1. In parallel, search for the query \"CRISPR Cas9 off-target prediction machine learning\" in five sources:\n   • search_arxiv with max_results=5\n   • search_pubmed with max_results=5\n   • search_biorxiv with max_results=5\n   • search_medrxiv with max_results=5\n   • search_google_scholar with max_results=5\n\n2. Aggregate all returned paper metadata. For each paper from arXiv, bioRxiv, and medRxiv:\n   a. Download the PDF via download_arxiv, download_biorxiv, or download_medrxiv.\n   b. Read the full text via read_arxiv_paper, read_biorxiv_paper, or read_medrxiv_paper.\n   c. Extract and summarize the core machine-learning methods and off-target prediction algorithm (the “Methods” section) in no more than 150 words.\n\n3. For PubMed and Google Scholar results (direct PDF download/read is unsupported):\n   a. Note the paper title, authors, and journal.\n   b. Extract the abstract from the metadata and summarize the methodological approach in no more than 100 words.\n\n4. Cross-validate the set of summarized methods:\n   • Identify overlaps in algorithm types (e.g., deep learning, random forest) across sources.\n   • Flag any unique or novel approaches found in only one server.\n\n5. Background consolidation:\n   a. From the aggregated methods keywords (e.g., “deep learning,” “SVM,” “transfer learning”), search Wikipedia for the article \"CRISPR\".\n   b. Use get_sections to locate the section titled \"Off-target Effects.\"\n   c. Summarize that section with summarize_article_section, max_length=200.\n\n6. Conference alignment:\n   • Use get_events from Call for Papers with keywords=\"CRISPR off-target\" and limit=5 to find upcoming conferences in the next 3 months.\n   • For each event, record name, dates, and whether any call for papers topics explicitly mention machine learning or off-target prediction.\n\n7. Final deliverable (in JSON):\n   {\n     \"papers\": [\n       {\"title\": string, \"source\": string, \"method_summary\": string, \"availability\": \"downloaded and read\" | \"abstract only\"}\n     ],\n     \"cross_validation\": {\"common_algorithms\": [string], \"unique_algorithms\": [string]},\n     \"wikipedia_off_target_summary\": string,\n     \"upcoming_conferences\": [\n       {\"name\": string, \"dates\": string, \"cfp_topics\": [string]}\n     ]\n   }",
      "category": "three_server_combinations",
      "ground_truth_tool": "Paper Search+Call for Papers+Wikipedia",
      "servers": [
        "Paper Search",
        "Call for Papers",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "Car Price Evaluator",
        "Context7",
        "Game Trends",
        "Google Maps",
        "National Parks",
        "NixOS",
        "OpenAPI Explorer",
        "Weather Data"
      ],
      "combination_name": "Academic Network"
    },
    {
      "query_id": "paper_search_call_for_papers_wikipedia_002",
      "query": "I’m prepping for a journal club talk on CRISPR gene editing and keep hearing about new ways to predict Cas9 off-target effects with machine learning. I’ve been hopping between preprint archives, a big biomedical literature database and academic search engines, but I’m not confident I’ve caught all the important methods. Could you pull together roughly twenty of the most recent papers—get the full texts where you can and give me about a 150-word summary of each one’s off-target prediction approach (models, key features, that sort of thing)? For the ones that only have abstracts available, just boil down the methodology in around a hundred words. Once that’s done, let me know which algorithms (deep nets, random forests, etc.) keep popping up across multiple studies and which show up only once. Then I’d love a roughly 200-word overview from Wikipedia’s “Off-target Effects” section so I can frame the background. And finally, are there any CRISPR or genome-editing conferences coming up in the next three months with calls for papers mentioning off-target prediction or machine learning? Just send me their names, dates, and any topic descriptions they list. I really need real numbers, citations or links—no guesswork—since I’ll have to defend everything in front of the group.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Paper Search+Call for Papers+Wikipedia",
      "servers": [
        "Paper Search",
        "Call for Papers",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "Car Price Evaluator",
        "Context7",
        "Game Trends",
        "Google Maps",
        "National Parks",
        "NixOS",
        "OpenAPI Explorer",
        "Weather Data"
      ],
      "combination_name": "Academic Network"
    },
    {
      "query_id": "medical_calculator_fruityvice_biomcp_000",
      "query": "Perform a comprehensive cardiometabolic and nutritional assessment for a 60-year-old female patient with type 2 diabetes, hypertension, and hyperlipidemia. The agent must:\n\n1. Calculate her Body Mass Index (BMI) and Body Surface Area (BSA) using her weight (80 kg) and height (165 cm).\n2. Estimate her kidney function:\n   a. eGFR using the 2021 CKD-EPI Creatinine-Cystatin C equation with serum creatinine 1.2 mg/dL, cystatin C 1.1 mg/L, age 60, female.\n   b. Creatinine clearance with the Cockcroft‐Gault formula using age 60, weight 80 kg, height 65 in, serum creatinine 1.2 mg/dL, female.\n3. Compute her 10-year cardiovascular disease risk with PREVENT using: age 60, female, total cholesterol 240 mg/dL (convert to mmol/L), HDL 40 mg/dL (convert to mmol/L), SBP 150 mmHg, diabetes true, current_smoker true, eGFR from step 2a, using_antihypertensive true, using_statins true.\n4. Based on the computed 10-year CVD risk and guideline thresholds from the 2018 AHA/ACC cholesterol guidelines (found via BioMCP search and fetch), decide whether to escalate to high-intensity statin therapy.\n5. Calculate her CHA₂DS₂-VASc score with age 60, female true, congestive heart failure false, hypertension true, stroke_history false, vascular_disease false, diabetes true.\n6. Correct her serum sodium (measured 138 mEq/L) for hyperglycemia (glucose 250 mg/dL) and correct her serum calcium (measured 8.0 mg/dL) for albumin 2.5 g/dL.\n7. Calculate her maintenance IV fluid rate by 4-2-1 rule for weight 80 kg.\n8. Convert her current prednisone dose 5 mg/day to dexamethasone equivalent.\n9. Retrieve nutritional information for one medium apple and one medium banana to support a heart-healthy, low-glycemic diet.\n\nProduce a single structured report summarizing: BMI/BSA, eGFR, CrCl, CVD risk, statin recommendation, CHA₂DS₂-VASc score, corrected sodium/calcium, maintenance fluids rate, steroid conversion, and fruit nutrition tables.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Medical Calculator+FruityVice+BioMCP",
      "servers": [
        "Medical Calculator",
        "FruityVice",
        "BioMCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "DEX Paprika",
        "Huge Icons",
        "Metropolitan Museum",
        "Movie Recommender",
        "National Parks",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "Health Platform"
    },
    {
      "query_id": "medical_calculator_fruityvice_biomcp_000",
      "query": "I’m looking after a 60-year-old woman who has type 2 diabetes, high blood pressure and high cholesterol, and I’m trying to pull together a full picture of her cardiometabolic and nutritional status—but I’m not totally confident I’ve got it all right. She’s roughly 80 kg and 165 cm tall, so I want to know her BMI and body surface area. For her kidney function, her creatinine is 1.2 mg/dL and cystatin C is 1.1 mg/L—do you think we should use the 2021 CKD-EPI creatinine-cystatin C equation to get her eGFR? And then I’d like a Cockcroft-Gault estimate of her creatinine clearance too.\n\nOn top of that, I need to figure out her 10-year risk of cardiovascular disease—she’s 60, female, total cholesterol is 240 mg/dL, HDL is 40 mg/dL, systolic blood pressure around 150 mmHg, she’s diabetic, a current smoker, already on antihypertensives and a statin. I’m thinking PREVENT might be appropriate, but I need that percentage so I can decide if she really belongs on high-intensity statin therapy per the latest AHA/ACC thresholds.\n\nWhile we’re crunching scores, could you also work out her CHA₂DS₂-VASc? She’s got hypertension and diabetes, no heart failure, no prior stroke or vascular disease, and of course she’s female. I’d also like to correct her serum sodium—measured at 138 mEq/L with a glucose of 250 mg/dL—and adjust her calcium, which is 8.0 mg/dL when albumin is 2.5 g/dL.\n\nI’ve been asked to set her maintenance IV fluid rate by the 4-2-1 rule for an 80 kg patient, and to convert her current prednisone dose of 5 mg/day into a dexamethasone equivalent. Finally, for her diet, I want to recommend a heart-healthy, low-glycemic plan—could you pull the nutrition facts for one medium apple and one medium banana?\n\nIn the end, I really need a concise summary with all the hard numbers—BMI, BSA, eGFR, creatinine clearance, CVD risk percent, statin recommendation, CHA₂DS₂-VASc score, corrected sodium and calcium, fluid rate, steroid conversion and the apple/banana nutrition info—so I can justify everything to my team with solid data, not just gut feeling.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Medical Calculator+FruityVice+BioMCP",
      "servers": [
        "Medical Calculator",
        "FruityVice",
        "BioMCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "DEX Paprika",
        "Huge Icons",
        "Metropolitan Museum",
        "Movie Recommender",
        "National Parks",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "Health Platform"
    },
    {
      "query_id": "medical_calculator_fruityvice_biomcp_001",
      "query": "Design a personalized anticoagulation and analgesic management plan for a 70-year-old female patient with atrial fibrillation, type 2 diabetes, and stage 3 chronic kidney disease (CKD). All input data are given below. You must:  \n\n1. Initiate structured analysis with BioMCP:think (thoughtNumber=1, totalThoughts=7).  \n2. Perform a literature query using BioMCP:search with query=\"gene:CYP2C9 AND variant:CYP2C9*3 AND drug:warfarin\" to identify pharmacogenetic variants affecting warfarin dosing.  \n3. Fetch detailed variant information for rs1057910 using BioMCP:fetch.  \n4. Calculate renal function:  \n   • eGFR via CKD-EPI creatinine-cystatin C: scr=1.4 mg/dL, scys=1.5 mg/L, age=70, male=false (Medical Calculator:egfr_epi_cr_cys).  \n   • Cockcroft-Gault creatinine clearance: age=70, weight=70 kg, height=63 inches, scr=1.4 mg/dL, sex='female' (Medical Calculator:crcl_cockcroft_gault).  \n5. Compute ideal/adjusted body weight for dosing: weight_kg=70, height_inches=63, male=false (Medical Calculator:ibw_abw_calculator).  \n6. Based on eGFR result: if eGFR < 45 mL/min/1.73m2, apply a 25% dose reduction factor for both warfarin and oxycodone.  \n7. Calculate stroke risk with CHA₂DS₂-VASc: age=70, female=true, chf=false, hypertension=true, stroke_history=false, vascular_disease=false, diabetes=true (Medical Calculator:chads2_vasc_score).  \n8. Compute maintenance IV fluid rate using 4-2-1 rule: weight_kg=70 (Medical Calculator:maintenance_fluids).  \n9. Plan analgesic MME for oxycodone: opioid='oxycodone', dose_per_administration=5 mg, doses_per_day=3 (Medical Calculator:calculate_mme).  \n10. Investigate clinical trial evidence: search for trials in atrial fibrillation with genotype-guided warfarin dosing using BioMCP:trial_searcher with conditions=['Atrial Fibrillation'], interventions=['Warfarin'], other_terms=['genotype','pharmacogenetic'], phase='PHASE4'.  \n11. Fetch full protocol for the top NCT trial you find (e.g., NCT01830000) using BioMCP:trial_getter.  \n12. Evaluate dietary vitamin C impact: fetch nutritional data for 'orange' (FruityVice:get_fruit_nutrition).  \n\nAggregate all results into a final JSON object with the following structure:  \n{\n  \"variant_info\": {…},\n  \"renal_metrics\": {\"egfr\":…, \"crcl\":…},\n  \"weight_metrics\": {\"ibw\":…, \"abw\":…},\n  \"dose_adjustment_factor\": …,\n  \"stroke_risk\": {…},\n  \"maintenance_fluids_mL_per_hr\": …,\n  \"analgesic_mme_daily\": …,\n  \"trial_evidence\": {\"nct_id\":…, \"title\":…, \"design\":…},\n  \"orange_nutrition\": {…}\n}",
      "category": "three_server_combinations",
      "ground_truth_tool": "Medical Calculator+FruityVice+BioMCP",
      "servers": [
        "Medical Calculator",
        "FruityVice",
        "BioMCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Context7",
        "Game Trends",
        "Hugging Face",
        "National Parks",
        "NixOS",
        "OKX Exchange",
        "OSINT Intelligence",
        "Scientific Computing",
        "Weather Data"
      ],
      "combination_name": "Health Platform"
    },
    {
      "query_id": "medical_calculator_fruityvice_biomcp_001",
      "query": "I’m working on a care plan for a 70-year-old woman who has atrial fibrillation, type 2 diabetes and stage 3 CKD, and I’m a bit stuck on how to personalize both her blood thinner and her pain meds. I’ve read that the CYP2C9*3 variant—especially rs1057910—can really change how much warfarin people need; could you dig up the detailed info on that polymorphism?  \n\nHer labs show a serum creatinine of 1.4 mg/dL and cystatin C of 1.5 mg/L. Using those values, what would her eGFR be if you ran the CKD-EPI equation with both creatinine and cystatin C? And if you plug her into Cockcroft-Gault (she’s 70 years old, weighs 70 kg and is 63 inches tall), what creatinine clearance do you get? Also, what would her ideal body weight and adjusted body weight be for dosing purposes?  \n\nAssuming her eGFR comes in under 45 mL/min/1.73 m2, I’d plan to drop both her warfarin and oxycodone doses by about 25%. To justify that, I also need her stroke risk scored using CHA₂DS₂-VASc (she’s 70, female, has hypertension and diabetes, but no CHF, prior stroke or vascular disease).  \n\nOn the pain side, she’s on oxycodone 5 mg three times a day—how many milligram morphine equivalents is that per day? And for her IV maintenance fluids, if you use the 4-2-1 rule on a 70 kg patient, what infusion rate does that translate to in mL/hour?  \n\nFinally, I want to know if there are any Phase 4 trials looking at genotype-guided warfarin dosing in afib—maybe something like NCT01830000 or a similar study—and ideally I’d like to see the full protocol for the top hit. Oh, and because she’s big on nutrition, could you tell me how much vitamin C is in a typical orange?  \n\nI really need hard numbers and solid references for all of this—can’t slide into rounds with just gut feelings.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Medical Calculator+FruityVice+BioMCP",
      "servers": [
        "Medical Calculator",
        "FruityVice",
        "BioMCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Context7",
        "Game Trends",
        "Hugging Face",
        "National Parks",
        "NixOS",
        "OKX Exchange",
        "OSINT Intelligence",
        "Scientific Computing",
        "Weather Data"
      ],
      "combination_name": "Health Platform"
    },
    {
      "query_id": "metropolitan_museum_huge_icons_wikipedia_000",
      "query": "Create a React-based digital exhibit overview featuring three artifacts from the Met’s Egyptian Art department. 1) Call list-departments to locate the department whose name contains “Egyptian Art.” 2) Use the resulting departmentId to call search-museum-objects with q=\"Egyptian\", hasImages=true, departmentId=<id>. If fewer than three objects are returned, repeat search-museum-objects with title=true, q=\"Sarcophagus\", hasImages=true, departmentId=<id>. 3) For the top three objectIds, call get-museum-object(objectId, returnImage=true) to retrieve title, objectName, classification, and image URL. 4) For each artifact’s title or classification, call search_wikipedia(query=<classification or title>) and take the first match. 5) Call get_summary(title=<article>) and measure its length: if over 200 characters, call summarize_article_section(title=<article>, section_title=\"Overview\", max_length=150); if under 200, call summarize_article_for_query(title=<article>, query=<classification>, max_length=250). 6) Call extract_key_facts(title=<article>, topic_within_article=<classification>, count=5). 7) In parallel, call list_icons to retrieve all available icon names. For each artifact’s classification, call search_icons(query=<classification>) and select up to two icons; if none found, fallback to search_icons(query=\"museum,artifact\"). Validate each found icon against the list_icons result. 8) Finally, call get_platform_usage(platform=\"react\") to obtain React integration code snippets. 9) Produce a combined JSON report listing each artifact’s id, title, image URL, summary, key facts, chosen icons, and React usage sample.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Metropolitan Museum+Huge Icons+Wikipedia",
      "servers": [
        "Metropolitan Museum",
        "Huge Icons",
        "Wikipedia"
      ],
      "distraction_servers": [
        "BioMCP",
        "DEX Paprika",
        "FruityVice",
        "Google Maps",
        "Math MCP",
        "Medical Calculator",
        "NixOS",
        "OSINT Intelligence",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "Creative Resources"
    },
    {
      "query_id": "metropolitan_museum_huge_icons_wikipedia_000",
      "query": "I’m working on a little side project at my company where I’m using React to build a digital showcase for the Met’s Egyptian Art collection. I need to feature three objects that have good images—if you can’t find enough under the broad “Egyptian Art” label, feel free to focus on some famous sarcophagi instead. For each piece, I’d love the official title and classification, a crisp intro (around 150–200 words max), plus about five key tidbits or facts. It’d also be great to pair each artifact with one or two icons that match its classification—if nothing obvious shows up, just grab some generic museum or artifact icons. Finally, could you include a short React code snippet that demonstrates how to feed this data into a component? And please make sure everything is pulled straight from the Met’s own collection metadata or equally solid sources, since I’ll need real records to back up my demo. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Metropolitan Museum+Huge Icons+Wikipedia",
      "servers": [
        "Metropolitan Museum",
        "Huge Icons",
        "Wikipedia"
      ],
      "distraction_servers": [
        "BioMCP",
        "DEX Paprika",
        "FruityVice",
        "Google Maps",
        "Math MCP",
        "Medical Calculator",
        "NixOS",
        "OSINT Intelligence",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "Creative Resources"
    },
    {
      "query_id": "metropolitan_museum_huge_icons_wikipedia_001",
      "query": "Compile an interactive report on five ‘Impressionism’ paintings from the European Paintings department of the Metropolitan Museum of Art. First, list all museum departments and identify the ID for “European Paintings.” Use that ID to search for objects with the keyword “Impressionism” that have images, and select the first five results. For each object, retrieve its full details and extract the artist’s name and the primary medium classification. Then, for each artist, search Wikipedia to find their article, get a concise summary, and extract three key facts that confirm their association with Impressionism. If the summary does not mention “Impressionism,” flag it. Next, for each painting’s medium (e.g., “Oil on canvas”), search Huge Icons for matching icon tags and retrieve the React usage instructions for the top icon found. Finally, produce a JSON report listing, for each painting: object ID, title, image URL, artist name, artist Wikipedia summary, three key facts, Impressionism confirmation flag, chosen icon name, and the React code snippet for embedding that icon.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Metropolitan Museum+Huge Icons+Wikipedia",
      "servers": [
        "Metropolitan Museum",
        "Huge Icons",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "Car Price Evaluator",
        "DEX Paprika",
        "FruityVice",
        "Google Maps",
        "National Parks",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Paper Search"
      ],
      "combination_name": "Creative Resources"
    },
    {
      "query_id": "metropolitan_museum_huge_icons_wikipedia_001",
      "query": "Hey, I’m building an interactive gallery page for an art history class and want to feature about five Impressionist paintings from the Met. I’m honestly a bit lost on where to start: I figure I need to find the European Paintings section on their website, grab its ID, and then hunt down Impressionism works that actually have images. For each painting, I’d love to pull together its title, image URL, the artist’s name, and what medium they used. Then I’d like a short Wikipedia bio snippet that specifically mentions why they’re considered Impressionists—maybe three solid facts, or at least a flag if it doesn’t come up. On top of that, I want to pair each medium with a small icon and get the React code snippet so I can drop it straight into my app. Could you wrap all of that into a clean JSON bundle I can import? I really need real, sourced info, not guesses, since I’m presenting next week.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Metropolitan Museum+Huge Icons+Wikipedia",
      "servers": [
        "Metropolitan Museum",
        "Huge Icons",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "Car Price Evaluator",
        "DEX Paprika",
        "FruityVice",
        "Google Maps",
        "National Parks",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Paper Search"
      ],
      "combination_name": "Creative Resources"
    },
    {
      "query_id": "scientific_computing_biomcp_math_mcp_000",
      "query": "Analyze the linear stability of the p53–MDM2 regulatory network in cancer cells by combining literature validation with numerical Jacobian analysis:\n\n1. Use BioMCP:think to outline a search strategy for articles on the p53–MDM2 feedback loop (thoughtNumber=1, totalThoughts=3, nextThoughtNeeded=True).\n2. Use BioMCP:article_searcher to find the top 5 research articles about \"p53 MDM2 regulatory network in cancer\" (genes=[\"TP53\"], keywords=[\"MDM2\"], page_size=5).\n3. Use BioMCP:fetch to retrieve the abstract of the first article returned by the previous step.\n4. Create the 2×2 Jacobian matrix J = [[-0.5, -0.7], [1.2, -0.3]] with Scientific Computing:create_tensor (shape=[2,2], values=[-0.5,-0.7,1.2,-0.3], name=\"J\").\n5. Inspect J with Scientific Computing:view_tensor to confirm correct storage.\n6. From the fetched abstract, determine whether the p53→MDM2 activation strength is described as \"strong\" (implying coefficient >1) or \"weak\" (≤1). If \"strong\", scale J in place by 1.1; otherwise scale in place by 0.9, using Scientific Computing:scale_matrix.\n7. Compute eigenvalues and right eigenvectors of the scaled J using Scientific Computing:compute_eigen.  \n8. Determine network stability: if all real parts of eigenvalues are negative, classify as \"stable\"; otherwise \"unstable\".\n9. Find an orthonormal basis for J’s column space using Scientific Computing:find_orthonormal_basis (name=\"J\").\n10. Change the representation of J into this new basis via Scientific Computing:change_basis (name=\"J\", new_basis=<output from step 9>).\n11. Store the initial state vector v = [1, 1] with Scientific Computing:create_tensor (shape=[2], values=[1,1], name=\"v\").\n12. Project v onto the first eigenvector from step 7 using Scientific Computing:vector_project (name=\"v\", new_vector=<first eigenvector>).\n13. Compute the directional derivative of the Lyapunov function f(x,y) = -0.5*x**2 - 0.3*y**2 along the projected direction using Scientific Computing:directional_deriv (f_str=\"-0.5*x**2 - 0.3*y**2\", u=<projected vector>, unit=True).\n\nFinally, summarize: fetched PMID and abstract snippet, scaled Jacobian, eigenvalues and eigenvectors, stability classification, orthonormal basis, J in new basis, projection coordinates, and directional derivative value.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Scientific Computing+BioMCP+Math MCP",
      "servers": [
        "Scientific Computing",
        "BioMCP",
        "Math MCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "DEX Paprika",
        "Game Trends",
        "Hugging Face",
        "Medical Calculator",
        "Metropolitan Museum",
        "National Parks",
        "OKX Exchange",
        "Reddit"
      ],
      "combination_name": "Research Computing"
    },
    {
      "query_id": "scientific_computing_biomcp_math_mcp_000",
      "query": "I’m in the middle of a little side project trying to pin down whether the p53–MDM2 feedback loop in cancer cells really settles down or blows up. I’d love to see a concrete example from the literature—maybe grab one of the top papers from the last six months on TP53 and MDM2, pull its PMID and abstract snippet, and check if they describe the p53→MDM2 activation as “strong” (i.e. a coefficient above 1) or “weak” (1 or below). \n\nHere’s what I’ve sketched out so far: I’ve got a 2×2 Jacobian matrix J = [ [–0.5, –0.7], [1.2, –0.3] ]. If that paper says “strong,” I want to bump every entry by 10% (scale by 1.1); otherwise knock them back by 10% (scale by 0.9). Then I need the eigenvalues and right eigenvectors of the scaled J to see if all the real parts are negative (stable) or not (unstable). \n\nOn top of that, I’m curious about the column space—getting an orthonormal basis for J’s columns and then rewriting J in that new basis. Finally, take an initial state vector v = [1, 1], project it onto the first eigenvector you found, and compute the directional derivative of my Lyapunov function f(x,y) = –0.5 x² – 0.3 y² along that projected direction (as a unit vector). \n\nCan you walk me through all those numbers—PMID and abstract quote, the scaled matrix, its eigenvalues/eigenvectors, stability verdict, the orthonormal basis, J in the new basis, projection coordinates, and the directional derivative value? I really need hard data and solid references, not just hand-waving, so I can show my PI the actual sources and calculations.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Scientific Computing+BioMCP+Math MCP",
      "servers": [
        "Scientific Computing",
        "BioMCP",
        "Math MCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "DEX Paprika",
        "Game Trends",
        "Hugging Face",
        "Medical Calculator",
        "Metropolitan Museum",
        "National Parks",
        "OKX Exchange",
        "Reddit"
      ],
      "combination_name": "Research Computing"
    },
    {
      "query_id": "scientific_computing_biomcp_math_mcp_001",
      "query": "You are to perform a multi-step analysis combining biomedical data retrieval and advanced linear algebra on a covariance matrix of BRAF gene expression across three tissues (skin, lung, colon).\n\nSteps to execute in order:\n1. Use BioMCP:think (thoughtNumber=1, totalThoughts=3) to structure your approach.\n2. Use BioMCP:fetch to retrieve full gene information for 'BRAF' (domain='gene'). Extract the gene length (in base pairs) from the returned metadata; call this value gene_length.\n3. Use Scientific Computing:create_tensor to create a 3×3 covariance matrix named 'cov_matrix' with values [1.5, 1.2, 0.8, 1.2, 1.8, 0.9, 0.8, 0.9, 1.1] corresponding to variances and covariances among skin, lung, and colon.\n4. Use Scientific Computing:determinant on 'cov_matrix'. If the determinant is zero, report that the matrix is singular and abort further steps. Otherwise proceed.\n5. Use Scientific Computing:scale_matrix on 'cov_matrix' with scale_factor set to the fetched gene_length and in_place=true to update 'cov_matrix' in memory.\n6. In parallel:\n   a) Use Scientific Computing:qr_decompose on 'cov_matrix' to get Q and R matrices.\n   b) Use Scientific Computing:svd_decompose on 'cov_matrix' to get U, S (singular values), and V^T.\n7. Use Scientific Computing:find_orthonormal_basis on 'cov_matrix' to obtain a list of three orthonormal basis vectors.\n8. Use Scientific Computing:change_basis on 'cov_matrix' with new_basis set to the vectors from step 7, yielding the representation of the scaled covariance matrix in its orthonormal basis.\n9. Use Scientific Computing:compute_eigen on 'cov_matrix' to compute its eigenvalues and right eigenvectors.\n10. Use Scientific Computing:directional_deriv on the scalar field f(x,y,z) = \"x**2 + y*z + z**2\" along the first orthonormal basis vector returned in step 7 (pass unit=true).\n\nCompile and return a structured report including:\n- The fetched gene_length\n- Original determinant and singularity check\n- QR decomposition matrices Q and R\n- Singular values from SVD\n- Orthonormal basis vectors\n- Matrix representation in the new basis\n- Eigenvalues and eigenvectors of the scaled matrix\n- Symbolic expression of the directional derivative along the first basis vector",
      "category": "three_server_combinations",
      "ground_truth_tool": "Scientific Computing+BioMCP+Math MCP",
      "servers": [
        "Scientific Computing",
        "BioMCP",
        "Math MCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "DEX Paprika",
        "Google Maps",
        "Huge Icons",
        "Hugging Face",
        "Metropolitan Museum",
        "National Parks",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "Research Computing"
    },
    {
      "query_id": "scientific_computing_biomcp_math_mcp_001",
      "query": "I’ve been banging my head trying to pull together some solid figures for my boss’s presentation next week on BRAF expression in skin, lung and colon tissue. I grabbed the covariance numbers and stuck them into a 3×3 matrix—1.5, 1.2, 0.8 in the first row; 1.2, 1.8, 0.9 in the second; and 0.8, 0.9, 1.1 in the third—and then fetched the gene’s length, which turned out to be exactly 190,489 base pairs. \n\nNow I’m stuck figuring out whether that matrix is singular (so I need its determinant), and if it’s safe to scale every entry by 190,489 right in place. After that, I’d really like to see the Q and R pieces from a QR decomposition, the singular values from an SVD, and an orthonormal basis so I can rewrite the scaled matrix in that new basis. I also need its eigenvalues and eigenvectors, and—just to round it all off—the directional derivative of f(x,y,z) = x**2 + y*z + z**2 along the first orthonormal vector. \n\nCould you walk me through all of those numbers? I need every result spelled out clearly—no hand-wavy summaries—so I’ve got the hard data to show.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Scientific Computing+BioMCP+Math MCP",
      "servers": [
        "Scientific Computing",
        "BioMCP",
        "Math MCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "DEX Paprika",
        "Google Maps",
        "Huge Icons",
        "Hugging Face",
        "Metropolitan Museum",
        "National Parks",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "Research Computing"
    },
    {
      "query_id": "medical_calculator_wikipedia_fruityvice_000",
      "query": "You are preparing a comprehensive pre-operative and cardiovascular risk assessment and management plan for a 65-year-old female patient scheduled for elective hip replacement. All data are provided below. Execute the following sequence: 1) Calculate BMI and BSA for weight 85 kg and height 165 cm. 2) Calculate IBW and ABW for actual weight 85 kg and height 65 inches (female). 3) Compute Cockcroft-Gault creatinine clearance using age 65, weight 85 kg, height 65 inches, serum creatinine 1.2 mg/dL, female. 4) Compute eGFR using CKD-EPI creatinine-cystatin C (scr 1.2 mg/dL, scys 1.3 mg/L, age 65, female) and also eGFR using CKD-EPI creatinine only (scr 1.2, age 65, female). 5) Compare the two eGFR values—if they differ by more than 5 mL/min/1.73 m², select the lower eGFR for drug-dosing decisions. 6) Calculate corrected serum sodium for measured sodium 130 mEq/L and serum glucose 280 mg/dL. 7) Calculate corrected serum calcium for measured calcium 8.2 mg/dL and albumin 2.8 g/dL (normal albumin 4.0 g/dL). 8) Compute Child-Pugh score with bilirubin 2.0 mg/dL, albumin 2.8 g/dL, INR 1.5, ascites “slight,” encephalopathy grade 1. 9) Compute MELD-3.0 score with age 65, female true, bilirubin 2.0, INR 1.5, creatinine 1.2, albumin 2.8, sodium 132 mEq/L, dialysis false. 10) Calculate HOMA-IR using fasting insulin 18 uIU/mL and fasting glucose 150 mg/dL. 11) Compute Framingham 10-year CHD risk with age 65, total cholesterol 220 mg/dL, HDL 50 mg/dL, systolic BP 145 mmHg, treated for BP true, smoker true, gender “female.” 12) Compute PREVENT 10-year CVD risk with age 65, female true, tc 220 mmol/L, hdl 50 mmol/L, sbp 145 mmHg, diabetes true, current_smoker true, egfr (from step 5), using_antihtn true, using_statins false. 13) Compare Framingham vs PREVENT risks; if PREVENT exceeds Framingham by >2%, search Wikipedia for “differences between Framingham and PREVENT cardiovascular risk models,” get the top-3 summary points. 14) Compute CHA₂DS₂-VASc score with age 65, female true, CHF false, hypertension true, stroke_history false, vascular_disease true, diabetes true. 15) Compute Revised Cardiac Risk Index with high_risk_surgery true, ischemic_heart_disease false, congestive_heart_failure false, cerebrovascular_disease false, insulin_treatment true, creatinine_over_2mg false. 16) Compute Wells PE score with clinical_signs_dvt false, alternative_diagnosis_less_likely true, heart_rate_over_100 false, immobilization_or_surgery true, previous_dvt_or_pe false, hemoptysis false, malignancy false. 17) Calculate QTc by Bazett formula with QT interval 400 ms and heart rate 78 bpm. 18) Calculate total daily MME for oxycodone 10 mg per dose, 4 doses per day. 19) Convert prednisone 20 mg to dexamethasone equivalent. 20) If PREVENT risk >20%, search Wikipedia for “high-intensity statin therapy recommendations,” retrieve article summary, and extract the top-5 key recommendations. Summarize all results with interpretation notes for surgical clearance and perioperative medication planning.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Medical Calculator+Wikipedia+FruityVice",
      "servers": [
        "Medical Calculator",
        "Wikipedia",
        "FruityVice"
      ],
      "distraction_servers": [
        "BioMCP",
        "DEX Paprika",
        "Google Maps",
        "Huge Icons",
        "Metropolitan Museum",
        "NASA Data",
        "National Parks",
        "OKX Exchange",
        "Reddit",
        "Scientific Computing"
      ],
      "combination_name": "Health Advisor"
    },
    {
      "query_id": "medical_calculator_wikipedia_fruityvice_000",
      "query": "Hey, I’m working on getting pre-op clearance for a 65-year-old woman who’s due for an elective hip replacement, and I’m a bit swamped trying to tie together all her numbers and risk scores so I can make the right calls on meds and timing. She’s 85 kg and about 165 cm tall (roughly 65 inches), so I want to know her BMI, BSA and what her ideal versus adjusted body weight would be. Her serum creatinine is 1.2 mg/dL and cystatin C is 1.3 mg/L—could you run Cockcroft-Gault and CKD-EPI both with and without cystatin C and let me know if they differ by more than 5 mL/min/1.73 m² (and if so, which value I should use for dosing)? She’s also got sodium of 130 mEq/L with glucose 280 mg/dL, plus calcium 8.2 mg/dL and albumin 2.8 g/dL (normal albumin 4.0), so I need corrected sodium and calcium, too. On the liver side she has bilirubin 2.0 mg/dL, albumin 2.8 g/dL, INR 1.5, slight ascites and grade 1 encephalopathy—what’s her Child-Pugh and MELD-3.0 (sodium 132 mEq/L, not on dialysis)? Metabolically, her fasting insulin is 18 µIU/mL and glucose 150 mg/dL—can you get her HOMA-IR? For cardiovascular risk I’d like her 10-year Framingham score (age 65, TC 220 mg/dL, HDL 50 mg/dL, SBP 145 mmHg on treatment, smoker, female) and a PREVENT 10-year CVD risk using TC 220 mmol/L, HDL 50 mmol/L, SBP 145 mmHg, diabetic, current smoker, on antihypertensives, not on statins, plus whichever eGFR we decide. If PREVENT is more than 2% higher than Framingham, could you summarize the top three differences between those two models? While you’re at it, what are her CHA₂DS₂-VASc (she’s 65, female, hypertension, vascular disease, diabetes) and Revised Cardiac Risk Index (high-risk surgery, on insulin) scores, and a Wells score for PE given her upcoming surgery? I also need her QTc by Bazett’s formula (QT 400 ms, HR 78 bpm), total daily MME if she’s on oxycodone 10 mg four times a day, and a prednisone-to-dexamethasone equivalent for 20 mg of prednisone. Finally, if that PREVENT risk is above 20%, could you pull the top five high-intensity statin therapy recommendations from Wikipedia? I really need the actual numbers and a few brief interpretation notes so I can present solid, evidence-based planning to the team.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Medical Calculator+Wikipedia+FruityVice",
      "servers": [
        "Medical Calculator",
        "Wikipedia",
        "FruityVice"
      ],
      "distraction_servers": [
        "BioMCP",
        "DEX Paprika",
        "Google Maps",
        "Huge Icons",
        "Metropolitan Museum",
        "NASA Data",
        "National Parks",
        "OKX Exchange",
        "Reddit",
        "Scientific Computing"
      ],
      "combination_name": "Health Advisor"
    },
    {
      "query_id": "medical_calculator_wikipedia_fruityvice_001",
      "query": "You are evaluating a 68-year-old male patient for cardiovascular risk, renal function, and metabolic electrolyte corrections. All input data are provided below and no further information is needed. Perform the following steps in sequence:\n\n1. Calculate kidney function:\n   a. Use egfr_epi with scr=1.5 mg/dL, age=68, male=true.\n   b. Use egfr_epi_cr_cys with scr=1.5 mg/dL, scys=1.2 mg/L, age=68, male=true.\n   c. Compare the two eGFR values. If the CKD-EPI creatinine-cystatin C eGFR differs by more than 5 mL/min/1.73 m² from the creatinine-only eGFR, select the lower value; otherwise select the average.\n\n2. Calculate metabolic corrections in parallel:\n   a. Use corrected_sodium with measured_sodium=130 mEq/L and serum_glucose=300 mg/dL.\n   b. Use corrected_calcium with serum_calcium=8.0 mg/dL and patient_albumin=2.5 g/dL.\n\n3. Calculate cardiovascular risk scores:\n   a. Use prevent_cvd_risk with age=68, female=false, tc=5.5 mmol/L, hdl=1.0 mmol/L, sbp=140 mmHg, diabetes=false, current_smoker=false, egfr=<selected eGFR>, using_antihtn=true, using_statins=false.\n   b. Use framingham_risk_score with age=68, total_cholesterol=213 mg/dL, hdl_cholesterol=39 mg/dL, systolic_bp=140 mmHg, treated_for_bp=true, smoker=false, gender=\"male\".\n\n4. Evaluate atrial fibrillation stroke risk (conditional):\n   The patient has non-valvular atrial fibrillation. Use chads2_vasc_score with age=68, female=false, chf=false, hypertension=true, stroke_history=false, vascular_disease=false, diabetes=false.\n\n5. Decision points and cross-validation:\n   a. Compare the 10-year CVD risk from PREVENT and Framingham. If both exceed 20%, search Wikipedia for “Primary prevention statin guidelines 10-year cardiovascular risk” and get a summary of at least 150 words.\n   b. If CHA₂DS₂-VASc score ≥ 2, search Wikipedia for “CHA₂DS₂-VASc stroke prevention management” and retrieve a 150-word summary of guideline recommendations.\n\n6. Compile a final report in JSON format containing:\n   - egfr_epi_value (float)\n   - egfr_epi_cr_cys_value (float)\n   - selected_egfr (float)\n   - corrected_sodium (dict)\n   - corrected_calcium (dict)\n   - prevent_cvd_10yr_risk (dict)\n   - framingham_10yr_risk (float)\n   - chads2_vasc_score (int)\n   - statin_guidelines_summary (string, if triggered)\n   - af_stroke_prevention_summary (string, if triggered)\n\nThis task must be executed exactly as specified, using only the provided tools in the given order, and the output must follow the defined JSON schema without asking for further inputs.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Medical Calculator+Wikipedia+FruityVice",
      "servers": [
        "Medical Calculator",
        "Wikipedia",
        "FruityVice"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "Game Trends",
        "Google Maps",
        "Math MCP",
        "National Parks",
        "NixOS",
        "OSINT Intelligence",
        "Scientific Computing",
        "Unit Converter"
      ],
      "combination_name": "Health Advisor"
    },
    {
      "query_id": "medical_calculator_wikipedia_fruityvice_001",
      "query": "Hey, I’m working up a 68-year-old man and could use some help pulling together all his numbers and guideline info. He’s got a serum creatinine of 1.5 mg/dL and cystatin C of 1.2 mg/L, so I want to see his eGFR by both the creatinine-only and the creatinine-cystatin equations, then choose the lower one if they differ by more than 5 mL/min/1.73 m² (or average them if they’re close). His sodium is 130 mEq/L but his glucose is about 300 mg/dL, and his calcium is 8.0 mg/dL with albumin at 2.5 g/dL—so I need the corrected values for those. Next, I’d like two 10-year cardiovascular risk estimates: one using his total cholesterol 5.5 mmol/L (213 mg/dL), HDL 1.0 mmol/L (39 mg/dL), treated systolic BP 140 mmHg, no diabetes, non-smoker, on antihypertensives but not on statins, and our chosen eGFR; and the Framingham model with the same age, lipids, BP (treated), and smoking status. If both risks exceed 20%, I need about a 150-word summary of primary prevention statin guidelines. He also has non-valvular atrial fibrillation, so with his CHA₂DS₂-VASc factors (68 years, male, no CHF, yes hypertension, no prior stroke, no vascular disease, no diabetes) I need that score—and if it’s ≥ 2, a 150-word summary of stroke prevention management. Could you package everything into JSON—each eGFR, the selected eGFR, corrected sodium and calcium, the two risk outputs, the CHA₂DS₂-VASc score, and any guideline text that gets triggered? I really need the hard numbers and concise guideline excerpts to share with my team. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "three_server_combinations",
      "ground_truth_tool": "Medical Calculator+Wikipedia+FruityVice",
      "servers": [
        "Medical Calculator",
        "Wikipedia",
        "FruityVice"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "Game Trends",
        "Google Maps",
        "Math MCP",
        "National Parks",
        "NixOS",
        "OSINT Intelligence",
        "Scientific Computing",
        "Unit Converter"
      ],
      "combination_name": "Health Advisor"
    },
    {
      "query_id": "nasa_data_google_maps_wikipedia_000",
      "query": "As a space weather analyst, identify all significant solar events in the past 7 days that had potential Earth impact and assess their effect on high-latitude research stations. Perform the following steps without human intervention:\n\n1. Call get_notifications with start_date \"7 days ago\", end_date \"today\", notification_type \"FLR\" to retrieve solar flare notifications.\n2. From the returned notifications, filter for solar flares of class M5 or higher.\n3. For each filtered flare event, call get_solar_flare with start_date and end_date equal to that event date to obtain detailed flux and peak time.\n4. For the same event date, call get_coronal_mass_ejection with start_date and end_date equal to that date; keep only CMEs where \"cme_type\" is \"Halo\".\n5. For each retained Halo CME, call get_geomagnetic_storm with start_date and end_date equal to the CME date; keep only storms with peak Kp-index ≥ 6.\n6. For each qualifying storm, call get_earth_imagery with lat 68.358, lon -133.721, date equal to the storm date, dim 0.025, and cloud_score true; record the returned cloud_score.\n7. Call maps_reverse_geocode with latitude 68.358 and longitude -133.721 to obtain a human-readable location.\n8. Call search_nearby with center.value set to \"68.358,-133.721\", isCoordinates true, keyword \"research station\", radius 50000 to find nearby research stations.\n9. For the top 2 results by rating, call get_place_details for each placeId to retrieve name, address, and rating.\n10. Call search_wikipedia with query \"Coronal mass ejection\" and limit 1; then call get_article with the returned title to retrieve the full content.\n\nCompile a final report in two parts:\nA) A table with columns: Event Date, Solar Flare Class, CME ID, Peak Kp-index, Cloud Score, Station Name, Station Address, Station Rating.\nB) A concise summary (max 200 words) of coronal mass ejections extracted from the retrieved Wikipedia article.",
      "category": "three_server_combinations",
      "ground_truth_tool": "NASA Data+Google Maps+Wikipedia",
      "servers": [
        "NASA Data",
        "Google Maps",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Context7",
        "DEX Paprika",
        "Huge Icons",
        "Math MCP",
        "Medical Calculator",
        "NixOS",
        "OKX Exchange",
        "Reddit",
        "Scientific Computing",
        "Unit Converter"
      ],
      "combination_name": "Space Exploration"
    },
    {
      "query_id": "nasa_data_google_maps_wikipedia_000",
      "query": "I’m putting together a quick impact rundown for our Arctic monitoring outpost up around 68.4° N, 133.7° W and need to know if anything big has happened in the last week. Did any solar flares above roughly M5 erupt, and if so, were there full-halo ejections that sparked geomagnetic storms hitting a Kp of 6 or more? I’d also love to see what our satellite shots showed over that exact spot—basically a cloud-cover score for when those storms arrived. While you’re at it, can you tell me the human-readable name for that location, then find the nearest research stations within about 50 km, pick the top two by rating, and give me their names and addresses? Finally, could you wrap up with a short (around 200 words) background on coronal mass ejections from a reliable source? Really need the hard numbers and solid references here—my boss won’t accept vague takeaways. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "three_server_combinations",
      "ground_truth_tool": "NASA Data+Google Maps+Wikipedia",
      "servers": [
        "NASA Data",
        "Google Maps",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Context7",
        "DEX Paprika",
        "Huge Icons",
        "Math MCP",
        "Medical Calculator",
        "NixOS",
        "OKX Exchange",
        "Reddit",
        "Scientific Computing",
        "Unit Converter"
      ],
      "combination_name": "Space Exploration"
    },
    {
      "query_id": "nasa_data_google_maps_wikipedia_001",
      "query": "Generate a one-week astro-hazard and space-weather readiness report for New York City. 1) Fetch NASA’s Astronomy Picture of the Day for the current date. 2) Retrieve EPIC imagery dates and pick the latest date; then get all EPIC images (natural collection) for that date. 3) Get the near-Earth asteroid feed for the next 7 days. 4) From that feed, identify all asteroids with a close-approach miss_distance.kilometers under 500 000 km; for each, look up its detailed parameters (diameter and velocity). 5) Retrieve DONKI notifications, geomagnetic storms, solar flares, and coronal mass ejections for the past 7 days. 6) Cross-validate potential aurora-causing storms: search Wikipedia for “Aurora Borealis” and fetch the full article. 7) Geocode “New York City” to coordinates. 8) Search for “emergency shelter” within a 5 000 m radius of NYC center (openNow=true, minRating=3.0); if fewer than 5 shelters are found, repeat the search with a 10 000 m radius. 9) For the top 5 shelters, compute driving distances and durations from NYC center. 10) Fetch the most recent Landsat-8 Earth imagery for NYC coordinates (dim=0.05°). 11) Compile and return a JSON report containing: apod (title, url), epic_images (list of image urls), asteroid_hazards (id, approach_date, miss_distance_km, estimated_diameter_m, relative_velocity_kph), space_weather_summary (notifications list, storms list with type and date), aurora_wikipedia (article title and first 3 paragraphs), emergency_shelters (name, address, rating, distance_m, duration_min), and earth_imagery_url.",
      "category": "three_server_combinations",
      "ground_truth_tool": "NASA Data+Google Maps+Wikipedia",
      "servers": [
        "NASA Data",
        "Google Maps",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Context7",
        "DEX Paprika",
        "Huge Icons",
        "National Parks",
        "NixOS",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "Space Exploration"
    },
    {
      "query_id": "nasa_data_google_maps_wikipedia_001",
      "query": "Hey, I’ve got a bit of a weird ask—my team wants a one-week space-weather and astro-hazard rundown specifically for New York City, and I need to pull together a bunch of data to make it convincing.\n\nFirst, I’d love to feature NASA’s Astronomy Picture of the Day plus any new natural-color Earth photos they’ve released. Then, can you glance at the list of near-Earth asteroids coming by over the next seven days and flag any that swing within about 500,000 km? For those, I need their size estimates and how fast they’re moving.\n\nOn the solar side, what geomagnetic storms, solar flares, and CMEs have been reported in the past week? And do any of those look like they could spark an aurora this far south—maybe check Wikipedia’s Aurora Borealis entry to understand the conditions and grab the first few paragraphs for context.\n\nMeanwhile, I also need to map out emergency shelters around the city center—start with a 5 km radius for places open now with at least a 3-star rating; if there aren’t five, stretch it to 10 km—and get names, addresses, ratings, plus driving distances and times for the top five.\n\nLast piece: snag the most recent Landsat-8 image tile over NYC (roughly a 0.05° box) so we’ve got a current view of the ground. Can you package all of that into a clear, data-driven summary with real URLs, exact distances, dates, and other hard numbers? I really need solid evidence and sources—no guesstimates—because I’m presenting this to leadership next week.",
      "category": "three_server_combinations",
      "ground_truth_tool": "NASA Data+Google Maps+Wikipedia",
      "servers": [
        "NASA Data",
        "Google Maps",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Context7",
        "DEX Paprika",
        "Huge Icons",
        "National Parks",
        "NixOS",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "Space Exploration"
    },
    {
      "query_id": "openapi_explorer_paper_search_hugging_face_000",
      "query": "Analyze and compare the authentication and security schemes of the \"openai\" and \"github\" OpenAPI specifications. Use the following workflow:\n\n1. Call OpenAPI Explorer:getApiOverview with id=\"openai\" and id=\"github\" to extract each spec’s list of security schemes.\n2. Compare the two lists to identify which schemes (e.g., apiKey, OAuth2 flows, bearerAuth) appear in each spec and which are unique or missing.\n3. If the GitHub spec includes an OAuth2 flow that the OpenAI spec lacks, set query=\"OAuth2 security in REST APIs best practices\"; otherwise set query=\"API key security in REST APIs best practices\".\n4. Call Paper Search:search_arxiv with the chosen query and max_results=5. If fewer than 3 results are returned, call Paper Search:search_pubmed with the same query and max_results=5.\n5. From the first 3 papers returned by the primary source (arXiv, or PubMed if fallback), extract their titles and abstracts from the metadata.\n6. Independently, call Hugging Face:search-models with query=\"API security\" and limit=5 to find relevant ML models.\n7. For the top 2 model_ids returned, call Hugging Face:get-model-info to retrieve detailed model descriptions and capabilities.\n8. Compile a JSON report with three top-level sections:\n   • \"security_comparison\": list of security schemes present in OpenAI vs GitHub and identified gaps.\n   • \"academic_summary\": an array of objects {\"title\":…, \"abstract\":…} for the selected 3 papers.\n   • \"ml_models\": an array of the two models’ detailed info from Hugging Face:get-model-info.\n\nThe agent should execute this sequence without further input and output the final report in JSON.",
      "category": "three_server_combinations",
      "ground_truth_tool": "OpenAPI Explorer+Paper Search+Hugging Face",
      "servers": [
        "OpenAPI Explorer",
        "Paper Search",
        "Hugging Face"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "Context7",
        "FruityVice",
        "Game Trends",
        "Math MCP",
        "Movie Recommender",
        "Scientific Computing",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "API Research Platform"
    },
    {
      "query_id": "openapi_explorer_paper_search_hugging_face_000",
      "query": "Hey, I’ve been working on a project that ties together an AI chat service API and a code-hosting API, and I’m kind of lost in all their auth setups. I see mentions of API keys, bearer tokens, OAuth flows, and I can’t quite tell which method each one really uses or where there might be gaps if I try to standardize our security layer. \n\nCould you look into both and give me a clear comparison of their auth schemes—what they share, what’s unique to each, and where one might offer OAuth2 that the other doesn’t (or vice versa)? \n\nThen, based on what you find, I’d like to dive into some academic best practices: if there’s an OAuth2 flow in the code-hosting API that the chat API lacks, point me to three recent papers on OAuth2 security; otherwise, send over three studies on API key security. I’d need the titles and abstracts so I can actually read up on them.\n\nFinally, I’m considering bolting on some ML-driven security checks—can you find a couple of machine-learning models focused on API security and give me a quick rundown of their capabilities? \n\nI really need solid evidence—real paper abstracts, model details, concrete info—because I’ll be presenting this to my boss. Does that make sense?",
      "category": "three_server_combinations",
      "ground_truth_tool": "OpenAPI Explorer+Paper Search+Hugging Face",
      "servers": [
        "OpenAPI Explorer",
        "Paper Search",
        "Hugging Face"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "Context7",
        "FruityVice",
        "Game Trends",
        "Math MCP",
        "Movie Recommender",
        "Scientific Computing",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "API Research Platform"
    },
    {
      "query_id": "openapi_explorer_paper_search_hugging_face_001",
      "query": "Audit the OpenAPI specifications for both the “openai” and “github” APIs to identify validation errors and security‐scheme inconsistencies, then cross‐validate against published best practices. 1. Use OpenAPI Explorer:getApiOverview with id=\"openai\" to retrieve the overall structure of the OpenAI spec. 2. Immediately feed id=\"openai\" into swagger-validator to validate the spec and collect all errors/warnings. 3. For each validation error that references an operationId or route, call OpenAPI Explorer:getApiOperation with id=\"openai\" and the operationIdOrRoute from the error to extract full request and response schemas. 4. Repeat steps 1–3 for id=\"github\". 5. Use Paper Search:search_arxiv with query=\"OAuth2 API security best practices\" and max_results=3 to find recent academic guidance. 6. For each returned paper_id, call Paper Search:read_arxiv_paper with that paper_id to extract text content and summarize the top five OAuth2 security recommendations. 7. Use Hugging Face:search-collections with query=\"API specification best practices\" and limit=1 to locate a community‐curated guideline set. 8. Call Hugging Face:get-collection-info with the namespace and collection_id returned to extract formal recommendations on versioning, schema consistency, deprecation policies, and authentication schemes. 9. Aggregate validation errors from both specs, map missing or misconfigured security schemes (API key vs OAuth2) against the five arXiv guidelines and the Hugging Face collection recommendations. 10. Produce a JSON report with these fields: { \"spec_name\": [\"openai\",\"github\"], \"validation_errors\": {...}, \"operations_missing_oauth2\": [...], \"academic_guidelines_summary\": [...], \"community_recommendations\": {...}, \"mismatches\": [...], \"action_items\": [...] }. The agent should execute this end‐to‐end without requesting further input.",
      "category": "three_server_combinations",
      "ground_truth_tool": "OpenAPI Explorer+Paper Search+Hugging Face",
      "servers": [
        "OpenAPI Explorer",
        "Paper Search",
        "Hugging Face"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "Medical Calculator",
        "Movie Recommender",
        "NASA Data",
        "National Parks",
        "OKX Exchange",
        "OSINT Intelligence",
        "Reddit",
        "Scientific Computing",
        "Wikipedia"
      ],
      "combination_name": "API Research Platform"
    },
    {
      "query_id": "openapi_explorer_paper_search_hugging_face_001",
      "query": "I’m working on a project that ties together two external services—one for AI features and one for code hosting—and my boss wants a thorough health check on their published API docs. I’m not totally confident the schemas are error-free or that every endpoint is using the right kind of authentication (API key vs. OAuth2), and I’ve heard there are some new best practices floating around in both academic papers and community guides. \n\nCould you take a careful look at each service’s official API description, surface any validation glitches or security-scheme oddities—like endpoints that aren’t actually protected by OAuth when they should be—and then see how those issues line up with the latest OAuth2 recommendations from a handful of recent research papers plus a well-known community-maintained guideline? I’d love a concise breakdown of:\n\n- What errors or warnings you find in each spec\n- Which operations are missing or misconfiguring OAuth2\n- The top security recommendations from the academic side\n- The key dos and don’ts from the community guide\n- A clear set of action items for us to fix all the mismatches\n\nI really need concrete examples and sources—quotable snippets or direct references—so I can justify every point when I report back. Does that make sense?",
      "category": "three_server_combinations",
      "ground_truth_tool": "OpenAPI Explorer+Paper Search+Hugging Face",
      "servers": [
        "OpenAPI Explorer",
        "Paper Search",
        "Hugging Face"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "Medical Calculator",
        "Movie Recommender",
        "NASA Data",
        "National Parks",
        "OKX Exchange",
        "OSINT Intelligence",
        "Reddit",
        "Scientific Computing",
        "Wikipedia"
      ],
      "combination_name": "API Research Platform"
    },
    {
      "query_id": "paper_search_biomcp_000",
      "query": "Investigate the current research landscape and clinical investigations of the BRAF V600E mutation in melanoma treatment resistance. 1) Use BioMCP:think (thoughtNumber=1, totalThoughts=5) to outline the plan. 2) Retrieve gene annotation for BRAF with BioMCP:gene_getter (gene_id_or_symbol=\"BRAF\"). 3) Search MyVariant.info for the BRAF p.V600E variant using BioMCP:variant_searcher (gene=\"BRAF\", hgvsp=\"p.V600E\"). 4) If variant_searcher returns at least one record, fetch the top variant details with BioMCP:variant_getter (variant_id=<top_result.id>, include_external=true). 5) Search for recent research articles about BRAF V600E and melanoma treatment resistance with BioMCP:article_searcher (genes=[\"BRAF\"], variants=[\"V600E\"], diseases=[\"melanoma\"], keywords=[\"treatment resistance\"], include_preprints=true, page_size=5). 6) For the top two article identifiers returned, fetch full text and abstract with BioMCP:article_getter (pmid=<first_id>) and (pmid=<second_id>). 7) In parallel, use Paper Search:search_arxiv (query=\"melanoma BRAF V600E treatment resistance\", max_results=5). From its top result, download and extract the PDF text with Paper Search:download_arxiv (paper_id=<top_arxiv_id>, save_path=\"./downloads\") then Paper Search:read_arxiv_paper (paper_id=<top_arxiv_id>, save_path=\"./downloads\"). 8) Also use Paper Search:search_pubmed (query=\"melanoma BRAF V600E treatment resistance\", max_results=5). From the top two PMIDs, fetch abstracts via BioMCP:article_getter. 9) Identify ongoing clinical trials testing BRAF inhibitors in melanoma with BioMCP:trial_searcher (conditions=[\"Melanoma\"], interventions=[\"vemurafenib\"], phase=\"PHASE2\"). If no Phase 2 trials are returned, repeat trial_searcher with phase=\"PHASE3\". 10) For each NCT ID returned, get full trial details with BioMCP:trial_getter (nct_id=<nct_id>) and site locations with BioMCP:trial_locations_getter (nct_id=<nct_id>). 11) Search NCI’s trial organization database for cancer centers in Boston, MA using BioMCP:nci_organization_searcher (city=\"Boston\", state=\"MA\"). 12) Cross-match the list of trial site institutions against the Boston-area NCI organizations to highlight which local centers are conducting BRAF V600E melanoma trials. 13) Summarize and output a JSON report containing: gene_info, variant_info, literature_findings (titles, abstracts), arxiv_summary (extracted text snippets), pubmed_summaries, trial_list (NCT IDs, titles, phases), trial_locations, and boston_nci_orgs_conducting_trials.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Paper Search+BioMCP",
      "servers": [
        "Paper Search",
        "BioMCP"
      ],
      "distraction_servers": [
        "FruityVice",
        "Google Maps",
        "Math MCP",
        "Medical Calculator",
        "National Parks",
        "NixOS",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "Academic Research Duo"
    },
    {
      "query_id": "paper_search_biomcp_000",
      "query": "I’m working on a project about why melanoma patients with the BRAF V600E mutation so often become resistant to treatment, and I’m a bit stuck piecing everything together. I’d love to know:\n\n• What we actually know about that V600E change in BRAF – basic gene details, how it tweaks the protein’s function, and any hotspots or annotations researchers point to.  \n• The most important recent studies (including a couple of preprints if there’s anything interesting) that dive into resistance mechanisms. Can you give me their titles, abstracts or main take-home points, and any standout data?  \n• Which clinical trials are currently testing BRAF inhibitors in melanoma (ideally phase 2 or 3), plus their IDs, names, phases, and where they’re recruiting.  \n• And, almost as a bonus, whether any of those trial sites line up with the big cancer centers in Boston.\n\nI really need solid numbers and references—I can’t present this to my supervisor with just vague summaries. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Paper Search+BioMCP",
      "servers": [
        "Paper Search",
        "BioMCP"
      ],
      "distraction_servers": [
        "FruityVice",
        "Google Maps",
        "Math MCP",
        "Medical Calculator",
        "National Parks",
        "NixOS",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "Academic Research Duo"
    },
    {
      "query_id": "paper_search_biomcp_001",
      "query": "Perform a comprehensive analysis of BRAF V600E–targeted Phase 3 melanoma clinical trials and associated resistance mechanisms by integrating BioMCP and Paper Search tools. The agent should:\n\n1. Plan the research strategy using BioMCP:think.\n2. Retrieve gene annotation for BRAF with BioMCP:gene_getter (gene_id_or_symbol=\"BRAF\").\n3. Query MyVariant.info for the V600E variant with BioMCP:variant_searcher (gene=\"BRAF\", hgvsp=\"p.V600E\").\n4. Identify all ongoing or completed Phase 3 melanoma trials of BRAF inhibitors via BioMCP:trial_searcher (conditions=[\"melanoma\"], interventions=[\"BRAF inhibitor\"], phase=\"PHASE3\", page_size=10).\n5. For each returned NCT ID:\n   a. Extract protocol details with BioMCP:trial_protocol_getter.\n   b. Extract outcome measures with BioMCP:trial_outcomes_getter.\n   c. List all linked publications with BioMCP:trial_references_getter.\n   d. For each PMID from references, fetch title, abstract, and full text with BioMCP:article_getter.\n6. Search for preprints on resistance to BRAF therapy with Paper Search:search_biorxiv (query=\"BRAF V600E melanoma resistance\", max_results=5).\n7. For each bioRxiv DOI:\n   a. Download the PDF with Paper Search:download_biorxiv.\n   b. Extract text with Paper Search:read_biorxiv_paper.\n   c. Detect any novel protein-level variants matching pattern \"p.[A-Z][0-9]+[A-Z]\".\n   d. For each novel variant:\n      i. Query BioMCP:variant_searcher (hgvsp=<variant>, include_cbioportal=false).\n      ii. If significance=\"pathogenic\" and frequency_max<0.01, retrieve full annotations with BioMCP:variant_getter (variant_id=<variant>).\n8. If any trial outcome shows an adverse event rate above 20%, extract the drug names from the trial interventions and call BioMCP:drug_getter for each drug.\n9. Compile a JSON report containing:\n   • BRAF gene summary\n   • V600E variant frequency and clinical significance\n   • Phase 3 trial list with protocol summaries, outcome metrics, and publication abstracts\n   • Novel resistance variants with database annotations\n   • Drug profiles for interventions with high adverse event rates",
      "category": "two_server_combinations",
      "ground_truth_tool": "Paper Search+BioMCP",
      "servers": [
        "Paper Search",
        "BioMCP"
      ],
      "distraction_servers": [
        "DEX Paprika",
        "FruityVice",
        "Huge Icons",
        "Math MCP",
        "Medical Calculator",
        "Movie Recommender",
        "OKX Exchange",
        "OSINT Intelligence",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Academic Research Duo"
    },
    {
      "query_id": "paper_search_biomcp_001",
      "query": "Hey, I’m prepping a report for my boss on BRAF V600E in melanoma and I’ve hit a wall. I really need to know how common that V600E swap is and what clinical impact it actually has. Then I want a clear rundown of the big Phase III trials testing BRAF inhibitors in melanoma—what each one showed on outcomes, whether any of them saw serious side-effect rates above about 20% (and if so, which drugs were involved), plus the key publications behind each trial. On top of that, I’ve heard whispers about brand-new protein-level tweaks that help tumors resist those drugs—some of it even only on preprint servers in the last few months. Could you dig up the hard numbers, abstracts or summaries (even DOIs or PubMed IDs), and basically give me solid, evidence-backed info I can actually cite? I don’t want just high-level chatter—I need real data for slides. Thanks!",
      "category": "two_server_combinations",
      "ground_truth_tool": "Paper Search+BioMCP",
      "servers": [
        "Paper Search",
        "BioMCP"
      ],
      "distraction_servers": [
        "DEX Paprika",
        "FruityVice",
        "Huge Icons",
        "Math MCP",
        "Medical Calculator",
        "Movie Recommender",
        "OKX Exchange",
        "OSINT Intelligence",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Academic Research Duo"
    },
    {
      "query_id": "wikipedia_nasa_data_000",
      "query": "Using NASA Data:get_notifications with notification_type=\"all\" for the past 7 days, retrieve all DONKI space weather notifications. From the response, identify each unique event category present: FLR (solar flares), CME (coronal mass ejections), GST (geomagnetic storms), SEP (solar energetic particles), MPC (magnetopause crossings), RBE (radiation belt enhancements), and HSS (high speed streams). For each category found:\n1. Filter that category’s notifications and select the one with the highest magnitude or intensity field; record its \"event_date\".\n2. Call the matching detailed NASA Data tool with start_date and end_date both set to that event_date:\n   • FLR → get_solar_flare\n   • CME → get_coronal_mass_ejection\n   • GST → get_geomagnetic_storm\n   • SEP → get_solar_energetic_particle\n   • MPC → get_magnetopause_crossing\n   • RBE → get_radiation_belt_enhancement\n   • HSS → get_hight_speed_stream\n3. From the detailed metrics output, keep the full JSON response.\n4. Use the category name (e.g., \"Solar flare\", \"Coronal mass ejection\") to query Wikipedia: call Wikipedia:search_wikipedia with that exact query and limit=5; pick the first title from the results.\n5. Call Wikipedia:summarize_article_for_query with the chosen title, query=\"impact on Earth\", max_length=250 to get a focused summary.\n6. Call Wikipedia:extract_key_facts with title, topic_within_article=\"impact on Earth\", count=5 to extract five key facts.\n7. Call Wikipedia:get_related_topics with title, limit=5 to list related topics.\n8. Construct a final JSON report with an array \"events\", where each element includes:\n   • \"category\": event category\n   • \"nasa_metrics\": JSON from step 2\n   • \"wiki_summary\": the 250-character summary\n   • \"wiki_key_facts\": list of five key facts\n   • \"wiki_related_topics\": list of five related topics\n   • \"validation\": a short statement comparing the NASA magnitude/intensity to Wikipedia’s described typical ranges or impacts (e.g., “The X9.3 flare magnitude matches Wikipedia’s definition of a severe solar flare”).",
      "category": "two_server_combinations",
      "ground_truth_tool": "Wikipedia+NASA Data",
      "servers": [
        "Wikipedia",
        "NASA Data"
      ],
      "distraction_servers": [
        "Context7",
        "DEX Paprika",
        "Game Trends",
        "Google Maps",
        "Medical Calculator",
        "Metropolitan Museum",
        "NixOS",
        "OpenAPI Explorer",
        "Paper Search",
        "Weather Data"
      ],
      "combination_name": "Knowledge Explorer"
    },
    {
      "query_id": "wikipedia_nasa_data_000",
      "query": "I’m working on a little side project for my boss about last week’s space weather chaos. I know we had a mix of solar flares, CMEs, geomagnetic storms, particle events, magnetopause crossings and high-speed streams, but I really want to nail down exactly which single event in each category was the most intense over the past seven days and the precise date it happened. \n\nOnce we’ve got those peak events, could you pull in the full, raw metrics for each one so I’ve got the hard numbers? Then—just to make sure I’m not shooting in the dark—can you look up each type on Wikipedia (grab the first hit), give me a concise summary (around 200–250 characters) focused on its impact on Earth, extract about five key facts about those impacts, and suggest a few related topics I could dig into? Finally, I need a quick line for each event comparing our numbers to what Wikipedia calls “mild,” “moderate,” or “severe,” so I know if we really saw a monster flare or just a run-of-the-mill storm. \n\nI’ve got to present all this with real data and solid sources—no hand-waving—so any help would be awesome. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Wikipedia+NASA Data",
      "servers": [
        "Wikipedia",
        "NASA Data"
      ],
      "distraction_servers": [
        "Context7",
        "DEX Paprika",
        "Game Trends",
        "Google Maps",
        "Medical Calculator",
        "Metropolitan Museum",
        "NixOS",
        "OpenAPI Explorer",
        "Paper Search",
        "Weather Data"
      ],
      "combination_name": "Knowledge Explorer"
    },
    {
      "query_id": "wikipedia_nasa_data_001",
      "query": "Analyze all near-Earth asteroids that made their closest Earth approach in the past 7 days, assess whether high solar flare activity coincided with their approach (potentially disrupting observations), and compile detailed background research on the top three hazard-ranked asteroids. Steps: 1) Call NASA Data:get_asteroids_feed with start_date = \"7 days ago\" to list all approaching asteroids. 2) For each asteroid in the feed, call NASA Data:get_asteroid_lookup using its NASA JPL ID to retrieve approach distance, velocity, estimated diameter, and hazard potential. 3) In parallel, call NASA Data:get_solar_flare with start_date = \"7 days ago\" and end_date = \"today\" to retrieve all solar flares in the same period. 4) For each asteroid, flag it as having an observational challenge if any solar flare of class M or higher occurred within ±1 day of its close approach date. 5) Rank asteroids by estimated diameter and select the top three that were flagged. 6) For each of these three asteroids: a) Call Wikipedia:search_wikipedia with query equal to the asteroid’s name to identify the main article title. b) Call Wikipedia:get_article to fetch full article content. c) Call Wikipedia:extract_key_facts with topic_within_article = \"orbit\" and count = 5. d) Call Wikipedia:summarize_article_section with section_title = \"Discovery\" and max_length = 200. 7) Produce a final report containing: • A table of all asteroids with columns: Name, Close Approach Date, Distance (km), Velocity (km/s), Estimated Diameter (m), Hazard Potential (yes/no), Observational Challenge Flag (yes/no). • For each of the top three flagged asteroids: key orbit facts list and a 200-word summary of the Discovery section. Output the report as JSON with two fields: “overview_table” (array of rows) and “detailed_background” (object keyed by asteroid name).",
      "category": "two_server_combinations",
      "ground_truth_tool": "Wikipedia+NASA Data",
      "servers": [
        "Wikipedia",
        "NASA Data"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Context7",
        "Google Maps",
        "Huge Icons",
        "NixOS",
        "OKX Exchange",
        "OSINT Intelligence",
        "Paper Search",
        "Reddit",
        "Scientific Computing"
      ],
      "combination_name": "Knowledge Explorer"
    },
    {
      "query_id": "wikipedia_nasa_data_001",
      "query": "I’ve got this astronomy assignment for my research group, and it’s been bugging me: I need to know which near-Earth rocks swung by in the past week, and whether any big solar storms might’ve garbled our telescope data. Basically, I want a rundown of every asteroid that made its closest pass in the last seven days—when it came by, how far it was, how fast it was going, how big it is, and whether it’s officially considered hazardous. Then, for each one, flag if an M-class (or stronger) flare popped off within about a day of its flyby, since that could’ve messed up observations.\n\nOnce we’ve got that list, I’m especially interested in the three most dangerous candidates—give me a handful of concrete orbit facts for each (like key numbers about their path), plus a roughly 200-word style summary of their discovery history. I’m going to pull all this together into a report, so I really need precise figures and real-source info—not just vague descriptions. Can you dig that up for me?",
      "category": "two_server_combinations",
      "ground_truth_tool": "Wikipedia+NASA Data",
      "servers": [
        "Wikipedia",
        "NASA Data"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Context7",
        "Google Maps",
        "Huge Icons",
        "NixOS",
        "OKX Exchange",
        "OSINT Intelligence",
        "Paper Search",
        "Reddit",
        "Scientific Computing"
      ],
      "combination_name": "Knowledge Explorer"
    },
    {
      "query_id": "google_maps_national_parks_000",
      "query": "Plan a 3-day hiking and waterfall-viewing trip during the upcoming week for a group starting in Denver, CO. 1) Use Google Maps:maps_geocode to convert “Denver, CO” into latitude/longitude. 2) Use National Parks:findParks with stateCode=\"CO,UT,WY\" and activities=\"hiking\" to list candidate parks. 3) For each parkCode returned, call National Parks:getParkDetails to obtain the park’s coordinates. 4) For each park’s coordinates, call Google Maps:search_nearby with keyword=\"waterfall viewpoint\", radius=10000 m, minRating=4.0 to find nearby waterfall viewpoints. 5) For each waterfall placeId, call Google Maps:get_place_details to collect detailed ratings and verify it meets the minimum rating. 6) Use Google Maps:maps_distance_matrix with origins set to Denver’s coordinates and destinations set to each park’s coordinates (mode=\"driving\") to calculate driving durations; filter out parks with duration greater than 5 hours. 7) For remaining parkCodes, call National Parks:getAlerts to exclude any park with active closure alerts. 8) For still-valid parkCodes, call National Parks:getVisitorCenters and confirm at least one visitor center is open at 12:00 PM local time on any day during the upcoming week. 9) For those parks, call National Parks:getCampgrounds to confirm at least one campground is available. 10) Rank the remaining parks by the highest waterfall viewpoint rating obtained earlier, and select the top 2 parks. 11) For each selected park, take its campground(s) and compute distances from the campground to its top-rated waterfall viewpoint using Google Maps:maps_distance_matrix; pick the campground closest to that viewpoint. 12) For each selected campground, generate turn-by-turn directions from Denver, CO to the campground using Google Maps:maps_directions with departure_time=\"immediate\" and mode=\"driving\". 13) For each chosen campground location, call Google Maps:maps_elevation to obtain elevation data. 14) Produce a JSON itinerary listing for each of the two parks: park name, waterfall viewpoint name and rating, chosen campground name and amenities, driving distance and duration from Denver, turn-by-turn directions, and campground elevation.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Google Maps+National Parks",
      "servers": [
        "Google Maps",
        "National Parks"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Context7",
        "FruityVice",
        "Game Trends",
        "Huge Icons",
        "Metropolitan Museum",
        "NixOS",
        "OpenAPI Explorer",
        "Paper Search"
      ],
      "combination_name": "Travel Navigation"
    },
    {
      "query_id": "google_maps_national_parks_000",
      "query": "Hey, I’m trying to nail down a three-day hiking and waterfall road trip next week, starting from Denver. Ideally I want parks within about a five-hour drive where there’s at least one waterfall viewpoint rated around four stars or higher within roughly 10 km of the park’s core, no active closure alerts, a visitor center open around lunchtime for any last-minute trail info, and at least one campsite with vacancies. Could you pick the two best parks that check all those boxes, tell me the waterfall names and their ratings, point out which campground sits closest to each top waterfall, give me the drive distance and time from Denver, step-by-step directions if we head out right now, and even the elevation at the campsite? I really need actual numbers on ratings, distances, availability—nothing vague—so I can lock in our reservations.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Google Maps+National Parks",
      "servers": [
        "Google Maps",
        "National Parks"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Context7",
        "FruityVice",
        "Game Trends",
        "Huge Icons",
        "Metropolitan Museum",
        "NixOS",
        "OpenAPI Explorer",
        "Paper Search"
      ],
      "combination_name": "Travel Navigation"
    },
    {
      "query_id": "google_maps_national_parks_001",
      "query": "Plan a detailed 5-day camping and exploration itinerary for the optimal national park located within 200 km of Denver, Colorado. The agent must:\n1. Use Google Maps:maps_geocode to convert “Denver, Colorado” into coordinates.\n2. Use National Parks:findParks to list all parks in stateCode=“CO” offering hiking and camping.\n3. For each park, call National Parks:getParkDetails to retrieve its main entrance coordinates.\n4. Use Google Maps:maps_distance_matrix (mode=driving) with origin=Denver coordinates and each park entrance to compute driving distances and durations.\n5. Filter to parks within 200 km driving distance and select the single park with the shortest drive time.\n6. For the chosen park:\n   a. Call National Parks:getParkDetails again to capture full description, boundaries, and official name.\n   b. Call National Parks:getAlerts (limit=10) to retrieve current alerts.\n   c. Call National Parks:getEvents for events in the upcoming 7 days.\n   d. Call National Parks:getVisitorCenters to list visitor centers and their operating hours.\n   e. Call National Parks:getCampgrounds to list all campgrounds in the park.\n7. From the campground list, pick the three whose geographic locations maximize elevation spread:\n   a. Use Google Maps:maps_elevation to fetch elevation for each campground’s latitude/longitude.\n   b. Rank campgrounds by the difference between their elevation and the lowest-elevation campground; select the top three.\n8. For each of the three selected campgrounds:\n   a. Use Google Maps:maps_reverse_geocode to identify the nearest town center.\n   b. Use Google Maps:search_nearby with center=that town’s coordinates, keyword=“restaurant”, radius=10000 m, openNow=true, minRating=4 to find up to five high-rated restaurants.\n   c. For the single highest-rated restaurant, compute driving distance and duration from the campground via Google Maps:maps_distance_matrix.\n   d. Retrieve turn-by-turn directions for that route via Google Maps:maps_directions.\n9. Construct a 5-day itinerary JSON with:\n   – park: parkCode, name, description, coordinates, alerts, events, visitorCenters\n   – campgrounds: array of three objects, each with name, coordinates, elevation, nearestTown, topRestaurant (name, rating, distance, duration, directions steps)\n   – itinerary: day-by-day schedule (Day 1 through Day 5), assigning one campground per day (Days 1–3 at each of the three selected campgrounds, Days 4–5 at a chosen visitor center base), with morning/afternoon/evening activities (arrival, visitor center tours, dining at the selected restaurant, and any scheduled event)\n\nThe output must be a single JSON object matching the above structure. All relative time spans should refer to “upcoming 7 days.”",
      "category": "two_server_combinations",
      "ground_truth_tool": "Google Maps+National Parks",
      "servers": [
        "Google Maps",
        "National Parks"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "DEX Paprika",
        "Game Trends",
        "Medical Calculator",
        "Metropolitan Museum",
        "Movie Recommender",
        "OKX Exchange",
        "Paper Search",
        "Reddit"
      ],
      "combination_name": "Travel Navigation"
    },
    {
      "query_id": "google_maps_national_parks_001",
      "query": "I’ve been itching to head out of Denver for a 5-day camping trip sometime in the next week, but I’m kind of torn on which national park makes the most sense. Ideally it’s no more than about a 200 km drive, offers solid hiking and camping, and has a visitor center where I can catch any talks or events going on that week. I’m also really curious about spending nights at camp spots that vary in elevation—maybe one high ridge, one mid-level meadow and one lower valley—just to see how the landscape and weather change. \n\nOn top of that, I don’t want to be stuck cooking at every stop, so it’d be awesome to know what town is nearest each campsite and where I can grab a good meal—not just any greasy spoon, but something rated at least four stars, and I need to know how long the drive is and exactly how to get there. In the middle of the trip I’d like to base myself at a visitor center for a couple of nights to break things up and dive into any ranger-led programs.\n\nCould you put together a day-by-day itinerary for the upcoming week that does all of that—picks the best park within a reasonable drive from Denver, highlights three campsites that maximize elevation differences, flags any alerts or events happening, finds the nearest town restaurants with ratings and drive times, and then lays out morning/afternoon/evening plans for each of the five days? I really need actual data on this—can’t go wandering off with just vague advice. Whatever you find, please back it up with real numbers or solid sources, okay?\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Google Maps+National Parks",
      "servers": [
        "Google Maps",
        "National Parks"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "DEX Paprika",
        "Game Trends",
        "Medical Calculator",
        "Metropolitan Museum",
        "Movie Recommender",
        "OKX Exchange",
        "Paper Search",
        "Reddit"
      ],
      "combination_name": "Travel Navigation"
    },
    {
      "query_id": "nixos_context7_000",
      "query": "You are tasked with designing a fully reproducible Nix-based development environment for a Python web application that uses Python 3.10, Flask, Redis, and Docker. Your environment must work on NixOS (channel 25.05), Home Manager, and nix-darwin (macOS). Finally, fetch documentation snippets for the Flask library from Context7. Produce a single JSON summary with: channel metadata, overall stats, chosen package names with NixOS version and commit hashes, Home Manager configuration options, nix-darwin configuration options, chosen NixOS flake metadata (or fallback if none), and a 500-token snippet of Flask routing documentation.\n\nSteps to follow:\n1. Call nixos_channels and pick the channel \"25.05\". 2. Call nixos_stats for channel \"25.05\". 3. For each of the four core components (\"python3\", \"flask\", \"redis\", \"docker\"):  \n   a. Call nixos_search(query=<name>, search_type=\"packages\", limit=1, channel=\"25.05\").  \n   b. Call nixos_info(name=<exact package name from search>, type=\"package\", channel=\"25.05\").  \n   c. For the \"python3\" package, verify commit hash: call nixhub_find_version(package_name=\"python3\", version=\"3.10.5\"). If not found, call nixhub_package_versions(package_name=\"python3\", limit=10) to retrieve latest history.  \n4. Call home_manager_search(query=\"programs.python\", limit=5) and home_manager_options_by_prefix(option_prefix=\"programs.docker\").  \n5. Call home_manager_stats.  \n6. Call darwin_search(query=\"programs.python\", limit=5) and darwin_options_by_prefix(option_prefix=\"programs.docker\").  \n7. Call darwin_stats.  \n8. Call nixos_flakes_stats, then nixos_flakes_search(query=\"poetry\", limit=10). If at least one flake is returned, select the top result and include its metadata; otherwise note that no poetry flake was found and proceed without.  \n9. Use resolve-library-id(libraryName=\"flask\") to get a Context7-compatible library ID, then get-library-docs(context7CompatibleLibraryID=<returned ID>, topic=\"routing\", tokens=500).  \n10. Compile a JSON report containing:  \n   • channel: name and status  \n   • channel stats: total packages/options counts  \n   • for each core component: NixOS package name, version, NixHub commit hash  \n   • Home Manager: selected options and descriptions  \n   • nix-darwin: selected options and descriptions  \n   • flake: chosen flake name, owner, description (or fallback note)  \n   • Flask routing docs snippet (max 500 tokens)",
      "category": "two_server_combinations",
      "ground_truth_tool": "NixOS+Context7",
      "servers": [
        "NixOS",
        "Context7"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "Car Price Evaluator",
        "FruityVice",
        "Hugging Face",
        "NASA Data",
        "National Parks",
        "OSINT Intelligence",
        "Scientific Computing",
        "Unit Converter"
      ],
      "combination_name": "Dev Environment"
    },
    {
      "query_id": "nixos_context7_000",
      "query": "I’ve been banging my head trying to get a Flask-based web app running in a totally reproducible way across our team’s setups. We need Python 3.10, Flask itself, Redis, and Docker all coming from the same Nix channel (we’re on 25.05), plus config snippets that play nicely with Home Manager on Linux laptops and nix-darwin on macOS. On top of that, my lead wants a tiny excerpt—like 500 words or so—on how Flask routing works to stick in our README. \n\nWhat would really help is if you could pull together:\n• A quick snapshot of the 25.05 channel (how big it is, broadly speaking)  \n• The exact Nix package names and versions for python3, flask, redis, and docker, ideally with the commit or revision that pins them  \n• The main Home Manager options we should set for Python and Docker, with their descriptions  \n• The equivalent nix-darwin settings so my mac-using teammate can just drop them in  \n• Whether there’s a Poetry flake out there we can lean on (or a note if none exist)  \n• And finally, about 500 tokens’ worth of official Flask routing docs so I can paste it straight into our project guide  \n\nIf you could wrap all of that up as a single JSON I can hand off to my team, that’d save me hours of guesswork—and give me the hard data I need to prove this setup will actually work everywhere. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "NixOS+Context7",
      "servers": [
        "NixOS",
        "Context7"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "Car Price Evaluator",
        "FruityVice",
        "Hugging Face",
        "NASA Data",
        "National Parks",
        "OSINT Intelligence",
        "Scientific Computing",
        "Unit Converter"
      ],
      "combination_name": "Dev Environment"
    },
    {
      "query_id": "nixos_context7_001",
      "query": "Audit and document the MongoDB service setup on NixOS (unstable) and fetch its upstream indexing documentation. Perform the following steps in sequence:\n\n1. List all available NixOS channels and confirm the unstable channel is active.\n2. Retrieve statistics for the unstable channel (package and option counts).\n3. Search for the ‘mongodb’ package in the unstable channel and identify the exact package name and latest version.\n4. Get detailed info on the ‘mongodb’ package to extract its version string.\n5. Use NixHub to find the commit hash corresponding to that exact MongoDB version for reproducible builds.\n6. Browse Home Manager options with the prefix ‘services.mongodb’ to determine how to enable the MongoDB service in home configurations.\n   • If no Home Manager options are found, fall back to listing nix-darwin options with the same prefix.\n7. Resolve the Context7 library ID for ‘mongodb’ to locate the official MongoDB documentation endpoint.\n8. Fetch up to 2,000 tokens of the ‘indexing’ section from the MongoDB docs via Context7.\n\nExpected output format:\n- A summary table of channels and the unstable channel status.\n- Unstable channel stats (total packages/options).\n- The exact MongoDB package name and version.\n- The NixHub commit hash for that version.\n- A list of available Home Manager options for ‘services.mongodb.enable’ (or fallback nix-darwin options).\n- The Context7 library ID chosen and a 2000-token excerpt of the ‘indexing’ topic from MongoDB docs.",
      "category": "two_server_combinations",
      "ground_truth_tool": "NixOS+Context7",
      "servers": [
        "NixOS",
        "Context7"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "FruityVice",
        "Google Maps",
        "Hugging Face",
        "Math MCP",
        "OKX Exchange",
        "OSINT Intelligence",
        "Scientific Computing",
        "Wikipedia"
      ],
      "combination_name": "Dev Environment"
    },
    {
      "query_id": "nixos_context7_001",
      "query": "Hey, I’m setting up a NixOS box on the unstable branch for a project that needs MongoDB, but I’m not even 100% sure I’m on unstable right now. I’d love to double-check which channels are active and get a feel for how big the unstable channel is these days (packages and options count, roughly). After that, I want to know exactly which MongoDB package is shipped there today, grab its precise version string, and pin down the commit hash behind that build so I can keep things fully reproducible. \n\nOnce I know that, I need to enable MongoDB via Home Manager—though if there isn’t a `services.mongodb` option there I might have to fall back to checking nix-darwin. And finally, I really need a solid excerpt (around two thousand tokens or so) from the official MongoDB docs on indexing. \n\nI can’t walk into a planning meeting with “it should work”—I need actual numbers, commands or logs, commit IDs, and a real doc snippet. Can you pull all that together?\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "NixOS+Context7",
      "servers": [
        "NixOS",
        "Context7"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "FruityVice",
        "Google Maps",
        "Hugging Face",
        "Math MCP",
        "OKX Exchange",
        "OSINT Intelligence",
        "Scientific Computing",
        "Wikipedia"
      ],
      "combination_name": "Dev Environment"
    },
    {
      "query_id": "google_maps_weather_data_000",
      "query": "You are planning a one-day walking tour in Seattle, starting from your hotel in Downtown Seattle. You must use Google Maps and Weather Data tools to build a complete itinerary, select the optimal travel day based on forecast, and verify routes by both walking and driving. Follow these steps and return a final JSON with sections: “coffee_shops” (top 3 with name, address, rating), “selected_day” (date and weather summary), “current_weather” (temperature, conditions, humidity, wind), “itinerary” (ordered list of legs with origin, destination, mode, departure_time, duration), and “viewpoint” (formatted_address, elevation_meters):\n\n1. Geocode “Downtown Seattle” to get starting coordinates.\n2. Search nearby with keyword “Pike Place Market” within a 2000 m radius of those coordinates.\n3. Get place details for the Pike Place Market placeId.\n4. From the Pike Place coordinates, search nearby with keyword “coffee shop”, radius 500 m, minRating 4.5.  \n5. From that list, select the top 3 by rating and get place details for each.\n6. Use Weather Data search_locations_tool with query “Seattle” to confirm the city name.\n7. Call get_current_weather_tool and get_live_temp for “Seattle” to cross-validate current temperature and conditions.\n8. Call get_weather_forecast_tool for “Seattle” for the next 7 days. From the returned daily forecasts, pick the first day with zero precipitation probability and average temperature between 15 °C and 25 °C. If no day meets both, choose the day with the lowest precipitation probability.\n9. Geocode “Kerry Park viewpoint, Seattle” to get viewpoint coordinates.\n10. For the chosen travel day:\n   a. Get walking directions from your hotel (Downtown Seattle coords) to Pike Place Market, departing at 09:00 local time on that day.\n   b. Get walking directions from Pike Place Market to the highest-rated coffee shop, departing at 09:30 on that day.\n   c. Get walking directions from that coffee shop to Kerry Park viewpoint, departing at 10:00 on that day.\n11. Get a driving distance matrix from your hotel to Kerry Park viewpoint to compare driving duration.\n12. Get elevation for the Kerry Park viewpoint coordinates.\n13. Reverse geocode the viewpoint coordinates to obtain its formatted address.\n\nReturn a single JSON object with keys: “coffee_shops”, “selected_day”, “current_weather”, “itinerary”, and “viewpoint”.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Google Maps+Weather Data",
      "servers": [
        "Google Maps",
        "Weather Data"
      ],
      "distraction_servers": [
        "Context7",
        "DEX Paprika",
        "FruityVice",
        "Math MCP",
        "Medical Calculator",
        "NASA Data",
        "National Parks",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Unit Converter"
      ],
      "combination_name": "Location Services"
    },
    {
      "query_id": "google_maps_weather_data_000",
      "query": "I’m heading to Seattle for just one day soon and staying right in downtown. I really want to start my morning at Pike Place Market, grab coffee at the highest-rated spot nearby, then finish up at that famous skyline overlook (you know, the one everyone snaps on Instagram). \n\nProblem is, I’m not sure which day in the next week will give me a dry window with temps around 15–25 °C. And I’d like to know what Seattle’s weather looks like right now, too—temperature, humidity, wind, all that. \n\nOnce we’ve nailed down the best day, could you sketch out a walking plan? Something like leaving my hotel around 9 am to Pike Place, then strolling over to the top coffee shop around 9:30, and then hiking up to Kerry Park by 10. I’d also love to see how those walking times compare to just driving straight from the hotel to Kerry Park, purely for kicks. And finally, what’s the exact address of that viewpoint and its elevation above sea level?\n\nI really need actual numbers and solid details—can’t go showing up with just guesses!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Google Maps+Weather Data",
      "servers": [
        "Google Maps",
        "Weather Data"
      ],
      "distraction_servers": [
        "Context7",
        "DEX Paprika",
        "FruityVice",
        "Math MCP",
        "Medical Calculator",
        "NASA Data",
        "National Parks",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Unit Converter"
      ],
      "combination_name": "Location Services"
    },
    {
      "query_id": "google_maps_weather_data_001",
      "query": "Using only the provided tools, plan a one-day food and sightseeing itinerary in downtown Seattle that includes:\n\n1. Morning coffee: find the top-rated cafe (rating ≥4.5) currently open within 1 km of “downtown Seattle.”\n2. Midday lunch: from that cafe’s location, find a restaurant (rating ≥4.0) within a 15-minute walk (≤1 km) that is open at 12:00 noon, and select the highest-rated one.\n3. Afternoon sightseeing: from the chosen restaurant’s location, find the two highest-rated tourist attractions (rating ≥4.5) within 2 km.\n4. Weather check: fetch today’s weather forecast for Seattle. If precipitation is forecasted, plan the afternoon sightseeing on foot; otherwise plan by bicycle.\n5. Directions: generate turn-by-turn directions for\n   a. the walking route from the cafe to the restaurant,\n   b. the afternoon route from the restaurant to attraction #1,\n   c. the afternoon route from attraction #1 to attraction #2,\n   using the chosen travel mode.\n6. Elevation analysis: get elevation for the cafe, restaurant, and both attractions, compute the elevation gain for each leg (cafe→restaurant, restaurant→attraction #1, attraction #1→attraction #2), and flag any segment with a gain >50 m.\n\nReturn a final itinerary in JSON with place names, addresses, ratings, opening hours at the relevant times, travel times, chosen travel mode, weather summary, directions steps, elevation values, and any elevation-gain warnings.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Google Maps+Weather Data",
      "servers": [
        "Google Maps",
        "Weather Data"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "Car Price Evaluator",
        "FruityVice",
        "Huge Icons",
        "Movie Recommender",
        "NixOS",
        "OKX Exchange",
        "Paper Search",
        "Scientific Computing"
      ],
      "combination_name": "Location Services"
    },
    {
      "query_id": "google_maps_weather_data_001",
      "query": "Hey, I’m heading to Seattle this weekend with just one full day to explore downtown, and I’d love your help piecing together the perfect itinerary. I’d like to start my morning at a really top-rated coffee shop that’s actually open when I arrive—ideally within about a kilometer of the city center—and then around noon stroll over to the best lunch spot you can find within a 15-minute walk of that café. After lunch, I want to spend the afternoon hitting two must-see attractions, both highly rated and within a couple of kilometers of the restaurant.\n\nI’m also debating whether to walk or rent a bike for the afternoon, so could you check today’s weather forecast and recommend the best travel mode? And because I’m not a fan of brutal uphill battles, it’d be awesome if you could give me turn-by-turn directions for each leg and flag any climbs over roughly 50 meters in elevation. \n\nCould you send me a detailed plan—place names, addresses, ratings, opening hours at the times I need them, travel times, chosen travel mode, a quick weather summary, step-by-step directions, elevation numbers, and any warning about steep stretches? I really need solid numbers and facts, since I’m going to follow it exactly.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Google Maps+Weather Data",
      "servers": [
        "Google Maps",
        "Weather Data"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "Car Price Evaluator",
        "FruityVice",
        "Huge Icons",
        "Movie Recommender",
        "NixOS",
        "OKX Exchange",
        "Paper Search",
        "Scientific Computing"
      ],
      "combination_name": "Location Services"
    },
    {
      "query_id": "dex_paprika_okx_exchange_000",
      "query": "Conduct a multi-network arbitrage analysis for the token 'UNI' by executing the following steps:\n\n1. Call DEX Paprika:getNetworks to confirm supported networks.\n2. Search for the token 'UNI' across all networks using DEX Paprika:search with {\"query\": \"UNI\"}. Extract the tokenAddress for each network from the search results.\n3. Filter down to exactly the 'ethereum', 'polygon', and 'binance-smart-chain' networks and their corresponding UNI tokenAddress values.\n4. For each of these three networks:\n   a. Call DEX Paprika:getTokenDetails with the network and tokenAddress to confirm token metadata.\n   b. Call DEX Paprika:getTokenPools with network, tokenAddress, page=0, limit=5, sort='desc', orderBy='volume_usd' to retrieve the top 5 liquidity pools trading UNI.\n   c. For each of the top 5 pools returned:\n      i. Call DEX Paprika:getPoolDetails with network and poolAddress to fetch current pool price_usd.\n     ii. Call DEX Paprika:getPoolOHLCV with network, poolAddress, start='past 7 days', interval='24h', limit=7 to retrieve daily OHLCV for the past week.\n    iii. In parallel, call OKX Exchange:get_price with {\"instrument\": \"UNI-USDT\"} to get the latest spot price, and OKX Exchange:get_candlesticks with {\"instrument\": \"UNI-USDT\", \"bar\": \"1D\", \"limit\": 7} to retrieve the last 7 daily candlesticks.\n     iv. Compute the average pool close price from the 7 OHLCV points and the average OKX close price from candlesticks. Calculate the average percentage price difference between each pool and OKX.\n      v. If the average percentage price difference exceeds 2%, call DEX Paprika:getPoolTransactions with network, poolAddress, page=0, limit=20 to gather the 20 most recent transactions for liquidity depth analysis.\n5. Aggregate all findings into a JSON table with fields: network, poolAddress, DEX identifier, average pool price (USD), average OKX price (USD), average percentage difference, and average swap size in USD (if transaction analysis was triggered).\n6. Highlight all pools where the average percentage difference is above 2%.",
      "category": "two_server_combinations",
      "ground_truth_tool": "DEX Paprika+OKX Exchange",
      "servers": [
        "DEX Paprika",
        "OKX Exchange"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "Car Price Evaluator",
        "Game Trends",
        "Math MCP",
        "NixOS",
        "Paper Search",
        "Scientific Computing",
        "Unit Converter",
        "Wikipedia"
      ],
      "combination_name": "Crypto Trading"
    },
    {
      "query_id": "dex_paprika_okx_exchange_000",
      "query": "Hey, I’m looking into whether there’s any easy arbitrage for UNI right now on its main chains—Ethereum, Polygon, and BSC—over the past week. I know some pools on those networks can trade a bit above or below what UNI goes for on OKX’s UNI-USDT pair, but I’m not sure how big those gaps really are. Could you pull the biggest UNI pools by volume on each chain, figure out their average daily closing price for the last seven days, and then compare that to OKX’s average daily close? If any of those pools are drifting more than about 2% from OKX, I’d also like to see what kind of trade sizes or liquidity they’ve actually seen recently so I can tell if there’s enough depth to make a move. I need real numbers—average pool price, average OKX price, percent differences, and any swap-size or volume details for the outliers—because I’m trying to build a quick, data-driven arbitrage playbook. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "DEX Paprika+OKX Exchange",
      "servers": [
        "DEX Paprika",
        "OKX Exchange"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "Car Price Evaluator",
        "Game Trends",
        "Math MCP",
        "NixOS",
        "Paper Search",
        "Scientific Computing",
        "Unit Converter",
        "Wikipedia"
      ],
      "combination_name": "Crypto Trading"
    },
    {
      "query_id": "dex_paprika_okx_exchange_001",
      "query": "You are tasked with performing a deep liquidity and price correlation analysis for the leading Ethereum-based DEX pool and cross-validating its price dynamics against OKX market data.  Follow these steps exactly:\n\n1. Retrieve high-level DEX ecosystem statistics.\n2. List all supported networks and confirm Ethereum is available.\n3. On Ethereum, fetch all DEXes and identify the one with the highest total pool count.\n4. For that DEX, retrieve its pools sorted by USD volume (descending) and select the top pool.\n5. Get detailed information for that top pool, including the two constituent token addresses.\n6. For each token in the pool, fetch full token details (symbol, decimals).\n7. Pull the pool’s OHLCV history for the past 30 days with 24h intervals.\n8. On OKX Exchange, determine the appropriate instrument symbol by combining the two token symbols (e.g., TOKENA-TOKENB).\n9. Fetch 1-day interval candlesticks for that instrument over the past 30 days.\n10. Compute the Pearson correlation coefficient between the pool’s closing price (from OHLCV) and OKX’s closing price series.\n11. Identify any days where the price divergence (absolute difference) exceeded 2% of the OKX closing price.\n12. Retrieve the latest 10 transactions for the pool and summarize the volume and transaction types per day.\n\nProduce a JSON report containing:\n- ecosystemStats: result of step 1\n- chosenNetwork: network ID used\n- chosenDex: DEX ID and name with highest pool count\n- topPool: address, volume_usd, token0, token1\n- tokenDetails: array of both token detail objects\n- poolOhlcv: full 30-day OHLCV array\n- okxCandles: full 30-day candlestick array\n- priceCorrelation: correlation coefficient\n- divergenceDays: array of dates with >2% divergence and percent difference\n- recentTransactions: array of the 10 transaction objects grouped by date with totals",
      "category": "two_server_combinations",
      "ground_truth_tool": "DEX Paprika+OKX Exchange",
      "servers": [
        "DEX Paprika",
        "OKX Exchange"
      ],
      "distraction_servers": [
        "BioMCP",
        "Car Price Evaluator",
        "Math MCP",
        "Medical Calculator",
        "Metropolitan Museum",
        "NASA Data",
        "NixOS",
        "Reddit",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "Crypto Trading"
    },
    {
      "query_id": "dex_paprika_okx_exchange_001",
      "query": "Hey, I’ve been digging into DeFi for a project and I’m trying to figure out if a major liquidity pool on Ethereum really moves in step with the same pair on OKX. I’d love to get a feel for the overall DEX scene (make sure Ethereum’s in there), see which Ethereum exchange has the most pools, and then home in on that single biggest pool by dollar volume. Once we’ve got that, I want the token pair info, plus a daily on-chain price series for the past month, alongside OKX’s daily candles for the same pair. From there I’m hoping to crunch a correlation coefficient and call out any days where the on-chain price was off by more than 2%. And as a final touch, a quick rundown of the last ten swaps in that pool—volumes and types grouped by day—would really seal the deal. I need real numbers and dates so I can show my team something solid, not just gut feelings. Does that make sense?\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "DEX Paprika+OKX Exchange",
      "servers": [
        "DEX Paprika",
        "OKX Exchange"
      ],
      "distraction_servers": [
        "BioMCP",
        "Car Price Evaluator",
        "Math MCP",
        "Medical Calculator",
        "Metropolitan Museum",
        "NASA Data",
        "NixOS",
        "Reddit",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "Crypto Trading"
    },
    {
      "query_id": "metropolitan_museum_wikipedia_000",
      "query": "Investigate all Claude Monet paintings tagged “Impressionism” in the European Paintings department at the Metropolitan Museum. First, list departments and identify the departmentId for “European Paintings.” Then search for objects with q=\"Impressionism\", departmentId=<found id>, hasImages=true. Retrieve each object’s metadata and image, filter to those where artistDisplayName is “Claude Monet.” For each Monet painting, search Wikipedia for the painting title (fallback to \"painting title + Claude Monet\" if no direct match). For the matched article, get a general summary, a summary focused on “Impressionism,” and a summary of the “Composition” section. Extract the top 3 key facts about composition techniques. Cross-validate the objectDate and medium from the museum metadata against information in the Wikipedia summaries. Produce a report listing for each painting: title, objectDate, medium, image availability, matched Wikipedia article title, general summary, Impressionism-focused summary, composition section summary, 3 key composition facts, and a validation flag indicating whether date and medium match between sources.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Metropolitan Museum+Wikipedia",
      "servers": [
        "Metropolitan Museum",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "Context7",
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Huge Icons",
        "Hugging Face",
        "NASA Data",
        "OKX Exchange"
      ],
      "combination_name": "Cultural Knowledge"
    },
    {
      "query_id": "metropolitan_museum_wikipedia_000",
      "query": "I’ve been putting together a little talk on Monet’s Impressionist works and I’m really curious about what the Met has in its European Paintings section. I’m not even sure how many of his pieces there are tagged as “Impressionism” and have images you can actually look at, so could you dig into that catalog for me? Then, for each Monet painting you find, I’d love if you could hunt down the matching Wikipedia entry (and if the title alone doesn’t link, try “painting title + Claude Monet”), grab a quick overview of the work, anything the article says specifically about its Impressionist style, and whatever it says under “Composition.” From that last bit, could you pull out the top three compositional techniques Monet used? Oh, and one more thing—please double-check that the date and medium the Met lists match what’s on Wikipedia. I need solid, sourced details for my presentation—no guessing, just real metadata and Wikipedia citations, okay?\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Metropolitan Museum+Wikipedia",
      "servers": [
        "Metropolitan Museum",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "Context7",
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Huge Icons",
        "Hugging Face",
        "NASA Data",
        "OKX Exchange"
      ],
      "combination_name": "Cultural Knowledge"
    },
    {
      "query_id": "metropolitan_museum_wikipedia_001",
      "query": "Evaluate the representation and historical context of New Kingdom Egyptian chairs in the Metropolitan Museum of Art and compare their materials and design features against scholarly descriptions from Wikipedia.\n\nSteps:\n1. List all Met Museum departments to find the ID for “Egyptian, Classical, Ancient Near Eastern Art.”\n2. Search that department for objects with query “chair” and images.\n3. Retrieve details for the first three matching chairs and extract their “Period” and “Materials.”\n4. Filter those chairs by “New Kingdom” in their period. If fewer than two qualify, repeat search in the same department for “stool” and “footrest” with images, and gather until two New Kingdom items are found.\n5. From the two New Kingdom chairs, select the one with the richest materials list as the representative artifact.\n6. Search Wikipedia for “Ancient Egyptian furniture” (limit 5) and select the article titled “Ancient Egyptian furniture.”\n7. Get the summary of that article.\n8. Extract the top five key facts focused on “New Kingdom” furniture.\n9. Retrieve the sections of the article and summarize the “Construction and materials” section (max_length 150).\n10. Get up to five related topics for further scholarly context.\n11. Compare the materials list of the Met chair against the Wikipedia key facts materials. Flag any materials present in the artifact but not mentioned in the Wikipedia facts as potential anomalies.\n\nProduce a structured report including:\n- Department name and ID\n- List of candidate chairs (ID, title, period, materials)\n- Selected representative chair (with image URL) and its metadata\n- Wikipedia summary\n- Extracted key facts on New Kingdom furniture\n- Summarized “Construction and materials” section\n- Related topics\n- Comparison table of materials with anomalies flagged",
      "category": "two_server_combinations",
      "ground_truth_tool": "Metropolitan Museum+Wikipedia",
      "servers": [
        "Metropolitan Museum",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "DEX Paprika",
        "Game Trends",
        "Hugging Face",
        "Medical Calculator",
        "Movie Recommender",
        "OSINT Intelligence",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "Cultural Knowledge"
    },
    {
      "query_id": "metropolitan_museum_wikipedia_001",
      "query": "I’m putting together a small art-history spotlight on seating in New Kingdom Egypt—specifically what’s on view at the Met—and I’m a bit stuck on how to pull everything together. My professor wants me to pick out an example piece from the Met’s Egyptian section, but I’m not even sure what they call that department or how to find chairs with pictures in their collection. Once I have a few candidates, I need to know their dates (make sure they’re really New Kingdom) and exactly what they’re made of. If there aren’t enough chairs, I might have to slip in a stool or footrest to hit at least two examples, and then choose the one with the most elaborate materials list as my main focus.\n\nAfter that, I have to see what Wikipedia says about Ancient Egyptian furniture—grab the article summary, pull out the top five insights specifically about New Kingdom pieces, and boil down the “Construction and materials” bit into a quick blurb. It’d also help to know a handful of related topics I could mention for extra context. Finally, I need to check if my chosen Met object uses any materials that don’t show up in those Wikipedia facts—those could be neat anomalies to point out.\n\nI really need actual Met IDs, image links, periods, materials lists, the Wikipedia summary, key New Kingdom facts, that short construction/materials paragraph, related topics, and a note on any unmatched materials. Can you help me track it all down? I can’t go to my professor with guesses—gotta have real data or solid sources.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Metropolitan Museum+Wikipedia",
      "servers": [
        "Metropolitan Museum",
        "Wikipedia"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "DEX Paprika",
        "Game Trends",
        "Hugging Face",
        "Medical Calculator",
        "Movie Recommender",
        "OSINT Intelligence",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "Cultural Knowledge"
    },
    {
      "query_id": "scientific_computing_math_mcp_000",
      "query": "You are given a 3×3 covariance matrix for three financial assets and a 3×1 expected returns vector. Perform the following steps:\n1. Create the covariance matrix named “cov_matrix” with values [0.04, 0.006, 0.014, 0.006, 0.09, 0.02, 0.014, 0.02, 0.16].\n2. Create the expected returns vector named “exp_returns” with values [0.08, 0.12, 0.10].\n3. View “cov_matrix” to verify its contents.\n4. Compute the determinant of “cov_matrix”.\n   • If the determinant is zero, scale “cov_matrix” in place by a factor of 1.001 and recompute the determinant until it is non-zero.\n5. Compute eigenvalues and right eigenvectors of the (possibly scaled) “cov_matrix”.\n6. Identify the largest and smallest eigenvalues, then compute the condition number = (largest)/(smallest) using Math MCP division.\n   • If the condition number > 100, compute the SVD of “cov_matrix”, then assemble its pseudoinverse by computing 1/singular values (Math MCP division), forming a diagonal matrix, and multiplying V^T, diagonal, and U^T via multiply_matrices and transpose.\n   • Otherwise, compute the regular matrix inverse of “cov_matrix”.\n7. Multiply the inverse or pseudoinverse by “exp_returns” to compute the portfolio weight vector “weights”.\n8. Project “exp_returns” onto the principal eigenvector (the eigenvector corresponding to the largest eigenvalue) using vector_project, naming the result “proj_return”.\n9. Cross-validate that the weights sum to 1 by taking the dot product of “weights” with a ones vector [1,1,1] using vector_dot_product.\n\nProvide:\n- The nonzero determinant after any scaling.\n- The condition number.\n- Whether you used inverse or pseudoinverse.\n- The final inverse or pseudoinverse matrix.\n- The weight vector “weights”.\n- The projected return “proj_return”.\n- The dot-product sum of weights.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Scientific Computing+Math MCP",
      "servers": [
        "Scientific Computing",
        "Math MCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Context7",
        "FruityVice",
        "Hugging Face",
        "Movie Recommender",
        "NASA Data",
        "National Parks",
        "OpenAPI Explorer",
        "Reddit",
        "Wikipedia"
      ],
      "combination_name": "Science Tools"
    },
    {
      "query_id": "scientific_computing_math_mcp_000",
      "query": "I’m working on a mini portfolio analysis for a class project and could use some help untangling the math. I’ve got three assets with expected returns of 0.08, 0.12 and 0.10, and I estimated their covariance matrix as:\n\n[0.04 0.006 0.014  \n 0.006 0.09 0.02  \n 0.014 0.02 0.16]\n\nWhen I peeked at the determinant, I worried it might be zero or really small, so I thought I might gently bump the whole matrix by 0.1% until it’s safely nonzero. After that, I’d like to get its eigenvalues and eigenvectors, figure out the largest and smallest eigenvalue, and compute the condition number. If it turns out to be over 100, I’ll need to go the SVD route and build a pseudoinverse; otherwise a regular inverse should do. Once I’ve got whichever inverse is appropriate, I want to multiply it by the return vector [0.08, 0.12, 0.10] to see what portfolio weights pop out. I’m also curious to project the return vector onto the principal eigenvector (the one tied to the biggest eigenvalue) and then verify my weights sum to 1 by dotting them with [1,1,1].\n\nCould you walk me through all of that and give me the actual numbers? Specifically:  \n• The nonzero determinant after any tiny scaling  \n• The condition number  \n• Whether you ended up using an inverse or a pseudoinverse  \n• The full inverse (or pseudoinverse) matrix  \n• The final weight vector  \n• The projected return onto that top eigenvector  \n• And the dot‐product sum of the weights  \n\nI really need concrete figures—no hand-waving—because I have to show this to my professor and can’t just say “it works out.” Thanks!",
      "category": "two_server_combinations",
      "ground_truth_tool": "Scientific Computing+Math MCP",
      "servers": [
        "Scientific Computing",
        "Math MCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Context7",
        "FruityVice",
        "Hugging Face",
        "Movie Recommender",
        "NASA Data",
        "National Parks",
        "OpenAPI Explorer",
        "Reddit",
        "Wikipedia"
      ],
      "combination_name": "Science Tools"
    },
    {
      "query_id": "scientific_computing_math_mcp_001",
      "query": "You are given two 3×3 matrices A and B and a 3-element vector v:\n\n• Matrix A: [[4, 2, 1], [0, 3, -1], [2, 0, 1]]\n• Matrix B: [[1, 0, 2], [0, 2, 0], [1, 0, 1]]\n• Vector v: [1, 2, 3]\n\nPerform the following steps, using the provided Scientific Computing and Math MCP tools in sequence:\n\n1. Create tensors A, B, and v in the tensor store.\n2. Compute C = A + B (element-wise addition).\n3. Compute D = C – A (element-wise subtraction).\n4. Compute the inverse of B, B_inv = B⁻¹.\n5. Compute E = D @ B_inv (matrix multiplication).\n6. Compute the eigenvalues and eigenvectors of E.\n7. Perform SVD on E to obtain its singular values.\n8. Compute the condition number κ(E) = (largest singular value)/(smallest singular value) using Math MCP division.\n9. If κ(E) > 5.0, scale E in place by factor α = 1/κ(E) (so the new condition number ≤ 1). Otherwise, leave E unchanged. Use Math MCP division to compute α and Scientific Computing scale_matrix to apply it.\n10. Rename or view the (possibly) scaled matrix as E_scaled. Perform a fresh SVD on E_scaled and recompute κ(E_scaled) to confirm κ(E_scaled) ≤ 5.0.\n11. Change the basis of E_scaled into its eigenvector basis (the Q from the eigen decomposition) using change_basis.\n12. Project the original vector v onto the first eigenvector of E using vector_project.\n13. Compute the dot product between that projection and the original v using vector_dot_product.\n14. Compute the rank and determinant of E_scaled.\n15. Aggregate and return a JSON object with:\n   • eigenvalues (list), eigenvectors (matrix)\n   • original κ(E) and new κ(E_scaled)\n   • whether scaling was applied and the scale factor α\n   • the new basis representation of E_scaled\n   • the projection vector and dot product result\n   • the rank and determinant of E_scaled\n\nThe task must be executed step-by-step, respecting each tool’s input/output dependencies and performing the conditional scaling branch exactly once based on the computed condition number.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Scientific Computing+Math MCP",
      "servers": [
        "Scientific Computing",
        "Math MCP"
      ],
      "distraction_servers": [
        "BioMCP",
        "Car Price Evaluator",
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Google Maps",
        "Metropolitan Museum",
        "National Parks",
        "Paper Search",
        "Weather Data"
      ],
      "combination_name": "Science Tools"
    },
    {
      "query_id": "scientific_computing_math_mcp_001",
      "query": "I’m working on a linear algebra exercise for my thesis advisor and could really use some hard numbers. I’ve got two 3×3 matrices—one is  \n[4, 2, 1; 0, 3, –1; 2, 0, 1]  \nand the other is  \n[1, 0, 2; 0, 2, 0; 1, 0, 1]—plus the vector [1, 2, 3].  \n\nWhat I’m trying to do is mix those matrices together, invert that second one, multiply it all out, and then dig into its eigenvalues, eigenvectors, and singular values so I can calculate the condition number. If that condition number comes out above 5.0, I need to scale the matrix by exactly 1 over that number to stabilize it, then check again to make sure it’s under 5. After that I want to switch into the eigenvector basis of the scaled matrix, project my [1, 2, 3] vector onto the top eigenvector, take the dot product with the original [1, 2, 3], and finish by finding the rank and determinant of the scaled matrix.  \n\nCould you walk me through all the key results—every intermediate matrix (sum, inverse, etc.), the eigen- and singular-value details, the original and (if needed) adjusted condition numbers with the exact scaling factor, the basis-transformed matrix, the projection vector, the dot product, plus the final rank and determinant—so I’ve got real data to back up my write-up?\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Scientific Computing+Math MCP",
      "servers": [
        "Scientific Computing",
        "Math MCP"
      ],
      "distraction_servers": [
        "BioMCP",
        "Car Price Evaluator",
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Google Maps",
        "Metropolitan Museum",
        "National Parks",
        "Paper Search",
        "Weather Data"
      ],
      "combination_name": "Science Tools"
    },
    {
      "query_id": "hugging_face_paper_search_001",
      "query": "A natural language processing researcher needs to select a lightweight, open‐license transformer for named entity recognition (NER) and pair it with a well‐established NER dataset. Then they want to confirm the dataset’s popularity in recent publications and finally find an interactive Hugging Face Space for live benchmarking. Do the following:\n\n1. Use Hugging Face:search-models with query=\"ner\" and tags=\"token-classification\", limit=10 to retrieve up to 10 candidate NER models.\n2. For each returned model_id, call Hugging Face:get-model-info and filter to those with license exactly \"apache-2.0\" and weight_size (in bytes) less than 2_000_000_000 (≈2 GB).\n3. Use Hugging Face:search-datasets with query=\"ner\" and tags=\"token-classification\", limit=5 to retrieve up to 5 NER datasets.\n4. For each returned dataset_id, call Hugging Face:get-dataset-info and record its total number of examples (sum of all splits).\n5. Select the dataset with the highest total number of examples; call it SELECTED_DATASET.\n6. Cross-validate SELECTED_DATASET’s popularity:\n   a. Call Paper Search:search_arxiv with query=\"SELECTED_DATASET named entity recognition\" and max_results=5.  \n   b. For each returned paper, call Paper Search:read_arxiv_paper to extract text, count how many papers mention SELECTED_DATASET exactly by its dataset_id.  \n   c. If fewer than 3 papers mention SELECTED_DATASET, call Paper Search:search_pubmed with the same query and max_results=5 and count mentions there.  \n   d. Total publication mentions = arXiv_mentions + PubMed_mentions (if any).\n7. Using SELECTED_DATASET, call Hugging Face:search-spaces with query=\"ner-evaluation\" , sdk=\"gradio\" , limit=3 to find interactive demos.\n8. For the top returned space_id, call Hugging Face:get-space-info to retrieve its URL and description.\n\nProduce a final JSON report with keys:\n• selected_models: list of { model_id, license, weight_size }  \n• selected_dataset: { dataset_id, total_examples }  \n• publication_mentions: integer (total from arXiv and PubMed)  \n• interactive_space: { space_id, sdk, url }",
      "category": "two_server_combinations",
      "ground_truth_tool": "Hugging Face+Paper Search",
      "servers": [
        "Hugging Face",
        "Paper Search"
      ],
      "distraction_servers": [
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Math MCP",
        "Medical Calculator",
        "NASA Data",
        "NixOS",
        "OpenAPI Explorer",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "AI Research"
    },
    {
      "query_id": "hugging_face_paper_search_001",
      "query": "I’m knee-deep in a little side project on named entity recognition for my thesis and hitting a wall. I’d love to pick a really lightweight transformer—something under about 2 GB so it doesn’t swallow my laptop—and it has to be full open-license so I can share everything. Then I want to use a “classic” NER dataset with plenty of examples to give my results some weight. On top of that, I’m curious how many recent papers actually mention whichever dataset I choose—you know, to prove it’s still a popular benchmark. Finally, it’d be amazing to play around with a live demo in the browser so I can sanity-check my setup. \n\nCan you walk me through your top recommendations? I’m after the model name/ID, its license type and file size, the dataset name/ID with total example count, a ballpark of how often it’s been cited in the past few months, plus a link to an interactive demo. Really need real numbers and solid sources—I can’t bring vague guesses to my advisor!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Hugging Face+Paper Search",
      "servers": [
        "Hugging Face",
        "Paper Search"
      ],
      "distraction_servers": [
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Math MCP",
        "Medical Calculator",
        "NASA Data",
        "NixOS",
        "OpenAPI Explorer",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "AI Research"
    },
    {
      "query_id": "hugging_face_paper_search_002",
      "query": "Develop a production-ready sentiment analysis pipeline for COVID-19 clinical trial reports using Hugging Face Hub resources and academic paper sources. Execute the following steps in order:\n\n1. Use Hugging Face search-models with {\"query\":\"sentiment-analysis\",\"author\":\"huggingface\",\"tags\":\"text-classification\",\"limit\":5} to find 5 candidate models.\n2. From the returned list, take the top 3 model_ids and call get-model-info on each to retrieve model_size (GB), inference_speed (examples/sec), and architecture details.\n3. Use Hugging Face search-datasets with {\"query\":\"clinical trial\",\"tags\":\"text-classification\",\"limit\":3} to find 3 candidate datasets.\n4. For the top 2 dataset_ids, call get-dataset-info to obtain num_examples and label_distribution.\n5. If either dataset has fewer than 1000 examples, re-run search-datasets with {\"query\":\"medical reports\",\"tags\":\"text-classification\",\"limit\":2} to find additional datasets, then call get-dataset-info on those until you have at least two datasets each ≥1000 examples.\n6. Merge the selected datasets and verify total num_examples ≥2000. If still below 2000, perform one more search-datasets with {\"query\":\"healthcare reports\",\"tags\":\"text-classification\",\"limit\":2} and repeat get-dataset-info.\n7. Use Hugging Face search-collections with {\"owner\":\"nvidia\",\"item\":\"datasets\",\"query\":\"covid19\",\"limit\":2} to find up to 2 collections. For each, call get-collection-info to extract included dataset_ids and example counts; if a collection offers ≥500 examples, add it to the merged dataset.\n8. Call get-daily-papers to retrieve today’s list of curated papers. Filter for titles containing “COVID-19” from the past 3 months. If fewer than 5 papers meet this criterion, call Paper Search search_pubmed with {\"query\":\"COVID-19 clinical trial\",\"max_results\":10} and Paper Search search_arxiv with {\"query\":\"COVID-19 clinical trial\",\"max_results\":10} to supplement the list.\n9. For each selected paper:\n   a. If it has an arXiv ID, call download_arxiv with that ID, then read_arxiv_paper to extract full text.\n   b. If it only has a PubMed ID, call download_pubmed (note: PDF download is unsupported) and skip text extraction.\n10. Preprocess all extracted paper texts into a unified corpus of at least 5 documents.\n11. For each of the 3 candidate models from step 2, compute a suitability score = inference_speed (examples/sec) ÷ model_size (GB). Rank models by this score and select the top-scoring model_id.\n12. Use Hugging Face search-spaces with {\"query\":\"covid-sentiment-demo\",\"sdk\":\"gradio\",\"limit\":2} to find existing demo spaces. For each, call get-space-info to identify missing interface features (e.g., batch upload, live metrics).\n13. Propose a new Space configuration. Specify:\n    • space_id: a new name “covid-sentiment-pipeline”\n    • selected_model_id\n    • merged_dataset_ids (list)\n    • corpus_size (number of extracted documents)\n    • inference_config (batch_size, max_length)\n    • gradio components (text_input, sentiment_output, example_selector)\n\nExpected output: A JSON object with keys: selected_model_id, total_dataset_examples, number_of_papers_processed, final_score_ranking (list of model_ids and scores), proposed_space_name, and space_components (listing each configured component).",
      "category": "two_server_combinations",
      "ground_truth_tool": "Hugging Face+Paper Search",
      "servers": [
        "Hugging Face",
        "Paper Search"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "DEX Paprika",
        "FruityVice",
        "Google Maps",
        "NASA Data",
        "OKX Exchange",
        "OSINT Intelligence",
        "Reddit",
        "Wikipedia"
      ],
      "combination_name": "AI Research"
    },
    {
      "query_id": "hugging_face_paper_search_002",
      "query": "Hey, I’ve got to throw together a quick proof-of-concept sentiment tool for COVID-19 clinical trial reports by next week and I’m a bit stuck on the kickoff steps. Basically, I want to pick a few popular pre-trained sentiment models from the usual public hubs, compare how big they are on disk versus how many documents they can process per second, then pick the one that gives me the best speed-to-size trade-off.  \n\nNext, I need to pull together at least 2,000 trial-report texts from open datasets and supplement that with about five real COVID clinical-trial papers from the past three months—if the curated feeds don’t have enough, I’ll have to dive into preprint servers to top up. Once I’ve preprocessed everything into a mini-corpus, I want to spin up a simple web demo that can handle batch uploads and show live sentiment metrics.\n\nIn the end, I need a clear summary I can show my team:  \n• Which model I picked (with its size and throughput)  \n• Total number of data examples I’m working with  \n• How many papers made it into the corpus  \n• A ranking of the candidate models by their speed-to-size ratio  \n• A sketch of the demo’s key features/components  \n\nCould you help me pull together all those numbers and outline the final setup with real, evidence-backed figures? I really need concrete stats to convince everyone this has legs.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Hugging Face+Paper Search",
      "servers": [
        "Hugging Face",
        "Paper Search"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "DEX Paprika",
        "FruityVice",
        "Google Maps",
        "NASA Data",
        "OKX Exchange",
        "OSINT Intelligence",
        "Reddit",
        "Wikipedia"
      ],
      "combination_name": "AI Research"
    },
    {
      "query_id": "national_parks_weather_data_000",
      "query": "Using the National Parks and Weather Data tools, plan a 3-day hiking and camping trip within the next 7 days at national parks in California (CA) and Oregon (OR) that offer both hiking and camping. Execute the following sequence:\n1. Search for parks in CA and OR with activities \"hiking,camping\" (limit to 5 parks).\n2. For each park returned:\n   a. Get detailed park information to extract the nearest city name.\n   b. Retrieve current alerts to detect any closures or hazards.\n   c. List available campgrounds to confirm at least one campsite.\n   d. Find upcoming events scheduled within the next 7 days.\n   e. Obtain a 7-day weather forecast for the park’s nearest city.\n3. Exclude any park if it has active closure alerts, no campgrounds, or any forecast day with precipitation chance over 50%.\n4. For remaining parks, compute each park’s average daily high temperature over the forecast period and rank parks from coolest to warmest.\n5. Produce a final JSON itinerary of the top 3 parks, including: parkCode, park name, selected campground name(s), one highlighted event (if any), the forecast day with lowest precipitation chance (day of week and precip %), and the average high temperature.",
      "category": "two_server_combinations",
      "ground_truth_tool": "National Parks+Weather Data",
      "servers": [
        "National Parks",
        "Weather Data"
      ],
      "distraction_servers": [
        "DEX Paprika",
        "FruityVice",
        "Google Maps",
        "Hugging Face",
        "Medical Calculator",
        "Metropolitan Museum",
        "NixOS",
        "OpenAPI Explorer",
        "Reddit",
        "Scientific Computing"
      ],
      "combination_name": "Travel Weather"
    },
    {
      "query_id": "national_parks_weather_data_000",
      "query": "I’m trying to plan a quick three-day hiking and camping getaway sometime in the next week, and I can’t decide between parks in Northern California or down in Oregon. Ideally I’d like places that for sure have both trails and campsites open, zero closure alerts or hazards, and generally cool, dry weather—not something that turns into a mudfest or bakes me alive. It’d also be cool if there’s a ranger talk or small event going on, just to mix up the evenings. \n\nCould you help me narrow it down to the top three parks that meet all that? For each spot, I need to know what campground options are actually available, any active alerts to watch out for, the 7-day forecast in the closest town (so I can see which day is driest), and the average daytime high so I can rank them from coolest to warmest. I really need concrete numbers and facts—no guesses—so I can pick the best one with confidence.",
      "category": "two_server_combinations",
      "ground_truth_tool": "National Parks+Weather Data",
      "servers": [
        "National Parks",
        "Weather Data"
      ],
      "distraction_servers": [
        "DEX Paprika",
        "FruityVice",
        "Google Maps",
        "Hugging Face",
        "Medical Calculator",
        "Metropolitan Museum",
        "NixOS",
        "OpenAPI Explorer",
        "Reddit",
        "Scientific Computing"
      ],
      "combination_name": "Travel Weather"
    },
    {
      "query_id": "national_parks_weather_data_001",
      "query": "You are planning a 3-day hiking and camping trip to the top 3 national parks in Utah that offer both hiking and camping. For each park you must: 1) identify the park via `findParks` using stateCode=\"UT\" and activities=\"hiking,camping\" (limit=3); 2) get detailed information with `getParkDetails`; 3) retrieve current alerts with `getAlerts`; 4) list available campgrounds via `getCampgrounds`; 5) find upcoming events over the next 7 days with `getEvents` (dateStart=\"today\", dateEnd=\"in 7 days\"); 6) fetch visitor center details via `getVisitorCenters`; 7) extract the primary park city from the park details, normalize the city name using `search_locations_tool`, then obtain the current weather (`get_current_weather_tool` and validate temperature with `get_live_temp`) and a 7-day forecast (`get_weather_forecast_tool` with days=7) for that city. Finally, for each park, if any day in the 7-day forecast shows a precipitation probability >50%, identify indoor visitor centers by re-querying `getVisitorCenters` with q=\"indoor\"; otherwise recommend the best 3 hiking events (from the events list) that coincide with days forecasted as dry. Produce a consolidated JSON report listing for each park: park name, code, summary of alerts, top 2 campgrounds, 3-day weather outlook, recommended hiking days/events or indoor visitor center alternatives, and visitor center hours.",
      "category": "two_server_combinations",
      "ground_truth_tool": "National Parks+Weather Data",
      "servers": [
        "National Parks",
        "Weather Data"
      ],
      "distraction_servers": [
        "DEX Paprika",
        "FruityVice",
        "Google Maps",
        "NixOS",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Paper Search",
        "Reddit",
        "Scientific Computing",
        "Unit Converter"
      ],
      "combination_name": "Travel Weather"
    },
    {
      "query_id": "national_parks_weather_data_001",
      "query": "Hey, I’m planning a three-day hiking and camping trip through Utah’s three biggest parks that let you both sleep under the stars and hit some great trails. For each one, I’d love to know:\n\n• Any alerts or advisories I should watch out for  \n• Which campgrounds are really worth booking (top two, ideally)  \n• What events or ranger programs are happening in the next week  \n• The visitor center hours and whether they’ve got indoor exhibits—just in case it pours  \n• A simple three-day weather snapshot for the nearest town so I can pick my dry-day hikes  \n\nAt the end, I’d like a quick recommendation: if it looks like rain, point me to indoor visitor center options; if it’s clear, suggest the three best hiking events to join. Oh, and please give me each park’s official name and code so I can double-check details when I book. I really need solid numbers and dates—can’t just go on gut feelings here. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "National Parks+Weather Data",
      "servers": [
        "National Parks",
        "Weather Data"
      ],
      "distraction_servers": [
        "DEX Paprika",
        "FruityVice",
        "Google Maps",
        "NixOS",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Paper Search",
        "Reddit",
        "Scientific Computing",
        "Unit Converter"
      ],
      "combination_name": "Travel Weather"
    },
    {
      "query_id": "unit_converter_math_mcp_000",
      "query": "An HVAC engineer needs to calculate the total weekly heating and cooling energy required to maintain an interior set point of 22 °C inside a building with wall area 500 m² and overall heat transfer coefficient U = 0.35 W/(m²·K) for the next 7 days (starting tomorrow). The outside temperature forecast for the next 7 days is: 68 °F, 70 °F, 75 °F, 80 °F, 85 °F, 75 °F, 65 °F. Assuming the HVAC system runs 24 hours each day, perform the following steps in sequence using the available tools:\n\n1. Confirm supported units for temperature conversions and energy conversions.\n2. Convert all 7 forecasted outside temperatures from Fahrenheit to Celsius in a single batch.\n3. For each day, compute the temperature difference ΔT = |22 °C − outside °C| using Math MCP operations (subtract and, if needed, multiply negative results by −1 to get absolute values).\n4. Calculate the instantaneous heat transfer rate Q̇ (in watts) for each day using Q̇ = U × A × ΔT (use Math MCP multiplications: first U×A, then result×ΔT).\n5. Convert each Q̇ from watts to kilowatts.\n6. Compute daily energy consumption in kilowatt‐hours: daily_kWh = Q̇(kW) × 24 h.\n7. Sum the seven daily_kWh values to get total weekly energy consumption (kWh) and then compute the average daily consumption (kWh/day).\n8. Convert the total weekly energy consumption from kilowatt‐hours to megajoules and also to Btu.\n9. Compute the total weekly operating cost at a rate of USD 0.12 per kWh (use Math MCP multiplication).\n10. Provide a table listing for each day: day number (1–7), outside temperature in °C, ΔT in °C, Q̇ in kW, daily consumption in kWh. Then provide a summary of: total weekly consumption (kWh), average daily consumption (kWh/day), total in megajoules, total in Btu, and total cost in USD.\n\nNo external data is needed beyond the provided forecast and building parameters. All calculations must use the specified Unit Converter and Math MCP tools in the order defined above.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Unit Converter+Math MCP",
      "servers": [
        "Unit Converter",
        "Math MCP"
      ],
      "distraction_servers": [
        "Context7",
        "DEX Paprika",
        "Game Trends",
        "Medical Calculator",
        "Metropolitan Museum",
        "NASA Data",
        "National Parks",
        "OSINT Intelligence",
        "Scientific Computing",
        "Weather Data"
      ],
      "combination_name": "Conversion Tools"
    },
    {
      "query_id": "unit_converter_math_mcp_000",
      "query": "I’m trying to forecast the heating and cooling load for my office next week—my boss wants hard numbers. We keep the inside at 22 °C, the exterior wall area is 500 m² with a U-value of 0.35 W/(m²·K), and the seven-day temperature outlook (starting tomorrow) is 68 °F, 70 °F, 75 °F, 80 °F, 85 °F, 75 °F and 65 °F. I’m not sure how to turn those Fahrenheit readings into Celsius, figure out the daily temperature differences, calculate the heat flow in kW, then get the kWh for a full 24 hours each day, and finally sum it up for the week, find the average per day, convert that total into megajoules and Btu, and even estimate the cost at US $0.12 per kWh. Could you walk me through all that and give me a day-by-day breakdown (outside °C, ΔT, Q̇ in kW, daily kWh) plus a weekly summary of total kWh, average daily kWh, total in MJ, total in Btu, and total cost? I really need solid figures to show my manager—no guesswork, just concrete numbers.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Unit Converter+Math MCP",
      "servers": [
        "Unit Converter",
        "Math MCP"
      ],
      "distraction_servers": [
        "Context7",
        "DEX Paprika",
        "Game Trends",
        "Medical Calculator",
        "Metropolitan Museum",
        "NASA Data",
        "National Parks",
        "OSINT Intelligence",
        "Scientific Computing",
        "Weather Data"
      ],
      "combination_name": "Conversion Tools"
    },
    {
      "query_id": "unit_converter_math_mcp_001",
      "query": "You are an engineer designing a hot-water pumping and heating system. The system must heat water from 50°F inlet temperature to 120°F outlet temperature under three design flow scenarios: 150 gallon (US) per minute, 200 gallon (US) per minute, and 250 gallon (US) per minute. Perform the following steps:\n\n1. Use a single batch conversion request to convert:\n   - 50 °F → °C (inlet temperature)\n   - 120 °F → °C (outlet temperature)\n   - 150 gallon (US) → cubic meter\n   - 200 gallon (US) → cubic meter\n   - 250 gallon (US) → cubic meter\n   - 1 gram per cubic centimeter → kilograms per cubic meter (water density)\n\n2. Compute the temperature rise ΔT (°C) as outlet_C − inlet_C.\n\n3. For each flow scenario (i = 150, 200, 250 gpm):\n   a. Convert volumetric flow (m³ per minute) to m³ per second by dividing by 60.\n   b. Compute mass flow rate (kg/s) = volumetric_flow_m³/s × density_kg/m³.\n   c. Use specific heat capacity Cp = 4.186 kJ/(kg·K) to compute heat duty Q̇_i (kW) = mass_flow_kg/s × Cp_kJ/(kg·K) × ΔT_K.\n   d. Convert Q̇_i from kilowatt to horsepower.\n\n4. With the three horsepower results:\n   a. Calculate the arithmetic mean horsepower.\n   b. Calculate the median horsepower.\n   c. Determine the maximum horsepower.\n\n5. Decision: if the maximum horsepower exceeds 2 HP, specify that two identical pumps should operate in parallel; otherwise, one pump suffices.\n\n6. For each scenario, compute daily energy consumption: daily_kWh_i = Q̇_i (kW) × 24 hours. Then compute monthly energy cost over the next 30 days at $0.10 per kWh: monthly_cost_i = daily_kWh_i × 30 × 0.10.\n\n7. Compute the average monthly energy cost across the three scenarios.\n\n8. Cross-validate consumption of the 200 gpm scenario by converting its monthly_kWh from kilowatt-hour to Btu.\n\nProvide a structured report listing all intermediate values (temperature conversions, ΔT, volumetric flows, mass flows, heat duties, horsepower values, mean/median/max horsepower, pump count decision, daily_kWh, monthly_cost_i, average monthly cost, and cross-converted Btu result) and the final pump selection recommendation.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Unit Converter+Math MCP",
      "servers": [
        "Unit Converter",
        "Math MCP"
      ],
      "distraction_servers": [
        "Context7",
        "FruityVice",
        "Google Maps",
        "Huge Icons",
        "Hugging Face",
        "Movie Recommender",
        "NixOS",
        "OpenAPI Explorer",
        "Paper Search",
        "Weather Data"
      ],
      "combination_name": "Conversion Tools"
    },
    {
      "query_id": "unit_converter_math_mcp_001",
      "query": "Hey, I’m working on a hot-water pumping system and my boss is breathing down my neck for all the numbers. We’ve got water coming in at 50 °F and it needs to leave at 120 °F, and we’re looking at flow rates of 150, 200, and 250 gallons per minute. I need to see those temperatures in °C, convert the gpm figures into cubic meters per second, and turn the water density (1 g/cm³) into kg/m³. From there, I want to calculate the mass flow, use Cp = 4.186 kJ/(kg·K) to get the heat duty in kW, then convert that to horsepower. Once we have the three horsepower numbers, I’d like the average, median, and maximum so I can decide whether one pump will do or if I really need two in parallel (anything above 2 HP means two). After that, I need the daily energy use in kWh for each case, the cost over 30 days at $0.10 per kWh, and the average monthly cost across all three. And just to be sure, could you cross-check the 200 gpm scenario by converting its monthly kWh figure into BTU? I really need every intermediate value—temperature conversions, ΔT, volumetric and mass flows, heat duties, horsepower, energy use, costs, plus the pump recommendation—so I can back it all up with solid data.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Unit Converter+Math MCP",
      "servers": [
        "Unit Converter",
        "Math MCP"
      ],
      "distraction_servers": [
        "Context7",
        "FruityVice",
        "Google Maps",
        "Huge Icons",
        "Hugging Face",
        "Movie Recommender",
        "NixOS",
        "OpenAPI Explorer",
        "Paper Search",
        "Weather Data"
      ],
      "combination_name": "Conversion Tools"
    },
    {
      "query_id": "game_trends_reddit_000",
      "query": "Generate a cross-platform Weekly Gaming Trend & Community Sentiment Report for the upcoming week. The agent must:\n1. Verify the Game Trends API is healthy.\n2. In parallel, fetch Steam’s current trending games, top sellers, and most-played titles; also fetch Epic Games Store’s current trending games and currently free or upcoming free titles.\n3. From Steam data, compute a combined score for each title: 40% weight to trending rank, 30% to sales rank, and 30% to player count rank. Select the top five scored Steam titles.\n4. From Epic data, mark which of those five titles appear in Epic’s trending list and which of those are currently free or upcoming free.\n5. Identify the three titles that are (a) among Steam’s top five by score and (b) present in Epic’s trending list. If fewer than three satisfy both, include the next highest-scored Steam title that is in Epic’s free or upcoming free lists.\n6. Cross-validate these three titles against the unified list from get_all_trending_games. If any title is missing, flag a data inconsistency for that title.\n7. For each of the three selected games:\n   a. Attempt to fetch up to 10 hot Reddit threads from the game’s subreddit (subreddit name exactly matching the game title).  \n   b. If fewer than five threads are returned, fallback to fetching the top five threads from r/gaming.  \n   c. For each thread ID retrieved, fetch the full post content with up to three top-level comments and comment depth of two.\n8. Summarize for each game: Steam rank metrics, Epic status (trending/free), inconsistency flags, and a brief sentiment overview derived from the Reddit post titles and comments.\n\nOutput the report as a JSON array of three objects, each with fields: {\"game_name\",\"steam_trending_rank\",\"steam_sales_rank\",\"steam_played_rank\",\"epic_status\",\"inconsistency_flag\",\"reddit_threads\": [{\"post_id\",\"title\",\"top_comments\": [\"comment1\",\"comment2\",…]}],\"sentiment_summary\"}.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Game Trends+Reddit",
      "servers": [
        "Game Trends",
        "Reddit"
      ],
      "distraction_servers": [
        "Context7",
        "DEX Paprika",
        "Math MCP",
        "Medical Calculator",
        "Movie Recommender",
        "National Parks",
        "NixOS",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Paper Search"
      ],
      "combination_name": "Entertainment Social"
    },
    {
      "query_id": "game_trends_reddit_000",
      "query": "Hey, I’m prepping our next gaming newsletter and the boss wants a quick but solid rundown of which titles are going to crush it over the coming week on both Steam and the Epic store—bonus points if any are currently free or about to be. Could you dig into Steam’s hot trends, top sellers, and player counts, figure out the handful of games that really stand out when you weight those metrics, and then see which of those also show up as trending or freebies on Epic? I’d love to land on three final picks that bridge both platforms—or if we come up short, swap in the next best one that’s free on Epic. Also, it’d be great to cross-check against a general trending feed just to catch any odd gaps. Once we’ve got our trio, can you pull the top threads from each game’s subreddit (or fallback to r/gaming if they’re quiet), grab a few key comments, and give me a quick sentiment snapshot for each? I need it in a neat format I can drop straight into our tool—and it really has to be backed by real numbers and actual chatter, since I’ll need to show my editor the proof. Thanks!",
      "category": "two_server_combinations",
      "ground_truth_tool": "Game Trends+Reddit",
      "servers": [
        "Game Trends",
        "Reddit"
      ],
      "distraction_servers": [
        "Context7",
        "DEX Paprika",
        "Math MCP",
        "Medical Calculator",
        "Movie Recommender",
        "National Parks",
        "NixOS",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Paper Search"
      ],
      "combination_name": "Entertainment Social"
    },
    {
      "query_id": "game_trends_reddit_001",
      "query": "Perform a cross‐platform, cross‐server analysis to identify and investigate games with the largest gap between social interest and actual engagement metrics over the past week. Steps:\n1. Call Game Trends:get_api_health to check API status.\n   • If status is OK, call Game Trends:get_all_trending_games to retrieve a combined list of trending titles across Steam and Epic.\n   • If status is not OK, call Game Trends:get_steam_trending_games and Game Trends:get_epic_trending_games separately, then merge their results into one trending list.\n2. Call Game Trends:get_epic_free_games to retrieve all current and upcoming free titles on Epic Games Store, and tag any of those in the merged trending list with Free=Yes.\n3. In parallel, call Game Trends:get_steam_top_sellers and Game Trends:get_steam_most_played to get Steam’s current top‐selling and most‐played lists.\n4. For each game in the merged trending list:\n   a. If it appears in Steam top sellers, record its sales rank; otherwise mark SalesRank=None.\n   b. If it appears in Steam most played, record its player count rank; otherwise mark PlayerRank=None.\n   c. Compute a ‘difference_score’ = |TrendingRank – SalesRank| + |TrendingRank – PlayerRank| (treat None as a large penalty).\n5. Sort all trending games by descending difference_score and select the top 3 titles for deeper analysis.\n6. Call Reddit:fetch_reddit_hot_threads with subreddit=\"gaming\" and limit=50 to retrieve hot posts. For each of the top 3 games, scan the thread titles for mentions of the exact game name and count how many posts mention it.\n7. Identify which of the three has the highest reddit_mentions_count. For that single game, pick the highest‐scoring thread ID and call Reddit:fetch_reddit_post_content with comment_limit=20 and comment_depth=3 to retrieve detailed discussion.\n8. Produce a JSON report listing, for each of the three games: name, platform source(s), trending rank, sales rank, player rank, difference_score, Free tag (Yes/No), reddit_mentions_count; and for the game with the highest buzz, include the reddit_top_post_id and full reddit_comments output.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Game Trends+Reddit",
      "servers": [
        "Game Trends",
        "Reddit"
      ],
      "distraction_servers": [
        "Context7",
        "FruityVice",
        "Medical Calculator",
        "Metropolitan Museum",
        "Movie Recommender",
        "NASA Data",
        "National Parks",
        "Paper Search",
        "Scientific Computing",
        "Wikipedia"
      ],
      "combination_name": "Entertainment Social"
    },
    {
      "query_id": "game_trends_reddit_001",
      "query": "Hey, I’m working on a little side project to spot games that are huge on social hype but aren’t really selling or being played nearly as much—and I want to focus on the past week. You know how some titles suddenly shoot up the trending list on the big PC storefronts but then their sales rank or concurrent player count barely budges? I’d love to pull together all those trending picks, check their actual sales rank and player-count rank to come up with a simple “hype-vs-real” gap score, and tag any that happen to be free deals. Then, I want to zero in on the top three biggest mismatches and see how often each one pops up in the main gaming discussion community—counting how many hot threads mention them. For the single game that gets the most chatter, could you grab the most popular thread ID and pull about 20 comments (going down a few reply levels) so I can see what folks are saying? In the end, I need a clear breakdown for each of the three—name, where it trended, hype rank, sales rank, player rank, gap score, free-yes/no, mention count—and for that buzziest title include the top thread ID plus its comment snippet. I really need solid numbers and actual sources from the last seven days—no vague guesses—so I can back this up.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Game Trends+Reddit",
      "servers": [
        "Game Trends",
        "Reddit"
      ],
      "distraction_servers": [
        "Context7",
        "FruityVice",
        "Medical Calculator",
        "Metropolitan Museum",
        "Movie Recommender",
        "NASA Data",
        "National Parks",
        "Paper Search",
        "Scientific Computing",
        "Wikipedia"
      ],
      "combination_name": "Entertainment Social"
    },
    {
      "query_id": "scientific_computing_unit_converter_000",
      "query": "You are designing a coordinate transformation pipeline for a two-joint robotic arm segment. Joint 1 has an angle of 45° and link length of 12 inches; Joint 2 has an angle of –30° and link length of 8 inches. Perform the following steps in order:\n\n1. Convert joint angles from degrees to radians.\n2. Convert link lengths from inches to meters.\n3. Create two 3×3 rotation matrices R1 and R2 for rotations about the Z-axis by the converted angles.\n4. Multiply R1 and R2 to obtain the composite rotation R12.\n5. Construct the full 4×4 homogeneous transform T12:\n     [ R12   t; 0  1 ], where t = [0.3048+0.7071*0.2032, 0+0.7071*0.2032, 0].\n6. Verify orthonormality of R12 by:\n     a. Computing R12ᵀ @ R12.\n     b. Subtracting the 3×3 identity matrix to get an orthonormality error matrix.\n     c. Reporting the error matrix.\n7. Compute the determinant of R12 to ensure no reflection occurs.\n8. If the maximum absolute element in the orthonormality error exceeds 1e-6, perform QR decomposition on R12 and let R12_corr = Q; otherwise set R12_corr = R12.\n9. Find an orthonormal basis for the column space of R12_corr.\n10. Create and store an external force vector F = [5, –3, 2] N and a gravity vector G = [0, 0, –9.81] m/s².\n11. Project F onto the first orthonormal basis vector to obtain F_proj.\n12. Compute the dot product between F_proj and F.\n13. Compute the cross product between F_proj and G.\n14. Compute the directional derivative of the gravitational potential energy U = m·g·z with m=5 kg, g=9.81 m/s² along the direction of F_proj.\n\nDeliverables (all numeric arrays as stored tensor names and scalar results):\n- R1, R2, R12, T12\n- Orthonormality error matrix\n- Determinant of R12\n- Q and R if QR decomposition was applied\n- Orthonormal basis vectors\n- F_proj vector\n- Dot product scalar\n- Cross product vector\n- Directional derivative expression",
      "category": "two_server_combinations",
      "ground_truth_tool": "Scientific Computing+Unit Converter",
      "servers": [
        "Scientific Computing",
        "Unit Converter"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "Context7",
        "DEX Paprika",
        "Google Maps",
        "Movie Recommender",
        "NASA Data",
        "National Parks",
        "NixOS",
        "Weather Data"
      ],
      "combination_name": "Research Tools"
    },
    {
      "query_id": "scientific_computing_unit_converter_000",
      "query": "I’m working on a little two-joint robot arm for my project and honestly I’m getting stuck on all the math. The first hinge sits at exactly 45° with a 12-inch link, and the second swings to –30° on an 8-inch link. I know I have to switch those angles into radians and turn the inches into meters, then build two Z-axis rotation matrices, multiply them together, and pack everything into a 4×4 homogeneous transform. For my offset I’m using t = [0.3048 + 0.7071 * 0.2032, 0 + 0.7071 * 0.2032, 0]. After that I’d like to check RᵀR against the identity and see if any entry exceeds 1e-6—if it does I’ve heard a QR tweak might fix it. Beyond that I need to pull an orthonormal basis from the corrected rotation, shoot an external force F = [5, –3, 2] N onto the first basis vector, find the dot with the original F, cross it with gravity G = [0, 0, –9.81], and even get the directional derivative of U = m·g·z (with m=5 kg and g=9.81 m/s²) along that projection. I really need all the actual numbers—R1, R2, R12, the full T12, the tiny error matrix, the determinant, Q/R if you do the QR, the basis vectors, F_proj, the dot and cross results, plus the derivative—because I’ve got to present this with solid data next week, not hand-wavy estimates.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Scientific Computing+Unit Converter",
      "servers": [
        "Scientific Computing",
        "Unit Converter"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "Context7",
        "DEX Paprika",
        "Google Maps",
        "Movie Recommender",
        "NASA Data",
        "National Parks",
        "NixOS",
        "Weather Data"
      ],
      "combination_name": "Research Tools"
    },
    {
      "query_id": "scientific_computing_unit_converter_001",
      "query": "Simulate and analyze a 4×4 homogeneous transformation matrix for a rocket nozzle segment using physical parameters with mixed units, then perform a full linear-algebraic and symbolic analysis, including conditional scaling and vector operations, and finally visualize a related vector field.\n\nSteps:\n1. Convert the stagnation temperature 518°F to Kelvin.\n2. Convert the nozzle throat diameter 16 inches to meters.\n3. Convert the flow deflection angle 45° to radians.\n4. Convert the total energy input 5000 Btu to kilojoules.\n5. Using those converted values (T_K, d_m, θ_rad, E_kJ), create two 4×4 tensors:\n   • M1 (homogeneous transform):\n     [[cos(θ_rad), -sin(θ_rad), 0, d_m/2],\n      [sin(θ_rad),  cos(θ_rad), 0,     0],\n      [         0,           0, 1,     0],\n      [         0,           0, 0,     1]]\n   • M2 (diagonal physical scaling): diag([T_K, E_kJ, 1, 1])\n6. Compute M = M1 @ M2 (matrix multiplication).\n7. Compute det(M). If |det(M) − 1| < 1e-6, record “volume-preserving”; otherwise, compute scale_factor = det(M)**(−1/4), apply in-place scaling to M to enforce det=1, and record the new determinant.\n8. Compute M⁻¹, the eigenvalues and eigenvectors of M, its SVD (U, S, Vᵀ), its column-space orthonormal basis, the representation of M in that basis (change_basis), and its rank.\n9. Create two 3-component vectors v1 = [T_K, E_kJ, 0] and v2 = [E_kJ, T_K, 0]. Compute their dot product, cross product, and the projection of v1 onto v2.\n10. Symbolically compute the gradient of f(x,y,z)=x*y*z; compute the directional derivative of f along u=[1,1,1] (unitized); compute the curl of F=[x*y, y*z, z*x] and its divergence at point [1,2,3]; compute the Laplacian of f(x,y,z).\n11. Plot the 3D vector field F_plot=[z, -y, x] over bounds x,y,z∈[−1,1] with resolution n=8.\n\nProduce a structured report containing: all converted scalar values; the tensors M1, M2, M (before and after any scaling); det(M) (before and after); scale_factor if applied; M⁻¹; eigenvalues and eigenvectors; U, S, Vᵀ; orthonormal basis vectors; changed-basis matrix; rank; v1⋅v2; v1×v2; projection vector; symbolic gradient; directional derivative; curl and divergence results; Laplacian; and display the vector-field plot.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Scientific Computing+Unit Converter",
      "servers": [
        "Scientific Computing",
        "Unit Converter"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "DEX Paprika",
        "Google Maps",
        "Hugging Face",
        "Math MCP",
        "Medical Calculator",
        "Metropolitan Museum",
        "NixOS",
        "Paper Search",
        "Weather Data"
      ],
      "combination_name": "Research Tools"
    },
    {
      "query_id": "scientific_computing_unit_converter_001",
      "query": "Hey, I’m racing against the clock on a little rocket‐nozzle simulation and the units just aren’t playing nice. I logged the stagnation temperature at 518 °F, the throat diameter reads 16 inches, the flow deflection is 45°, and I’ve got about 5 000 Btu of total energy input—my code only wants SI, so I need those in Kelvin, meters, radians and kilojoules. \n\nOnce I’ve got those, I’d like to build a 4×4 transform that rotates by that angle and shifts by half the diameter, then apply a physical scaling on the temperature and energy axes. After multiplying them, I need to check if the determinant is unity (volume-preserving); if it isn’t, figure out the fourth-root scale factor to force det=1 and show me the before/after. Then I want to dig into the linear algebra: the inverse of that final matrix, its eigenvalues/eigenvectors, the full SVD (U, S, Vᵀ), an orthonormal basis for its column space, how the matrix looks in that new basis, and its rank.\n\nOn top of that, I have two 3-component vectors built from those converted scalars—v₁ = [T_K, E_kJ, 0] and v₂ = [E_kJ, T_K, 0]—and I’d like their dot product, cross product, and the projection of v₁ onto v₂. Then, for some symbolic crunching, consider f(x,y,z)=x·y·z: give me ∇f and the directional derivative along the unit vector [1,1,1]. Also take F=[x y, y z, z x], compute its curl and divergence at the point [1,2,3], and give me the Laplacian of f(x,y,z). \n\nFinally, I need a quick 3D plot of the field F_plot=[z, –y, x] over x,y,z in [–1,1] with an 8×8×8 grid so I can stick it in my slides. Could you run through all of that and give me the exact converted numbers, the raw and any scaled matrices, det values (before/after) plus the scale factor if applied, the inverse, eigen stuff, U/S/Vᵀ, basis vectors, change-of-basis form, rank, the vector-operation results, the symbolic derivatives, and the plot? I really need hard numbers and visuals—no vague descriptions—so I can show my team real data. Thanks!",
      "category": "two_server_combinations",
      "ground_truth_tool": "Scientific Computing+Unit Converter",
      "servers": [
        "Scientific Computing",
        "Unit Converter"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "DEX Paprika",
        "Google Maps",
        "Hugging Face",
        "Math MCP",
        "Medical Calculator",
        "Metropolitan Museum",
        "NixOS",
        "Paper Search",
        "Weather Data"
      ],
      "combination_name": "Research Tools"
    },
    {
      "query_id": "wikipedia_paper_search_001",
      "query": "Investigate the current state of knowledge on CRISPR-Cas9 off-target effects and assess whether the Wikipedia entry needs updating. Steps:\n1. Use Wikipedia:search_wikipedia with query \"CRISPR-Cas9\" and limit 5 to find the main article title.\n2. Retrieve the full article using Wikipedia:get_article.\n3. Extract the list of sections using Wikipedia:get_sections to identify if an \"Off-target effects\" section exists.\n4. Obtain a tailored summary of off-target effects within the article using Wikipedia:summarize_article_for_query with query \"off-target effects\" and max_length 250.\n5. Extract the top 5 key facts on off-target effects using Wikipedia:extract_key_facts with topic_within_article \"off-target effects\" and count 5.\n6. Formulate the query \"CRISPR-Cas9 off-target effects\" and, in parallel, search for at least 5 papers in each of these sources:\n   • Paper Search:search_arxiv (max_results 5)\n   • Paper Search:search_pubmed (max_results 5)\n   • Paper Search:search_biorxiv (max_results 5)\n   • Paper Search:search_medrxiv (max_results 5)\n   • Paper Search:search_google_scholar (max_results 5)\n7. If any source returns fewer than 5 results, supplement from Google Scholar results to reach 5 unique papers total.\n8. From arXiv, bioRxiv, and medRxiv results take the top 2 paper IDs each, download PDFs with the respective download tool, and then read and extract the full text using the corresponding read tool.\n9. From each PDF, summarize in about 150 words the specific methods used to detect or mitigate off-target effects.\n10. Compare the 5 key facts extracted from Wikipedia with the method summaries from the downloaded papers to identify three novel detection or mitigation methods not currently reflected in the Wikipedia article.\n11. Produce a final JSON report containing:\n   • wikipedia_summary: the tailored summary of off-target effects\n   • wikipedia_key_facts: list of 5 key facts\n   • paper_metadata: object with arrays of returned metadata for each source\n   • methods_summaries: array of six 150-word summaries (two per server)\n   • novel_methods: list of three novel approaches from the papers\n   • update_recommendations: text describing how to update the Wikipedia entry to include the novel methods.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Wikipedia+Paper Search",
      "servers": [
        "Wikipedia",
        "Paper Search"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "FruityVice",
        "NixOS",
        "OKX Exchange",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Reddit",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "Knowledge Base"
    },
    {
      "query_id": "wikipedia_paper_search_001",
      "query": "Hey, I’m gearing up for a talk on CRISPR and its off-target editing issues, and I’m not convinced the main Wikipedia page is completely up to date. Could you peek at the off-target section there and let me know what it currently says? Then dive into the latest research—say a handful of recent papers from major preprint servers and journals over the past few months—and pull together about five key takeaways from the wiki plus roughly five papers per source. For the top two preprints, grab their PDFs and write up about 150 words each on how they detect or prevent off-target cuts. After that, compare those methods to what the wiki lists and flag three genuinely new techniques that aren’t mentioned yet. Finally, wrap it all into a single report that shows:\n\n- the current wiki summary  \n- five main facts it covers  \n- the list of papers you found  \n- the six 150-word method summaries  \n- the three novel approaches  \n- and your suggestions for updating the article\n\nI really need concrete data and solid evidence—no vague opinions—so I can confidently revise that entry.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Wikipedia+Paper Search",
      "servers": [
        "Wikipedia",
        "Paper Search"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "FruityVice",
        "NixOS",
        "OKX Exchange",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Reddit",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "Knowledge Base"
    },
    {
      "query_id": "wikipedia_paper_search_003",
      "query": "Compile a comprehensive report on CRISPR-Cas9 gene editing by combining foundational knowledge from Wikipedia with the latest experimental and clinical research.  \n\n1. Use the Wikipedia:search_wikipedia tool to look up “CRISPR-Cas9” (limit=1). Retrieve the article title.  \n2. Using that title, call Wikipedia:get_sections to list all section titles.  \n3. Call Wikipedia:extract_key_facts with count=5 to pull out the top five key facts from the entire article.  \n4. Check whether the sections list includes a section titled “Clinical applications”.  \n   • If “Clinical applications” is present:  \n     a. In parallel, use Paper Search:search_pubmed and Paper Search:search_medrxiv with query=\"CRISPR-Cas9 clinical trial\" and max_results=5 each.  \n   • If “Clinical applications” is not present:  \n     a. In parallel, use Paper Search:search_arxiv and Paper Search:search_biorxiv with query=\"CRISPR-Cas9 structural analysis\" and max_results=5 each.  \n5. For each of the two paper search results sets:  \n   a. Filter papers to those published in the past 6 months.  \n   b. Select the top 2 most recent papers (if fewer than 2 meet the date filter, select the 2 most recent regardless).  \n6. For each selected paper:  \n   • If from arXiv: call Paper Search:download_arxiv then Paper Search:read_arxiv_paper.  \n   • If from bioRxiv or medRxiv: call Paper Search:download_biorxiv or download_medrxiv then read_biorxiv_paper or read_medrxiv_paper.  \n   • If from PubMed: record metadata only (download_pubmed/read_pubmed_paper return unsupported messages).  \n7. From each paper’s text (or metadata for PubMed), produce a 200-word summary of methodology, sample size, key outcomes, and limitations.  \n8. Cross-validate: compare the five Wikipedia key facts against the combined paper summaries. Identify at least three points where the literature confirms, extends, or contradicts Wikipedia’s facts.  \n9. Synthesize a final JSON report with four fields:  \n   • \"foundation_summary\": a 150-word summary of CRISPR-Cas9 basics from Wikipedia.  \n   • \"latest_research_insights\": a consolidated summary of methods and outcomes from all eight papers.  \n   • \"cross_validation\": a list of at least three matched or mismatched points between Wikipedia and literature.  \n   • \"recommendations\": three concrete next-step research questions or papers to follow up on.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Wikipedia+Paper Search",
      "servers": [
        "Wikipedia",
        "Paper Search"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Context7",
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Hugging Face",
        "Movie Recommender",
        "National Parks",
        "OpenAPI Explorer",
        "Reddit"
      ],
      "combination_name": "Knowledge Base"
    },
    {
      "query_id": "wikipedia_paper_search_003",
      "query": "I’m prepping a big briefing on CRISPR-Cas9 for my lab and my advisor keeps asking for more than just the usual Wikipedia overview. Here’s what I’m hoping you can help me with:\n\nI’d like a concise, roughly 150-word plain-English primer on how CRISPR-Cas9 works—just the five or so most essential facts someone needs to know. Then, depending on whether that basic page even mentions “clinical applications,” I want you to dig into what’s been popping up in the last six months. If there is a clinical section, focus on recent trials; if not, pivot to structural or mechanistic studies. Aim for about eight papers total—two or so from each source—summarizing for each one: what they did, sample size, core results, any glaring limitations, and actual dates so I know it’s fresh. \n\nAfter that, please line up at least three clear spots where the new studies either back up, extend, or flat-out contradict those five key wiki facts. And to wrap it all up, give me three concrete next-step questions or leads for further reading. Really need hard data and solid dates—can’t walk into my advisor’s office with just vague claims. Does that make sense?",
      "category": "two_server_combinations",
      "ground_truth_tool": "Wikipedia+Paper Search",
      "servers": [
        "Wikipedia",
        "Paper Search"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Context7",
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Hugging Face",
        "Movie Recommender",
        "National Parks",
        "OpenAPI Explorer",
        "Reddit"
      ],
      "combination_name": "Knowledge Base"
    },
    {
      "query_id": "reddit_dex_paprika_000",
      "query": "You are a crypto research analyst. Perform an on-chain vs social signal correlation study for the Ethereum token and its top liquidity pools on the Ethereum network. Execute the following without asking for more information:  \n\n1. Call DEX Paprika:getNetworks to retrieve all supported networks and confirm “ethereum” is available.  \n2. Call DEX Paprika:search with query=\"Ethereum\" to identify the official Ethereum tokenAddress and its network.  \n3. Call DEX Paprika:getTokenDetails with network=\"ethereum\" and the discovered tokenAddress to fetch token metadata.  \n4. Call DEX Paprika:getTokenPools with network=\"ethereum\", tokenAddress from step 2, limit=10, orderBy=\"volume_usd\", sort=\"desc\" to list top pools trading Ethereum.  \n5. From the returned token pools, select the top 3 pools by USD volume.  For each of those 3 pools:  \n   a. Call DEX Paprika:getPoolDetails with network=\"ethereum\" and poolAddress to fetch pool composition and current metrics.  \n   b. Call DEX Paprika:getPoolOHLCV with network=\"ethereum\", poolAddress, start=\"7 days ago\", end=\"now\", limit=7, interval=\"24h\", inversed=false to retrieve daily OHLCV for the past 7 days.  \n   c. Call DEX Paprika:getPoolTransactions with network=\"ethereum\", poolAddress, page=0, limit=50 to retrieve the 50 most recent swap/add/remove events.  \n6. In parallel, fetch social signals:  \n   a. Call Reddit:fetch_reddit_hot_threads for subreddit=\"ethereum\", limit=5.  \n   b. Call Reddit:fetch_reddit_hot_threads for subreddit=\"cryptocurrency\", limit=5.  \n7. From each subreddit’s result, take the 3 hottest post IDs and call Reddit:fetch_reddit_post_content with post_id, comment_limit=10, comment_depth=2 to retrieve full thread content.  \n8. Analyze and cross-validate:  \n   • Correlate daily OHLCV and transaction spikes in each pool (step 5b/5c) with the timing and volume of Reddit discussions (step 7).  \n   • Identify any mentions of “ETH”, “Ethereum”, or the specific poolAddress strings in post titles or comments.  \n9. Produce a JSON report with three sections:  \n   • on_chain_metrics: token metadata, for each of the 3 pools include pool composition, 7-day OHLCV table, transaction count summary;  \n   • social_signals: list of fetched Reddit threads (subreddit, post title, URL, total comments) and a flag if the pool or token is mentioned;  \n   • correlation_insights: for each pool, note days where volume or transaction count spiked alongside elevated Reddit discussion (count of posts/comments that day), and whether mention overlap suggests social-driven movement.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Reddit+DEX Paprika",
      "servers": [
        "Reddit",
        "DEX Paprika"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Context7",
        "FruityVice",
        "Huge Icons",
        "Hugging Face",
        "Metropolitan Museum",
        "National Parks",
        "NixOS",
        "OKX Exchange",
        "Scientific Computing"
      ],
      "combination_name": "Social Markets"
    },
    {
      "query_id": "reddit_dex_paprika_000",
      "query": "I’m putting together a quick deep-dive on ETH and how its biggest liquidity spots are behaving versus what people are buzzing about online. First, I want to make sure I’m looking at the official Ethereum token on mainnet. Then could you find the three ETH pools moving the most USD volume right now and pull their daily 24-hour price/volume figures over the past week, plus roughly fifty of the latest swap/add/remove events for each? At the same time, grab the five hottest threads from r/ethereum and r/cryptocurrency, then dive into the three most engaging posts in each (with a handful of comments). Finally, line up any days when those pool volumes or transaction counts spike with peaks in Reddit chatter. I really need hard numbers and a clear breakdown of on-chain metrics, social buzz, and any patterns that suggest the two are linked—can you help me nail that down?\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Reddit+DEX Paprika",
      "servers": [
        "Reddit",
        "DEX Paprika"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Context7",
        "FruityVice",
        "Huge Icons",
        "Hugging Face",
        "Metropolitan Museum",
        "National Parks",
        "NixOS",
        "OKX Exchange",
        "Scientific Computing"
      ],
      "combination_name": "Social Markets"
    },
    {
      "query_id": "reddit_dex_paprika_001",
      "query": "You are an on-chain data analyst tasked with validating and profiling DeFi liquidity pools that are currently trending on Reddit. Execute the following steps in order, without any additional input:\n1. Call DEX Paprika:getStats to retrieve high-level statistics about the DexPaprika ecosystem.\n2. Call DEX Paprika:getNetworks to list all supported blockchain networks.\n3. Call Reddit:fetch_reddit_hot_threads with subreddit=\"cryptocurrency\" and limit=20. From the returned hot threads, identify any thread titles or URLs that contain a DEX Paprika pool page link (pattern “/pool/”). Extract up to 3 unique pool addresses from those links and record the corresponding post_id for each.\n4. For each extracted poolAddress:\n   a. Call DEX Paprika:search with query equal to the poolAddress. If the top result has type=\"pool\", record its network ID; otherwise skip this address.\n   b. Verify that the network ID appears in the list from step 2.\n   c. Call DEX Paprika:getPoolDetails with the verified network and poolAddress.\n   d. Call DEX Paprika:getPoolOHLCV with network, poolAddress, start=\"30 days ago\", limit=30, interval=\"24h\" to fetch daily price history over the past month.\n   e. Call DEX Paprika:getPoolTransactions with network, poolAddress, limit=10 to fetch the ten most recent swaps/adds/removes.\n   f. Call DEX Paprika:getNetworkPools with network, limit=5, sort=\"desc\", orderBy=\"volume_usd\" to retrieve the top five pools by volume on that network.\n   g. Call Reddit:fetch_reddit_post_content with the saved post_id, comment_limit=3, comment_depth=2 to fetch the top 3 comments on the original Reddit thread.\n5. Compile and output a comparative report for each poolAddress containing:\n   • network ID\n   • total volume_usd and liquidity from getPoolDetails\n   • average daily volume over the past 30 days (computed from OHLCV)\n   • count of the 10 most recent transactions\n   • rank of the pool by volume_usd among the network’s top 5 pools\n   • summary of the top 3 Reddit comments (text plus author)\n\nFormat your final output as a JSON array of objects, one per poolAddress, each with keys: network, poolAddress, volume_usd, liquidity_usd, avg_daily_volume_usd, recent_tx_count, rank_among_top5, reddit_comments_summary.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Reddit+DEX Paprika",
      "servers": [
        "Reddit",
        "DEX Paprika"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "FruityVice",
        "Huge Icons",
        "Math MCP",
        "Medical Calculator",
        "Metropolitan Museum",
        "Movie Recommender",
        "NixOS",
        "OpenAPI Explorer"
      ],
      "combination_name": "Social Markets"
    },
    {
      "query_id": "reddit_dex_paprika_001",
      "query": "I’m putting together a DeFi briefing for my trading desk and they’ve been fixated on a few liquidity pools everyone keeps linking in r/cryptocurrency. What I’d love is:\n\nSome high-level health stats on the DEX ecosystem and a quick list of which blockchains it covers, then a scan of the current top 20 hot threads on r/cryptocurrency to spot up to three unique pool page URLs. For each pool address found, can you:\n\n- Double-check it really maps to a live pool on one of the supported chains  \n- Pull its latest total volume and liquidity figures  \n- Fetch its daily price history for the past 30 days and compute the average daily volume  \n- Grab the ten most recent swap/add/remove events  \n- See how it ranks by volume against the top five pools on that same network  \n- And even bring back the first three comments from the original Reddit post for context  \n\nIdeally I’d get back a tidy JSON array where each entry has: network ID, pool address, volume_usd, liquidity_usd, avg_daily_volume_usd, recent_tx_count, rank_among_top5, plus a short summary of those top comments. I really need solid numbers here—no guesswork—so I can show the team hard data in our next meeting.",
      "category": "two_server_combinations",
      "ground_truth_tool": "Reddit+DEX Paprika",
      "servers": [
        "Reddit",
        "DEX Paprika"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "FruityVice",
        "Huge Icons",
        "Math MCP",
        "Medical Calculator",
        "Metropolitan Museum",
        "Movie Recommender",
        "NixOS",
        "OpenAPI Explorer"
      ],
      "combination_name": "Social Markets"
    },
    {
      "query_id": "openapi_explorer_000",
      "query": "Perform a comparative audit of “search” endpoints and their pagination strategies across three OpenAPI specifications: openai, github, and cloudflare.  \n1. For each spec (openai, github, cloudflare), call getApiOverview to retrieve the full list of paths and operations.  \n2. From each overview, identify every operation whose operationId or path contains the substring “search”.  \n3. For each identified search operation, call getApiOperation to fetch its details. Extract its parameters and response schema fields to determine if it supports pagination. Identify pagination style (page-based via page/page_size, cursor-based via cursor/next_cursor, or none), parameter names, types, required flags, and default values. Also note any response fields related to pagination (e.g., next_page, next_cursor).  \n4. If in any spec none of the search operations support pagination, then:  \n   a. Re-use getApiOverview on that spec and identify operations with “list” in their operationId or path.  \n   b. For each list operation, call getApiOperation and extract the same pagination data as above.  \n5. Consolidate findings into a comparative table (JSON array) with one entry per operation, containing: \n   • spec (openai/github/cloudflare)  \n   • operationId  \n   • path  \n   • paginationMechanism (\"page-based\", \"cursor-based\", or \"none\")  \n   • parameters: [{ name, in, type, required, default }]  \n   • responsePaginationFields: [field names]  \n6. Highlight any spec where search endpoints lack built-in pagination and must fall back to list endpoints, and compare the pagination consistency across all three specs.",
      "category": "single_server",
      "ground_truth_tool": "OpenAPI Explorer",
      "servers": [
        "OpenAPI Explorer"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "Context7",
        "DEX Paprika",
        "Hugging Face",
        "Metropolitan Museum",
        "NASA Data",
        "National Parks",
        "NixOS",
        "Weather Data"
      ],
      "combination_name": "Single Server: OpenAPI Explorer"
    },
    {
      "query_id": "openapi_explorer_000",
      "query": "Hey, I’m working on this new dashboard that pulls search results from three different services—one for AI stuff, one for code hosting, and one for edge networking—and I’m scratching my head over how each handles pagination. Some APIs might use a page/page_size setup, others a cursor or next_cursor, and I’m not even sure if all of them support paging in their search calls or if I have to switch to their “list” routes instead. \n\nCould you dig into each service’s search endpoints and tell me:\n• whether it pages at all or not  \n• if it does, what style it uses (page numbers, cursors, etc.)  \n• the exact parameter names, types, required flags, and defaults  \n• any response fields that indicate where to pick up the next batch  \n\nAnd if a service’s search doesn’t page, check its list endpoints the same way. I really need a solid breakdown—names, defaults, response tokens—the whole picture, so I can convince my boss this setup will actually work. Need real details, not guesses. Thanks!",
      "category": "single_server",
      "ground_truth_tool": "OpenAPI Explorer",
      "servers": [
        "OpenAPI Explorer"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "Context7",
        "DEX Paprika",
        "Hugging Face",
        "Metropolitan Museum",
        "NASA Data",
        "National Parks",
        "NixOS",
        "Weather Data"
      ],
      "combination_name": "Single Server: OpenAPI Explorer"
    },
    {
      "query_id": "openapi_explorer_001",
      "query": "Perform a cross-API specification audit comparing the OpenAI and GitHub OpenAPI specs. 1) Call OpenAPI Explorer:getApiOverview with id=\"openai\" to retrieve the full list of operations and securitySchemes. 2) From the OpenAI overview, select every operation whose operationId contains the substring \"create\"; for each of these operations, call OpenAPI Explorer:getApiOperation to extract the requestBody.schema.required properties and count how many required fields each operation has. 3) Call OpenAPI Explorer:getApiOverview with id=\"github\" to retrieve the full list of operations and securitySchemes. 4) From the GitHub overview, select every operation whose path begins with \"/repos\"; for each of these, call OpenAPI Explorer:getApiOperation to extract all path, query, and requestBody parameters and count how many are required. 5) Extract and list each security scheme type (e.g. apiKey, oauth2) defined in both specs. 6) Compare the two specs and produce a consolidated JSON report containing:  \n  - openai.authSchemes: list of security scheme names and types  \n  - openai.createOperations: array of { operationId, requiredParamCount }  \n  - github.authSchemes: list of security scheme names and types  \n  - github.repoOperations: array of { path, requiredParamCount }  \n  - crossComparison.operationsWithHighParamCount: list of operations (with spec and identifier) having more than 3 required parameters  \n  - crossComparison.commonSecurityTypes: intersection of security scheme types between OpenAI and GitHub specs",
      "category": "single_server",
      "ground_truth_tool": "OpenAPI Explorer",
      "servers": [
        "OpenAPI Explorer"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Car Price Evaluator",
        "Math MCP",
        "Medical Calculator",
        "Metropolitan Museum",
        "Movie Recommender",
        "NixOS",
        "OKX Exchange",
        "Paper Search",
        "Unit Converter"
      ],
      "combination_name": "Single Server: OpenAPI Explorer"
    },
    {
      "query_id": "openapi_explorer_001",
      "query": "Hey, I’m building a little integration for my team and could really use a sanity check on two services we’re about to hook up. One of them is an AI platform where most of what I’ll do is “create” stuff (models, completions, that kind of thing), and the other is a code-hosting service where I only care about endpoints under “/repos” for cloning, PRs, labels, etc. \n\nHere’s what I’m trying to figure out: for each of those AI create-calls, how many required fields do I actually need to send? And then for the repo routes on the other side, how many mandatory inputs are there in the path, query string or request body? On top of that, each service uses its own auth methods—API keys, OAuth2 flows, maybe others—and I’d love to know which types each one offers and which types they share so I can reuse our login flow.\n\nIt’d be a huge help if you could pull those counts straight from their specs, highlight any endpoints that demand more than three required inputs (those will need extra form design on our side), list out the auth scheme types for both platforms, and then point out the overlap. Ideally I’d get back a tidy JSON-style summary I can hand off to my manager. And please, real numbers only—I can’t show up with guesswork. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "OpenAPI Explorer",
      "servers": [
        "OpenAPI Explorer"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Car Price Evaluator",
        "Math MCP",
        "Medical Calculator",
        "Metropolitan Museum",
        "Movie Recommender",
        "NixOS",
        "OKX Exchange",
        "Paper Search",
        "Unit Converter"
      ],
      "combination_name": "Single Server: OpenAPI Explorer"
    },
    {
      "query_id": "unit_converter_000",
      "query": "Comprehensive Reactor X Startup Readiness Check: You are provided with a set of sensor readings taken just before startup of Reactor X. Each sensor reading has a specified threshold in SI units. Your job is to:\n1. Verify that all needed unit types are supported by calling list_supported_units (unit_type null to get all types).\n2. Perform a batched conversion of all 14 sensor readings into their SI threshold units using convert_batch.  For each request include: value, from_unit, to_unit, conversion_type, and a unique request_id.\n3. Parse the batch response and for each sensor compare the converted value to its SI threshold:\n   - If converted_value ≥ threshold_value, status is PASS.\n   - If converted_value < threshold_value, status is FAIL.\n4. For any sensor that FAILs, invoke the corresponding individual conversion tool (e.g., convert_temperature for temperature failures, convert_pressure for pressure failures, etc.) with the same input parameters as in the batch to cross-validate the conversion result.\n5. Produce a final JSON report listing each sensor with fields: sensor_name, original_reading (value + unit), converted_value (with unit), threshold_value (with unit), status (PASS/FAIL), cross_validation (object with batch_value and individual_value if cross-validation was performed).\n\nSensors and thresholds:\n1. inlet_temperature: 350 °F → threshold 150 °C (temperature)\n2. inlet_pressure: 50 psi → threshold 350 kPa (pressure)\n3. reactor_length: 10 ft → threshold 5 m (length)\n4. catalyst_weight: 500 lb → threshold 200 kg (mass)\n5. tank_volume: 2000 gallon (imperial) → threshold 8 m³ (volume)\n6. data_buffer: 2 gigabyte → threshold 1500 megabyte (computer_data)\n7. heat_exchanger_area: 1000 ft² → threshold 90 m² (area)\n8. motor_power: 50 horsepower → threshold 40 kilowatt (power)\n9. reaction_time: 2 hours → threshold 6000 seconds (time)\n10. valve_angle: 0.25 turns → threshold 45 degrees (angle)\n11. conveyor_speed: 2 meters/second → threshold 4000 feet/minute (speed)\n12. valve_force: 500 pounds force → threshold 2000 newtons (force)\n13. fluid_density: 128 pounds per cubic foot → threshold 2000 kilograms per cubic meter (density)\n14. fuel_energy: 10000 Btu → threshold 12000 kilojoule (energy)\n\nProduce the report exactly as specified; do not request further information.",
      "category": "single_server",
      "ground_truth_tool": "Unit Converter",
      "servers": [
        "Unit Converter"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "FruityVice",
        "Game Trends",
        "Google Maps",
        "Medical Calculator",
        "OKX Exchange",
        "OpenAPI Explorer",
        "Paper Search",
        "Scientific Computing",
        "Weather Data"
      ],
      "combination_name": "Single Server: Unit Converter"
    },
    {
      "query_id": "unit_converter_000",
      "query": "Hey, I’m prepping for a Reactor X startup tomorrow and it’s stressing me out a bit. My boss handed me 14 different sensor readings, all in weird units, and I need to know if we meet the safety thresholds (which are all in SI or related metric units). Here’s what I’ve got:\n\n- Inlet temperature: 350 °F (threshold 150 °C)  \n- Inlet pressure: 50 psi (threshold 350 kPa)  \n- Reactor length: 10 ft (threshold 5 m)  \n- Catalyst weight: 500 lb (threshold 200 kg)  \n- Tank volume: 2000 imperial gal (threshold 8 m³)  \n- Data buffer: 2 GB (threshold 1500 MB)  \n- Heat-exchanger area: 1000 ft² (threshold 90 m²)  \n- Motor power: 50 hp (threshold 40 kW)  \n- Reaction time: 2 hours (threshold 6000 s)  \n- Valve angle: 0.25 turns (threshold 45 °)  \n- Conveyor speed: 2 m/s (threshold 4000 ft/min)  \n- Valve force: 500 lbf (threshold 2000 N)  \n- Fluid density: 128 lb/ft³ (threshold 2000 kg/m³)  \n- Fuel energy: 10000 Btu (threshold 12000 kJ)  \n\nCould you convert each reading into the same units as its threshold, then tell me for each one whether it passes (converted ≥ threshold) or fails? And if any come up as a fail, I’d really appreciate you doing a second check with a different conversion route—just to be absolutely sure we didn’t slip up on units. \n\nIt’d be awesome if you could bundle everything in a JSON summary that shows, for each sensor: its name, the original reading, the converted value with units, the threshold with units, pass/fail status, and the cross-validation details when you’ve done that extra check. I really need actual numbers on this—can’t go to my boss with just opinions. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Unit Converter",
      "servers": [
        "Unit Converter"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "FruityVice",
        "Game Trends",
        "Google Maps",
        "Medical Calculator",
        "OKX Exchange",
        "OpenAPI Explorer",
        "Paper Search",
        "Scientific Computing",
        "Weather Data"
      ],
      "combination_name": "Single Server: Unit Converter"
    },
    {
      "query_id": "unit_converter_001",
      "query": "You are evaluating the performance of a high-altitude research drone during an upcoming 2 hour 30 minute test flight. All original measurements are in U.S. customary units. Using the provided unit conversion tools, you must:\n\n1. Verify supported units for angle conversions by calling list_supported_units for unit_type \"angle\".  \n   - If the unsupported unit \"grads\" is not listed, fall back to \"gons\".  \n\n2. Convert the following raw measurements to SI units:\n   • Engine inlet temperature: 500 °F → Kelvin  \n   • Engine outlet temperature: 300 °F → Kelvin  \n   • Propeller pitch angle: 15 degrees → choose target unit from step 1 (\"gons\")  \n   • Cruise speed: 60 knots → meters per second  \n   • Cruise altitude: 10 000 feet → meters  \n   • Wing span: 15 feet → meters  \n   • Wing area: 1 200 square inches → square meters  \n   • Cargo bay pressure: 50 psi → pascals  \n   • Takeoff thrust: 3 000 pounds-force → newtons  \n\n3. Convert the following storage, energy, and power metrics:\n   • On-board log storage: 2 gigabytes → bytes  \n   • Flight data downlink buffer: 10 megabytes → bytes  \n   • Battery capacity: 5 kilowatt-hours → joules  \n   • Average power draw (compute as battery capacity / flight duration): → compute in kilowatts, then convert kilowatts → horsepower  \n\n4. Compute fuel usage metrics:\n   • Fuel tank volume at start: 200 U.S. gallons → cubic meters  \n   • Fuel density: 810 grams per liter → kilograms per cubic meter  \n   • Calculate total starting fuel mass in kilograms by multiplying the converted volume by the converted density.  \n   • Decision point: if the starting fuel mass > 100 kg, convert that mass → tonnes; otherwise convert → pounds.  \n\n5. Convert flight duration:\n   • 2 hours 30 minutes → seconds  \n\n6. Summarize all converted values and derived metrics in a single JSON object with clearly labeled fields: original_value, original_unit, converted_value, converted_unit. Include computed fields: average_power: { value, unit }, starting_fuel_mass: { value, unit }, and a note about which branch was taken at the fuel-mass decision.\n\nYou must use list_supported_units, convert_temperature, convert_angle, convert_speed, convert_length, convert_area, convert_pressure, convert_force, convert_volume, convert_density, convert_computer_data, convert_energy, convert_power, convert_time, and convert_mass. Implement a batch conversion for the temperature, pressure, storage, energy, and power conversions; if any batch item fails, fall back to the individual convert_* calls. All steps must be performed without asking for further input.",
      "category": "single_server",
      "ground_truth_tool": "Unit Converter",
      "servers": [
        "Unit Converter"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Car Price Evaluator",
        "FruityVice",
        "Google Maps",
        "Huge Icons",
        "Hugging Face",
        "Math MCP",
        "National Parks",
        "NixOS",
        "Paper Search"
      ],
      "combination_name": "Single Server: Unit Converter"
    },
    {
      "query_id": "unit_converter_001",
      "query": "I’m gearing up for a 2 h 30 min high-altitude drone test flight and my boss wants every single detail in SI. Right now all my numbers are in U.S. customary units: engine inlet at 500 °F and outlet at 300 °F; propeller pitch is 15° (I’d like that in gons); cruise speed is 60 knots; altitude’s 10 000 ft; wing span 15 ft; wing area 1 200 in²; cargo-bay pressure 50 psi; takeoff thrust 3 000 lbf; on-board log storage is 2 GB with a 10 MB downlink buffer; battery capacity 5 kWh over this 2 h 30 min flight; and the fuel tank holds 200 US gal of fuel whose density is 810 g/L. \n\nCan you help me convert all of that—temperatures to kelvins, angle to gons, speed to m/s, lengths to meters, area to m², pressure to pascals, force to newtons, storage to bytes, energy to joules, compute the average power draw in kW then to horsepower, volume to m³, density to kg/m³, and time to seconds—then calculate the total starting fuel mass in kilograms and, if it ends up over 100 kg, report it in tonnes (otherwise in pounds)? In the end I need a tidy JSON where each entry has original_value, original_unit, converted_value, converted_unit, plus two computed fields—average_power and starting_fuel_mass—and a note on which fuel-mass branch you chose. I really need solid, data-driven numbers here—no hand-wavy estimates.",
      "category": "single_server",
      "ground_truth_tool": "Unit Converter",
      "servers": [
        "Unit Converter"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Car Price Evaluator",
        "FruityVice",
        "Google Maps",
        "Huge Icons",
        "Hugging Face",
        "Math MCP",
        "National Parks",
        "NixOS",
        "Paper Search"
      ],
      "combination_name": "Single Server: Unit Converter"
    },
    {
      "query_id": "wikipedia_000",
      "query": "You are tasked with producing a comprehensive research dossier on major global climate change negotiation frameworks as described in Wikipedia. Follow these steps exactly, using only the provided Wikipedia tools:\n\n1. Use Wikipedia:search_wikipedia with query=\"climate change negotiation frameworks\" and limit=5. Collect the returned article titles into a list called frameworks.\n\n2. For each title in frameworks, in parallel:\n   a. Call Wikipedia:get_article with the title to fetch the full content.\n   b. Call Wikipedia:get_sections with the title to retrieve the list of section titles.\n   c. If the sections list contains a section titled \"History\":\n        i. Call Wikipedia:summarize_article_section with title, section_title=\"History\", max_length=150.\n      Else:\n        i. Call Wikipedia:get_summary with title.\n   d. Call Wikipedia:get_links with title and count the number of returned links. Record this as link_count for that framework.\n\n3. Identify the framework titled exactly \"Paris Agreement\". For that title:\n   a. Call Wikipedia:summarize_article_for_query with title=\"Paris Agreement\", query=\"emission reduction targets\", max_length=200. Store the result as paris_emission_summary.\n   b. Call Wikipedia:extract_key_facts with title=\"Paris Agreement\", topic_within_article=\"emission reduction targets\", count=5. Store the list as paris_emission_facts.\n\n4. Identify the framework titled exactly \"Kyoto Protocol\". For that title:\n   a. Call Wikipedia:get_related_topics with title=\"Kyoto Protocol\", limit=5.\n   b. If fewer than 5 related topics are returned, re-call Wikipedia:get_related_topics with title=\"Kyoto Protocol\" and limit=10. Store the final list as kyoto_related_topics.\n\n5. Cross-validate the paris_emission_facts against paris_emission_summary. Produce a list of any facts from paris_emission_facts not explicitly mentioned in paris_emission_summary; call this mismatches.\n\n6. Compile and output a single JSON object with the following structure:\n{\n  \"frameworks\": [\n    {\"title\": string, \"summary\": string, \"link_count\": integer}, ... up to 5 frameworks\n  ],\n  \"paris_emission_summary\": string,\n  \"paris_emission_facts\": [string, … 5 items],\n  \"mismatches\": [string, …],\n  \"kyoto_related_topics\": [string, …]\n}\n\nEnsure you never request additional information and only use the specified Wikipedia tools in the order and logic described.",
      "category": "single_server",
      "ground_truth_tool": "Wikipedia",
      "servers": [
        "Wikipedia"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Context7",
        "FruityVice",
        "Google Maps",
        "Hugging Face",
        "Medical Calculator",
        "Metropolitan Museum",
        "OKX Exchange",
        "Unit Converter"
      ],
      "combination_name": "Single Server: Wikipedia"
    },
    {
      "query_id": "wikipedia_000",
      "query": "I’m prepping for a presentation on the big global climate deals and could really use some solid data. Could you find the main half-dozen—or so—negotiation frameworks that show up most often and give me a quick intro to each, plus roughly how many internal links or references they have? Then dive into the Paris Agreement: I’d like about a 200-word summary focused on its emission-reduction targets and five standout facts. After that, I’m curious what topics usually pop up alongside the Kyoto Protocol—aim for at least five related ideas, and if you only spot a few, try to round it out. Oh, and would you cross-check those five Paris facts against your summary and flag any that don’t actually appear there? Finally, please wrap everything into a single JSON output since my professor insists on that. And whatever you pull, make sure it’s backed by real numbers or citations—I can’t go in there with just opinions.",
      "category": "single_server",
      "ground_truth_tool": "Wikipedia",
      "servers": [
        "Wikipedia"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Context7",
        "FruityVice",
        "Google Maps",
        "Hugging Face",
        "Medical Calculator",
        "Metropolitan Museum",
        "OKX Exchange",
        "Unit Converter"
      ],
      "combination_name": "Single Server: Wikipedia"
    },
    {
      "query_id": "wikipedia_001",
      "query": "Compare the environmental impact and relevant policy frameworks of two renewable energy technologies: “Solar energy” and “Wind power”.\n\nSteps:\n1. Search Wikipedia for “Solar energy renewable energy” and take the top result as the primary article for Solar.\n2. Search Wikipedia for “Wind power renewable energy” and take the top result as the primary article for Wind.\n3. For each technology article:\n   a. Get the list of sections.\n   b. If a section titled exactly “Environmental impact” or “Environmental impacts” exists, summarize that section (max_length = 200). Otherwise, generate a tailored summary of that article for the query “environmental impact” (max_length = 200).\n   c. Extract the top 5 key facts from the article, focused on “Environmental impact”.\n4. Compare the two sets of environmental key facts side by side in a table.\n5. For each technology article, get up to 5 related topics; identify any policy or regulatory topics among them (e.g., “Feed-in tariff”, “Renewable energy policy”).\n6. If no explicit policy-related topics appear, perform a fresh Wikipedia search for “renewable energy policy frameworks” and select the top result as the policy article.\n7. Summarize the policy article for the query “incentives for solar and wind power” (max_length = 300).\n8. Cross-validate by checking each technology article’s links: determine whether the policy article appears in their outbound links.\n9. Finally, propose one additional renewable technology (from the related topics lists) to research next, and provide a 2-sentence rationale.\n\nExpected output format:\n{\n  \"solar_environmental_summary\": \"...\",\n  \"wind_environmental_summary\": \"...\",\n  \"solar_key_facts\": [\"fact1\", …],\n  \"wind_key_facts\": [\"fact1\", …],\n  \"comparison_table\": [{\"fact_index\":1, \"solar\":\"…\", \"wind\":\"…\"}, …],\n  \"policy_article_title\": \"…\",\n  \"policy_summary\": \"…\",\n  \"solar_links_policy_present\": true/false,\n  \"wind_links_policy_present\": true/false,\n  \"recommended_next_technology\": \"…\",\n  \"recommendation_rationale\": \"…\"\n}",
      "category": "single_server",
      "ground_truth_tool": "Wikipedia",
      "servers": [
        "Wikipedia"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Car Price Evaluator",
        "Game Trends",
        "Medical Calculator",
        "Movie Recommender",
        "NASA Data",
        "OKX Exchange",
        "OSINT Intelligence",
        "Weather Data"
      ],
      "combination_name": "Single Server: Wikipedia"
    },
    {
      "query_id": "wikipedia_001",
      "query": "I’m putting together a sustainability briefing and need to really understand how solar panels stack up against wind turbines when it comes to environmental impacts—think resource use, lifecycle emissions, land use, etc. Could you:\n\n- Give me a short, punchy summary of each technology’s environmental footprint (a paragraph or two each).\n- Pull out about five of the most important facts related to their environmental impact for each, and show them side-by-side so I can see the main differences at a glance.\n\nOn top of that, I’ve got to cover the policy side—what incentive schemes or regulatory frameworks are actually driving solar and wind adoption right now? A clear, two-to-three-paragraph overview of the key support mechanisms would be great. While you’re at it, when you look at the main write-ups on solar and wind, do they actually link to that policy overview? Let me know “yes” or “no” for each.\n\nLastly, if you spot another renewable technology in those policy discussions that seems like a smart next step for us to research, tell me which one and give me two sentences on why it’s worth a closer look. \n\nI’m presenting next week, so I really need solid numbers and references—can’t go in with just vague statements. Thanks!",
      "category": "single_server",
      "ground_truth_tool": "Wikipedia",
      "servers": [
        "Wikipedia"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Car Price Evaluator",
        "Game Trends",
        "Medical Calculator",
        "Movie Recommender",
        "NASA Data",
        "OKX Exchange",
        "OSINT Intelligence",
        "Weather Data"
      ],
      "combination_name": "Single Server: Wikipedia"
    },
    {
      "query_id": "google_maps_000",
      "query": "You are an AI-powered cycling tour planner. Design a one-day round-trip cycling route in the Central Park area of Denver that starts and ends at the Denver Art Museum in the upcoming Saturday morning window (9:00 AM–12:00 PM). Perform the following steps in sequence:\n1. Convert the landmark “Denver Art Museum” to geographic coordinates.\n2. Search for cafes within a 5 km radius of those coordinates.\n3. For each cafe returned (up to 20 results), retrieve detailed information and filter to those with a rating of at least 4.0 and operating hours that include 9:00 AM–12:00 PM on the upcoming Saturday.\n4. For the filtered cafes, extract their latitude/longitude.\n5. Retrieve elevation data for the Denver Art Museum and each cafe.\n6. Calculate bicycling distances and durations for a round-trip between the museum and each cafe.\n7. Compute total round-trip distance and absolute elevation gain (difference between start and cafe elevations).\n8. Rank the cafes by the lowest sum of total distance plus elevation gain.\n9. Select the top-ranked cafe as the primary stop.\n10. Reverse-geocode the chosen cafe’s coordinates into a human-readable address.\n11. Generate detailed turn-by-turn bicycling directions for the round-trip route between the museum and the selected cafe, specifying departure time for the outbound leg at 9:00 AM.\n\nDeliverables:\n- Detailed directions (leg-by-leg) and metrics (distance, duration, elevation change) for the selected round-trip route.\n- A summary table of all candidate cafes: name, address, rating, total round-trip distance, and elevation gain.\n\nThis task must be executed without further clarification.",
      "category": "single_server",
      "ground_truth_tool": "Google Maps",
      "servers": [
        "Google Maps"
      ],
      "distraction_servers": [
        "BioMCP",
        "DEX Paprika",
        "Huge Icons",
        "Medical Calculator",
        "Metropolitan Museum",
        "NASA Data",
        "OKX Exchange",
        "OpenAPI Explorer",
        "Scientific Computing",
        "Weather Data"
      ],
      "combination_name": "Single Server: Google Maps"
    },
    {
      "query_id": "google_maps_000",
      "query": "I’m trying to plan a fun bike ride around Denver’s Central Park this Saturday morning—thinking of starting and ending at the Denver Art Museum sometime between 9 and noon. I’d love to swing by a really good café on the way—something within a few miles that’s got at least a 4-star rating and is actually open when I’m riding. But I don’t want to kill myself on hills or end up riding forever, so I’m hoping to find the spot that gives me the shortest round-trip plus the least uphill grunt. \n\nCould you help me figure out which cafés in about a 5 km radius fit the bill, rank them by total distance plus elevation gain, pick the best one, and then give me turn-by-turn bike directions (with distances, estimated times, and elevation change) for a 9 AM departure? Also, it’d be awesome to get a quick summary of all the candidates—name, address, rating, distance and elevation details—so I can see why the top pick wins. I really need actual numbers here, not just opinions, so I can be confident this ride won’t turn into a slog.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Google Maps",
      "servers": [
        "Google Maps"
      ],
      "distraction_servers": [
        "BioMCP",
        "DEX Paprika",
        "Huge Icons",
        "Medical Calculator",
        "Metropolitan Museum",
        "NASA Data",
        "OKX Exchange",
        "OpenAPI Explorer",
        "Scientific Computing",
        "Weather Data"
      ],
      "combination_name": "Single Server: Google Maps"
    },
    {
      "query_id": "google_maps_001",
      "query": "Using only the provided Google Maps tools, plan a detailed bicycle outing for the upcoming weekend in downtown San Francisco. 1) Geocode “San Francisco City Hall” to obtain latitude/longitude as the central point. 2) Search for bicycle rental shops within a 2 000 m radius of that point with a minimum rating of 4.0 and openNow=true. If fewer than three shops are returned, reduce the minRating to 3.5 and repeat until you have at least three. 3) For each of the selected rental shops, fetch full place details (operating hours, phone number, website). 4) For each shop’s coordinates, search for cafés within 1 000 m radius with minimum rating 4.0 (openNow filter not applied). 5) Compute bicycling distances and durations between each rental shop and each café using the distance matrix tool. 6) Identify the single shop–café pair with the shortest bicycling duration. 7) Retrieve turn-by-turn bicycling directions from the chosen shop to the chosen café. 8) From the directions response, extract the coordinates of the origin, the destination, and the midpoint step. Use the elevation tool to get elevation for those three points. 9) Finally, reverse-geocode the midpoint coordinate to report its human-readable address. Provide as output: the selected shop’s name, address, operating hours, and contact; the selected café’s name, address, and rating; total bicycling distance and duration; full directions; and an elevation profile (origin, midpoint, destination elevations and midpoint address).",
      "category": "single_server",
      "ground_truth_tool": "Google Maps",
      "servers": [
        "Google Maps"
      ],
      "distraction_servers": [
        "BioMCP",
        "Context7",
        "FruityVice",
        "Hugging Face",
        "Metropolitan Museum",
        "National Parks",
        "OKX Exchange",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Scientific Computing"
      ],
      "combination_name": "Single Server: Google Maps"
    },
    {
      "query_id": "google_maps_001",
      "query": "I’m planning a bike outing in downtown San Francisco next weekend and could really use a hand. I want to start around City Hall and rent from a solid shop that’s actually open when I arrive—and ideally rated 4 stars or higher. If I can’t find at least three places like that within a couple of kilometers, I’m okay with dropping to 3.5 stars just to have enough options. Then I’d love to cruise over to a top-rated café about a kilometer away. What I’m really after is the bike-shop/coffee-shop pairing that gives me the shortest ride. \n\nCould you figure out which rental spot and café that is, and give me all the nitty-gritty? I’d need the shop’s address, hours, phone number and website, plus the café’s name, address and rating. Also please include the total biking distance and time, full turn-by-turn directions, and how hilly the route is by giving me elevations at the start, midpoint and end—and even the street address of that midpoint. I need actual numbers and real locations, not vague guesses, so I can share it with my friends and get everything booked. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Google Maps",
      "servers": [
        "Google Maps"
      ],
      "distraction_servers": [
        "BioMCP",
        "Context7",
        "FruityVice",
        "Hugging Face",
        "Metropolitan Museum",
        "National Parks",
        "OKX Exchange",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Scientific Computing"
      ],
      "combination_name": "Single Server: Google Maps"
    },
    {
      "query_id": "bibliomantic_000",
      "query": "Perform a comprehensive I Ching–based decision analysis for the question “Should I start a new business venture focusing on sustainable agriculture in the next six months in the downtown Seattle area?” This analysis must use the following tools in sequence and conditional branches:\n\n1. Call Bibliomantic:server_statistics with no parameters. Record current server load (e.g., current_requests vs. max_capacity). If load exceeds 80%, include a warning in the final report but continue execution.\n\n2. Call Bibliomantic:i_ching_divination with:\n   {\n     \"query\": \"Should I start a new business venture focusing on sustainable agriculture in the next six months in the downtown Seattle area?\"\n   }\n   Extract from its response:\n     • primary_hexagram_number (integer)\n     • changing_lines (array of line positions, or empty array)\n     • secondary_hexagram_number (integer, present only if changing_lines is nonempty)\n\n3. Call Bibliomantic:get_hexagram_details with { \"hexagram_number\": primary_hexagram_number }. Capture the hexagram’s Chinese name, Unicode symbol, full commentary, and individual line texts.\n\n4. If changing_lines is nonempty, call Bibliomantic:get_hexagram_details again with { \"hexagram_number\": secondary_hexagram_number }. Capture the resulting hexagram’s details.\n\n5. Call Bibliomantic:bibliomantic_consultation with:\n   {\n     \"query\": \"Should I start a new business venture focusing on sustainable agriculture in the next six months in the downtown Seattle area?\"\n   }\n   Extract its consultation_hexagram_number and its full advisory commentary.\n\n6. Compare primary_hexagram_number and consultation_hexagram_number:\n   • If they match, note that both tools agree on the core guidance.\n   • If they differ, analyze key thematic differences in their commentaries.\n\n7. Synthesize a final recommendation JSON with these sections:\n   {\n     \"server_stats\": { /* load metrics and warning if any */ },\n     \"primary_hexagram\": { number, name, symbol, commentary, lines },\n     \"secondary_hexagram\": { /* only if changing_lines present */ },\n     \"consultation_hexagram\": { number, commentary },\n     \"comparison\": { agreement: true|false, analysis: string },\n     \"recommendation\": string\n   }\n\nAll time references use “next six months.” No external data sources are required; all inputs and results come entirely from the calls above.",
      "category": "single_server",
      "ground_truth_tool": "Bibliomantic",
      "servers": [
        "Bibliomantic"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Context7",
        "DEX Paprika",
        "Google Maps",
        "Huge Icons",
        "Metropolitan Museum",
        "Movie Recommender",
        "OKX Exchange",
        "OpenAPI Explorer",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Bibliomantic"
    },
    {
      "query_id": "bibliomantic_000",
      "query": "Hey, I’ve been tossing around this idea of launching a small sustainable agriculture venture—think urban farming or community gardens—right in downtown Seattle over the next six months. I’m really on the fence about timing and direction, so I was wondering if you could do a deep-dive I Ching reading for me. \n\nLike, what hexagram comes up first? Do any lines shift, and if they do, what’s the follow-up hexagram all about? Then, maybe run a second style of I Ching consult just to see if it echoes the first reading or highlights something totally different. I’d love to get the exact Chinese names, symbols, full commentary, and the line-by-line texts—so I’m not just getting a TL;DR, but the actual guidance in its own words.\n\nAt the end, could you weigh both readings side by side? Do they agree on the core message, or is one more cautious while the other pushes forward? And then give me a final take—should I dive in now, tweak the plan, or wait a bit? I really need those real quotes and details to share with my partner and make a solid call. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Bibliomantic",
      "servers": [
        "Bibliomantic"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Context7",
        "DEX Paprika",
        "Google Maps",
        "Huge Icons",
        "Metropolitan Museum",
        "Movie Recommender",
        "OKX Exchange",
        "OpenAPI Explorer",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Bibliomantic"
    },
    {
      "query_id": "bibliomantic_001",
      "query": "You are an AI charged with conducting a cross-validated I Ching analysis for the strategic business question: \"Should we launch our flagship product in the Southeast Asian market during the upcoming quarter?\"  Follow this workflow exactly, using only the provided tools:\n\n1. Call Bibliomantic:server_statistics() to retrieve server statistics (including \"active_requests\").\n2. Always perform a lightweight divination: call Bibliomantic:i_ching_divination with query \"Should we launch our flagship product in the Southeast Asian market during the upcoming quarter?\". Extract from its result:\n   - primary_hexagram_light (integer)\n   - changing_lines_light (array of line positions, if any)\n3. If active_requests ≤ 50:\n   a. Perform a deep consultation: call Bibliomantic:bibliomantic_consultation with the same query. Extract:\n      - primary_hexagram_deep (integer)\n      - changing_lines_deep (array of line positions, if any)\n4. For each primary hexagram obtained (light and deep if step 3 was run), call Bibliomantic:get_hexagram_details with that hexagram number. Collect:\n   - detailed_commentary_light\n   - detailed_commentary_deep (if available)\n5. Cross-validate primary results:\n   a. If both light and deep primary_hexagram numbers exist and are identical, set final_primary_hexagram to that number and final_primary_commentary to the matching commentary.\n   b. If both exist and differ, perform one more light divination (call Bibliomantic:i_ching_divination again with the same query), extract tie_breaker_hexagram, call get_hexagram_details on tie_breaker_hexagram, and set final_primary_hexagram and final_primary_commentary from that tie-breaker result.\n   c. If active_requests > 50 (so no deep consultation), set final_primary_hexagram and final_primary_commentary from the light divination.\n6. Determine secondary hexagram:\n   - Check the changing lines from whichever consultation provided final_primary_hexagram (light, deep, or tie-breaker). If any changing lines are present, compute the secondary hexagram number by flipping those lines, then call get_hexagram_details with that secondary number to obtain secondary_commentary.\n7. Produce a final report JSON object with these fields:\n   • server_statistics: full stats object returned in step 1\n   • primary_readings: an object containing each method called (\"light\", \"deep\" if run, \"tie_breaker\" if run) with their hexagram numbers and raw tool outputs\n   • final_primary_hexagram (integer)\n   • final_primary_commentary (string)\n   • secondary_hexagram (integer, if any)\n   • secondary_commentary (string, if any)\n   • final_recommendation: a concise, actionable business recommendation synthesizing all retrieved commentaries\n\nEnsure each tool call uses the correct input schema and implement every conditional branch exactly as described.",
      "category": "single_server",
      "ground_truth_tool": "Bibliomantic",
      "servers": [
        "Bibliomantic"
      ],
      "distraction_servers": [
        "Context7",
        "Google Maps",
        "Metropolitan Museum",
        "National Parks",
        "NixOS",
        "OSINT Intelligence",
        "Paper Search",
        "Reddit",
        "Scientific Computing",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Bibliomantic"
    },
    {
      "query_id": "bibliomantic_001",
      "query": "I’m trying to decide whether we should roll out our flagship product in the Southeast Asian market over the next few months. Our leadership team’s split – some think it’s the perfect moment, others worry it’s too much of a gamble. I’d love to tap into some I Ching insight to guide us. Could you peek at the oracle’s load (if it’s handling fewer than about fifty readings, go ahead with a full, in-depth cast; if it’s busier, do a quick toss of the coins)? Jot down the hexagram numbers and any moving lines for each, then pull in the commentaries. If the quick and deep readings agree, that’s our final verdict; if they clash, maybe do one more light toss to break the tie. Then, if there are moving lines, flip them to see the secondary hexagram and note its message too. At the end, I need everything laid out – the raw toss results, the final hexagram and its write-up, the follow-up one if it exists, and a clear recommendation I can share with my boss. Please give me actual numbers and detailed notes so I’m not just presenting opinions.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Bibliomantic",
      "servers": [
        "Bibliomantic"
      ],
      "distraction_servers": [
        "Context7",
        "Google Maps",
        "Metropolitan Museum",
        "National Parks",
        "NixOS",
        "OSINT Intelligence",
        "Paper Search",
        "Reddit",
        "Scientific Computing",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Bibliomantic"
    },
    {
      "query_id": "biomcp_000",
      "query": "Perform a comprehensive, multi-server analysis of the BRAF V600E mutation in melanoma over the past 3 months, integrating literature, variant database records, clinical trial data, organizational sponsorship, biomarker criteria, drug annotations, safety reports, and device event records. Steps: 1. Use article_searcher to find articles on gene BRAF AND variant V600E AND disease melanoma published in the past 3 months; include preprints. 2. Fetch details for the top 5 PMIDs via article_getter. 3. Use variant_searcher for gene=\"BRAF\", hgvsp=\"p.V600E\" to retrieve allele frequency and clinical significance; if frequency >0.01, run a second article_searcher with keywords from fetched abstracts. 4. Search ClinicalTrials.gov via trial_searcher for condition melanoma AND intervention vemurafenib AND phase PHASE2|PHASE3 AND recruiting_status=OPEN; retrieve first 5 trials. 5. For each NCT ID, fetch protocol (trial_protocol_getter), outcomes (trial_outcomes_getter), and locations (trial_locations_getter). 6. Search NCI organizations sponsoring these trials via nci_organization_searcher using city and state from each location; fetch organization details via nci_organization_getter. 7. Search NCI biomarkers via nci_biomarker_searcher for \"PD-L1\"; fetch first 5 biomarker records. 8. Retrieve drug information for vemurafenib via drug_getter. 9. Search FDA adverse event reports via openfda_adverse_searcher for drug=\"vemurafenib\" AND serious=true in the past 3 months; if >10 results, fetch the 3 most serious via openfda_adverse_getter. 10. Search FDA device adverse events via openfda_device_searcher for genomic diagnostic devices (genomics_only=true) AND problem=\"sequence analysis\"; fetch top 3 via openfda_device_getter. Output: A structured JSON report with sections: literature_summaries (title, abstract snippets), variant_statistics (frequency, significance), trial_landscape (protocol summary, outcomes, sites), sponsor_profiles, biomarker_criteria, drug_profile, drug_safety_signals, device_event_reports.",
      "category": "single_server",
      "ground_truth_tool": "BioMCP",
      "servers": [
        "BioMCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Context7",
        "DEX Paprika",
        "Huge Icons",
        "Medical Calculator",
        "Movie Recommender",
        "NixOS",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Wikipedia"
      ],
      "combination_name": "Single Server: BioMCP"
    },
    {
      "query_id": "biomcp_000",
      "query": "I’ve been asked to put together a 360-degree update on the BRAF V600E mutation in melanoma and honestly, I’m a bit swamped. I need to know what’s come out in the last three months—papers (including any preprints), how often this mutation actually shows up and what that might mean clinically, which late-stage vemurafenib trials are still recruiting and who’s backing them, plus any biomarker angles (like PD-L1 criteria), the current take on vemurafenib’s profile, and whether there have been serious safety alerts or even hiccups with the genomic testing kits used in these studies. Basically, I can’t show up without solid figures—allele frequencies, trial counts, sponsor names, adverse-event tallies, device problem reports—everything tied back to real sources. Can you help me pull all that together in one place?\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "BioMCP",
      "servers": [
        "BioMCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Context7",
        "DEX Paprika",
        "Huge Icons",
        "Medical Calculator",
        "Movie Recommender",
        "NixOS",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Wikipedia"
      ],
      "combination_name": "Single Server: BioMCP"
    },
    {
      "query_id": "biomcp_001",
      "query": "Conduct a comprehensive, multi‐tool investigation of the BRAF V600E variant in melanoma to inform potential targeted therapy strategies. The agent must:\n1. Initiate structured reasoning with BioMCP:think.\n2. Retrieve current gene annotation for BRAF via BioMCP:gene_getter (gene_id_or_symbol=\"BRAF\").\n3. Retrieve up‐to‐date disease information for melanoma via BioMCP:disease_getter (disease_id_or_name=\"melanoma\").\n4. Perform a literature search via BioMCP:article_searcher for articles and preprints on BRAF V600E in melanoma (genes=[\"BRAF\"], variants=[\"V600E\"], diseases=[\"melanoma\"], include_preprints=true, page_size=10).\n5. Search MyVariant.info via BioMCP:variant_searcher for the BRAF p.V600E variant (gene=\"BRAF\", hgvsp=\"p.V600E\", include_cbioportal=false).\n6. Fetch detailed variant data via BioMCP:variant_getter for the top rsID returned in step 5.\n7. Query NCI’s biomarker vocabulary via BioMCP:nci_biomarker_searcher for name=\"BRAF V600E\" to obtain NCI biomarker codes.\n8. Search ClinicalTrials.gov via BioMCP:trial_searcher for open Phase 2 and 3 melanoma trials requiring those NCI biomarker codes (conditions=[\"melanoma\"], other_terms=[<codes from step 7>], recruiting_status=\"OPEN\", phase=[\"PHASE2\",\"PHASE3\"]).\n9. For each NCT ID from step 8:\n   a. Fetch core protocol via BioMCP:trial_protocol_getter.\n   b. Fetch outcome measures via BioMCP:trial_outcomes_getter.\n   c. Fetch related publications via BioMCP:trial_references_getter.\n   d. If outcomes are incomplete, fetch full trial record via BioMCP:trial_getter(detail=\"all\").\n10. Obtain current drug information via BioMCP:drug_getter for vemurafenib and dabrafenib.\n11. For each drug:\n   a. Search FDA approval records via BioMCP:openfda_approval_searcher (drug=<name>); then fetch full approval details via BioMCP:openfda_approval_getter for the leading application number.\n   b. Search official label sections via BioMCP:openfda_label_searcher (name=<name>, section=[\"indications\",\"warnings\"], limit=5).\n   c. Search serious adverse events via BioMCP:openfda_adverse_searcher (drug=<name>, serious=true, limit=20).\n12. Synthesize and cross‐validate:\n   – Compare NCI biomarker‐driven trial interventions with FDA‐approved indications and adverse event profiles.\n   – Highlight any discrepancies between trial outcomes and post‐marketing safety signals.\n\nExpected output: A structured JSON report containing sections for gene context, disease context, literature highlights, variant pathogenicity, trial landscape (with protocol and outcomes summaries), drug approval status, label warnings, and safety signal synthesis.",
      "category": "single_server",
      "ground_truth_tool": "BioMCP",
      "servers": [
        "BioMCP"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Car Price Evaluator",
        "FruityVice",
        "Google Maps",
        "National Parks",
        "OKX Exchange",
        "OpenAPI Explorer",
        "Paper Search",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: BioMCP"
    },
    {
      "query_id": "biomcp_001",
      "query": "I’m working on a melanoma project and really stuck piecing together everything about that common BRAF V600E change. My boss wants a solid briefing on how that mutation drives the disease, what recent studies are saying, and whether treatments like vemurafenib or dabrafenib are truly holding up in patients who carry it. On top of that, I need to know what clinical trials are actually enrolling V600E-positive melanoma folks right now, how those trials are set up, what outcomes they’re reporting (and if any published papers or updates back them up), and how all that lines up with the drugs’ approved uses and safety concerns. \n\nI’m not looking for vague summaries—I need hard numbers, trial IDs, approval dates, key label warnings, safety‐signal stats, that sort of thing—all from the latest half‐year or so. Can you help me pull together a clear, evidence‐backed overview covering:\n\n• The role of BRAF V600E in melanoma  \n• Highlights from recent papers on that mutation  \n• Open Phase 2/3 studies targeting it (designs, outcomes, refs)  \n• Approval status and key label sections for vemurafenib/dabrafenib  \n• Any serious adverse event patterns reported post‐approval  \n\nI’ve got to show real data and sources—nothing off the cuff—so I can recommend the best targeted strategy. Thanks!",
      "category": "single_server",
      "ground_truth_tool": "BioMCP",
      "servers": [
        "BioMCP"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Car Price Evaluator",
        "FruityVice",
        "Google Maps",
        "National Parks",
        "OKX Exchange",
        "OpenAPI Explorer",
        "Paper Search",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: BioMCP"
    },
    {
      "query_id": "call_for_papers_000",
      "query": "You are a research coordinator planning submissions for upcoming academic conferences. Using the Call for Papers:get_events tool, identify all conferences in Europe on \"Artificial Intelligence\" and \"Data Privacy\" that have open submission deadlines within the next 7 days.  \n\nSteps for the agent:\n1. Invoke Call for Papers:get_events with keywords \"Artificial Intelligence Europe\" and limit 10 to retrieve a list of AI-related events in Europe.\n2. Invoke Call for Papers:get_events with keywords \"Data Privacy Europe\" and limit 10 to retrieve a list of data privacy events in Europe.\n3. From each returned list, extract only events whose \"submission_deadline\" falls within the next 7 days (relative to today).\n4. Merge the filtered AI and Data Privacy event lists into a single list.\n5. Classify each event by urgency:\n   - Urgent (submission_deadline within next 24 hours)\n   - Normal (submission_deadline between 24 hours and 7 days)\n6. Sort the merged list by submission_deadline ascending.\n7. Produce a final table with columns: conference_name, location_city, submission_deadline (relative days from now), topic (\"AI\" or \"Data Privacy\"), and urgency classification.\n\nExpected Output Format (Markdown or plain text table):\n| conference_name | location_city | submission_deadline (days) | topic        | urgency |\n|-----------------|---------------|---------------------------|--------------|---------|\n| ...             | ...           | 1                         | AI           | Urgent  |\n| ...             | ...           | 3                         | Data Privacy | Normal  |",
      "category": "single_server",
      "ground_truth_tool": "Call for Papers",
      "servers": [
        "Call for Papers"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "FruityVice",
        "Huge Icons",
        "Math MCP",
        "Medical Calculator",
        "NASA Data",
        "OKX Exchange",
        "OSINT Intelligence",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Call for Papers"
    },
    {
      "query_id": "call_for_papers_000",
      "query": "Hey, I’m knee-deep in organizing paper submissions for my team and just noticed there are dozens of Europe-based conferences on AI and on data privacy with deadlines sneaking up in the next week. I’m kind of panicking because I don’t want to miss any last-call dates—some might even close in the next 24 hours. \n\nCould you pull together a list of those upcoming European events in artificial intelligence and data privacy that still have open calls over the next seven days? It’d be awesome if you could flag which ones are truly urgent (like closing in a day) versus those with a bit more breathing room, and jot down the city, how many days we’ve got left, and whether it’s AI or privacy. \n\nI really need solid info—actual deadlines and locations—so I can get our proposals in on time. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Call for Papers",
      "servers": [
        "Call for Papers"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "FruityVice",
        "Huge Icons",
        "Math MCP",
        "Medical Calculator",
        "NASA Data",
        "OKX Exchange",
        "OSINT Intelligence",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Call for Papers"
    },
    {
      "query_id": "call_for_papers_001",
      "query": "You are a research coordinator in a university’s sustainable energy department. Your goal is to identify the top five most relevant upcoming conferences in the next 6 months by leveraging different keyword searches and refining based on emerging subtopics.\n\nWorkflow:\n1. Parallel Search:\n   a. Call the Call for Papers:get_events tool with keywords=\"renewable energy\" and limit=10.\n   b. Call the Call for Papers:get_events tool with keywords=\"sustainable energy\" and limit=10.\n2. Filter both result sets to only keep events occurring within the next 6 months (relative to today).\n3. From the combined filtered list, extract the single-word subtopic that appears most frequently in conference titles (ignore common stop words like “and,” “the,” etc.).\n4. Conditional Refinement:\n   • If that top subtopic is “wind”, “solar”, or “hydro”, call Call for Papers:get_events again with keywords set to that subtopic + \" energy\" (for example, keywords=\"wind energy\") and limit=5.\n   • Otherwise, skip this refinement step.\n5. Aggregate:\n   • Merge all unique events from the initial two searches (after filtering) and, if performed, the refinement search.\n   • Sort the merged list by event start date ascending.\n   • Select the first 5 events.\n\nExpected Output:\nProvide a JSON array named “final_conferences” containing up to 5 objects with fields: {\"name\": string, \"start_date\": string (relative date), \"location\": string}.",
      "category": "single_server",
      "ground_truth_tool": "Call for Papers",
      "servers": [
        "Call for Papers"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "FruityVice",
        "Hugging Face",
        "Math MCP",
        "Medical Calculator",
        "NASA Data",
        "Reddit",
        "Unit Converter",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Call for Papers"
    },
    {
      "query_id": "call_for_papers_001",
      "query": "I’m working on my PhD in sustainable energy and my supervisor just asked me to pull together a shortlist of conferences happening over the next six months that I should really keep an eye on. Honestly, there are so many calls for papers out there under labels like “renewable energy” or “sustainable energy” that I’m getting lost. Could you find me about five upcoming conferences—complete with their names, when they start (relative to now), and where they’re held—and highlight any common themes? I’ve noticed terms like wind, solar or hydro seem to show up a lot in titles, so if one of those subtopics is particularly hot, maybe zoom in on that a bit more. I need to send something solid to my supervisor soon, so please back it up with real event details, not just guesses.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Call for Papers",
      "servers": [
        "Call for Papers"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "FruityVice",
        "Hugging Face",
        "Math MCP",
        "Medical Calculator",
        "NASA Data",
        "Reddit",
        "Unit Converter",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Call for Papers"
    },
    {
      "query_id": "car_price_evaluator_000",
      "query": "You are a market analyst for an automotive marketing campaign. Using the Car Price Evaluator tools, design a comprehensive report for next week’s campaign targeting both high-end trucks and budget cars, with cross-analysis on overlapping brands and motorcycle offerings.\n\nSteps:\n1. Fetch all truck brands by calling get_vehicles_by_type with vehicle_type=\"caminhoes\".\n2. For each truck brand returned:\n   a. Call search_car_price with the brand_name.\n   b. From the returned list of models and prices, identify models priced strictly above 100,000.\n   c. Count how many models exceed 100,000 for that brand.\n3. Select the top 3 truck brands with the highest counts of models over 100,000.\n4. Fetch all car brands by calling get_vehicles_by_type with vehicle_type=\"carros\".\n5. For each car brand returned:\n   a. Call search_car_price with the brand_name.\n   b. Compute the average model price for that brand.\n6. Select all car brands whose average model price is strictly below 60,000.\n7. Identify any brand names that appear in both the top-3 truck list and the low-cost car list (overlapping brands).\n8. If there are overlapping brands, for each overlapping brand:\n   a. Fetch the motorcycle brands by calling get_vehicles_by_type with vehicle_type=\"motos\" and filter to that brand name.\n   b. Call search_car_price for that brand name to list all motorcycle models and prices.\n9. Produce a final JSON report with:\n   - \"top_truck_brands\": list of objects {\"brand_name\", \"models_above_100k_count\"} for the top 3 trucks.\n   - \"low_cost_car_brands\": list of objects {\"brand_name\", \"average_price\"} for cars averaging below 60,000.\n   - \"overlapping_brands\": list of brand names appearing in both lists.\n   - \"overlapping_motorcycle_models\": object mapping each overlapping brand to its list of motorcycle models and prices.\n\nThe task must be executed without any external data; all information must come from the provided Car Price Evaluator tools.",
      "category": "single_server",
      "ground_truth_tool": "Car Price Evaluator",
      "servers": [
        "Car Price Evaluator"
      ],
      "distraction_servers": [
        "BioMCP",
        "Context7",
        "DEX Paprika",
        "Game Trends",
        "Medical Calculator",
        "Metropolitan Museum",
        "NASA Data",
        "Unit Converter",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Car Price Evaluator"
    },
    {
      "query_id": "car_price_evaluator_000",
      "query": "Hey, I’m prepping for a marketing push next week and could use some solid data. We need to spotlight the pickup brands that have the most models priced north of 100 000, while also highlighting car brands whose average model price sits under about 60 000. Then, if any brand shows up in both groups, I’d like to see what motorcycles they offer and how much those bikes go for. Could you pull together who the top three truck brands are (by count of six-figure models), which car brands make the budget cut, and any overlaps—and for those overlaps list out the bike models and their prices? I really need actual counts, averages, and price tags so I can back up my plan with real numbers, not just gut feelings. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Car Price Evaluator",
      "servers": [
        "Car Price Evaluator"
      ],
      "distraction_servers": [
        "BioMCP",
        "Context7",
        "DEX Paprika",
        "Game Trends",
        "Medical Calculator",
        "Metropolitan Museum",
        "NASA Data",
        "Unit Converter",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Car Price Evaluator"
    },
    {
      "query_id": "car_price_evaluator_001",
      "query": "You are asked to perform a market segmentation analysis of all car brands in the Brazilian FIPE database. First, fetch the complete list of car brands by calling get_vehicles_by_type with vehicle_type set to \"carros\". Then, for each returned brand_name, call search_car_price to retrieve all model prices and compute that brand’s average market price. Classify each brand into one of three price segments: low for average price below 40 000 BRL, mid for average price between 40 000 and 80 000 BRL, and high for average price at or above 80 000 BRL. For every brand in the high segment, perform two additional checks: 1) retrieve that brand’s code by calling get_car_brands and matching on name, and 2) determine if this brand also appears in the motorcycle or truck categories by calling get_vehicles_by_type separately for vehicle_type \"motos\" and \"caminhoes\" and checking the returned brand lists. Finally, produce a JSON report listing all car brands with their average price, assigned segment, and—for high-segment brands—include the brand_code and a boolean field diversified_across_types indicating whether the brand appears in either motorcycles or trucks.",
      "category": "single_server",
      "ground_truth_tool": "Car Price Evaluator",
      "servers": [
        "Car Price Evaluator"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "DEX Paprika",
        "Google Maps",
        "Hugging Face",
        "Math MCP",
        "National Parks",
        "Reddit",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Car Price Evaluator"
    },
    {
      "query_id": "car_price_evaluator_001",
      "query": "Hey, I’m working on a little overview for my boss about how Brazilian car brands line up price-wise. Basically, I want to see which brands are on the cheaper end (say under R$40 000 on average), which sit in a mid-range (around R$40–80 000), and which ones are in that premium R$80 000-plus territory. For those top-tier brands, it’d be great to know if they’re big enough to also show up in bikes or trucks—and if there’s some internal brand code we can reference. At the end, I need a simple rundown with each brand’s average price, its segment (low/mid/high), and for the high-end names, their code plus a yes/no on whether they’ve diversified into motorcycles or trucks. I really need actual numbers and facts here—not just gut feelings—so I can back my recommendations with solid data. Could you help me pull this together?\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Car Price Evaluator",
      "servers": [
        "Car Price Evaluator"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "DEX Paprika",
        "Google Maps",
        "Hugging Face",
        "Math MCP",
        "National Parks",
        "Reddit",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Car Price Evaluator"
    },
    {
      "query_id": "context7_000",
      "query": "Compare the routing documentation coverage in Next.js versus Gatsby in Context7. Steps: 1) Call Context7:resolve-library-id with libraryName=\"next.js\". 2) Call Context7:resolve-library-id with libraryName=\"gatsby\". 3) For each resolved ID, call Context7:get-library-docs with topic=\"routing\" and tokens=5000. 4) Extract the code snippet count from each result. 5) If the absolute difference in snippet counts > 10, recommend the library with more snippets. 6) If the difference ≤ 10, call Context7:get-library-docs again for each ID with topic=\"dynamic routing\" and tokens=3000, then compare snippet counts for these refined docs. 7) Produce a final report listing for each library: resolved ID, snippet counts for both topics, and a recommendation of which library has more comprehensive routing docs. Output Format (JSON): {\n  \"comparisons\": [\n    {\"libraryID\": string, \"routingSnippets\": number, \"dynamicRoutingSnippets\": number}\n  ],\n  \"recommendedLibrary\": string\n}",
      "category": "single_server",
      "ground_truth_tool": "Context7",
      "servers": [
        "Context7"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Car Price Evaluator",
        "Medical Calculator",
        "Metropolitan Museum",
        "Movie Recommender",
        "National Parks",
        "OSINT Intelligence",
        "Scientific Computing",
        "Unit Converter",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Context7"
    },
    {
      "query_id": "context7_000",
      "query": "I’m trying to choose between Next.js and Gatsby for a new project, and my manager wants a side-by-side look at their routing docs. Basically, I need to know how many real code examples each framework includes in its routing guide. If they’re almost neck-and-neck, I’d also like to see how many snippets they each have on dynamic routing. Could you dive into both official docs, count up those snippet examples for routing and dynamic routing, and let me know which one comes out ahead? I really need hard numbers—can’t just go to the boss with gut feelings.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Context7",
      "servers": [
        "Context7"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Car Price Evaluator",
        "Medical Calculator",
        "Metropolitan Museum",
        "Movie Recommender",
        "National Parks",
        "OSINT Intelligence",
        "Scientific Computing",
        "Unit Converter",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Context7"
    },
    {
      "query_id": "context7_001",
      "query": "Your team needs to select the most Documentation-rich JavaScript frontend framework for a new single-page application requiring robust routing and state management over the upcoming week. Execute the following steps:\n\n1. Use Context7:resolve-library-id with libraryName set to \"JavaScript frontend framework\" to retrieve all matching Context7-compatible library IDs and their metadata (trust score, description, code snippet counts).\n2. From the returned list, filter libraries with trust score ≥ 8. If fewer than two libraries meet this threshold, relax the filter to trust score ≥ 7.\n3. For each of the two highest-trust libraries, in parallel perform:\n   a. Call Context7:get-library-docs with context7CompatibleLibraryID equal to the library’s ID, topic \"routing\", tokens 2000. Count the number of code snippets in the returned documentation.\n   b. Call Context7:get-library-docs with context7CompatibleLibraryID equal to the library’s ID, topic \"state management\", tokens 2000. Count the number of code snippets in the returned documentation.\n4. For each library, compute total_snippets = routing_snippets + state_management_snippets. Rank libraries by total_snippets in descending order.\n5. If the top-ranked library’s total_snippets is ≥ 50, select it. If it is < 50, then for the second-ranked library call Context7:get-library-docs with topic \"advanced patterns\", tokens 2000, count its code snippets, and compare that count against the first library’s total_snippets. Select whichever library has the higher count.\n6. Produce a JSON report structured as:\n   {\n     \"libraries\": [\n       {\"id\": \"<library ID>\", \"trust\": <trust score>, \"routing_snippets\": <number>, \"state_management_snippets\": <number>, \"total_snippets\": <number>}, ...\n     ],\n     \"final_recommendation\": {\"library_id\": \"<chosen ID>\", \"justification\": \"<brief explanation>\"}\n   }\n\nThis task requires no additional inputs and must be completed by calling only the provided Context7 tools.",
      "category": "single_server",
      "ground_truth_tool": "Context7",
      "servers": [
        "Context7"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "DEX Paprika",
        "Hugging Face",
        "Medical Calculator",
        "National Parks",
        "OSINT Intelligence",
        "Reddit",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "Single Server: Context7"
    },
    {
      "query_id": "context7_001",
      "query": "Hey, I’ve got to lock in a JavaScript front-end framework for a new single-page app by next week, and routing plus solid state management are deal-breakers. I’m really prioritizing documentation that’s packed with real code examples, not just theory. Could you check out the two most highly regarded frameworks right now, tally up how many code snippets they each have for routing and for state handling, and see which one comes out ahead? If the front-runner has roughly 50 or more total snippets in those areas, I’ll go with that. If it falls short, I’d also want to know how many “advanced patterns” examples the second tool has and then pick whichever has more. I need actual counts to back this up—no vague opinions—so I can make a strong case to the team.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Context7",
      "servers": [
        "Context7"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "DEX Paprika",
        "Hugging Face",
        "Medical Calculator",
        "National Parks",
        "OSINT Intelligence",
        "Reddit",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "Single Server: Context7"
    },
    {
      "query_id": "dex_paprika_000",
      "query": "Perform a cross‐network comparative analysis of liquidity and volatility for the top pools on Ethereum and Solana, and evaluate USDC trading activity.  \n1. Call getNetworks to obtain all supported network IDs and identify “ethereum” and “solana.”  \n2. For each of these two networks, call getNetworkPools with orderBy set to “volume_usd”, sort “desc”, limit 3 to retrieve the top 3 liquidity pools by USD volume.  \n3. For each of the 6 pools obtained in step 2:\n   a. Call getPoolDetails to retrieve tokens, reserves, and last_price_change_usd_24h.\n   b. If last_price_change_usd_24h > 5%, flag the pool as high volatility and then:\n      i. Call getPoolOHLCV with interval “24h”, start “past 30 days”, limit 30 to get daily price data.\n      ii. Call getPoolTransactions with limit 50 to inspect recent swaps, adds, and removes.\n4. Independently, call search with query “USDC” to locate the USDC token identifier globally.  \n5. For each network (“ethereum” and “solana”):\n   a. Call getTokenDetails on the USDC tokenAddress from step 4.\n   b. Call getTokenPools with that tokenAddress, orderBy “volume_usd”, sort “desc”, limit 1 to find the single most liquid USDC pool.\n   c. For that USDC pool, call getPoolDetails and getPoolOHLCV with interval “24h”, start “past 30 days”, limit 30.\n6. Finally, call getStats to gather overall DEX Paprika ecosystem metrics.  \nProduce a JSON report with these sections:\n- networkSummary: for each network, list the top 3 pools with volume_usd, token pair, and volatility flag\n- volatilePools: for each flagged pool, include its OHLCV time series (30 points) and 50 most recent transactions\n- usdcPools: for ethereum and solana, USDC pool details and OHLCV series\n- ecosystemStats: output of getStats",
      "category": "single_server",
      "ground_truth_tool": "DEX Paprika",
      "servers": [
        "DEX Paprika"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Context7",
        "Game Trends",
        "Google Maps",
        "Hugging Face",
        "Medical Calculator",
        "Metropolitan Museum",
        "NASA Data",
        "OKX Exchange",
        "Weather Data"
      ],
      "combination_name": "Single Server: DEX Paprika"
    },
    {
      "query_id": "dex_paprika_000",
      "query": "I’m putting together a DeFi deep-dive for a client who’s curious how Ethereum stacks up against that other fast chain, Solana, in terms of big-money pools and how choppy they’ve been lately. Could you help me figure out which three pools on each network are moving the most USD volume right now, and call out any that jumped or dropped by more than about 5% in the last 24 hours? For those volatile ones, I’d love to see a daily price chart for roughly the past month and a look at the most recent ~50 swaps or liquidity moves. \n\nOn top of that, I need to know where USDC is getting the most action on each chain—so what’s the single largest USDC pair by volume, and how has its price trended day-to-day over the last month? And finally, can you give me a quick snapshot of overall DEX health across the ecosystem? I really need hard numbers and real data here—I can’t go in with just opinions. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "DEX Paprika",
      "servers": [
        "DEX Paprika"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Context7",
        "Game Trends",
        "Google Maps",
        "Hugging Face",
        "Medical Calculator",
        "Metropolitan Museum",
        "NASA Data",
        "OKX Exchange",
        "Weather Data"
      ],
      "combination_name": "Single Server: DEX Paprika"
    },
    {
      "query_id": "dex_paprika_001",
      "query": "Perform a comprehensive cross-network DeFi analysis on the Ethereum ('uniswap_v3') and Polygon ('quickswap') ecosystems using DEX Paprika tools. Steps:\n1. Call getStats for high-level ecosystem metrics.\n2. Call getNetworks to confirm 'ethereum' and 'polygon' network IDs.\n3. Call getNetworkDexes for each network and verify the DEX IDs 'uniswap_v3' (Ethereum) and 'quickswap' (Polygon).\n4. Call getDexPools for each DEX with parameters: network='ethereum', dex='uniswap_v3', limit=50, orderBy='volume_usd', sort='desc' and network='polygon', dex='quickswap', limit=50, orderBy='volume_usd', sort='desc'. Select the top 5 pools by volume on each network.\n5. For each selected pool address on both networks, call:\n   a. getPoolDetails (network, poolAddress).\n   b. getPoolOHLCV (network, poolAddress, start='6 months ago', end='now', interval='24h', limit=180).\n   c. getPoolTransactions (network, poolAddress, limit=100).\n6. From each pool's getPoolDetails response, extract both tokenAddress values; for each token:\n   a. Call getTokenDetails (network, tokenAddress).\n   b. Call getTokenPools (network, tokenAddress, limit=20, orderBy='volume_usd', sort='desc').\n7. Cross-network token availability checks: for each tokenAddress from Ethereum pools call getTokenPools on network='polygon'; for each tokenAddress from Polygon pools call getTokenPools on network='ethereum'.\n8. Compute each pool’s daily price volatility: calculate the standard deviation of daily closing prices divided by the mean closing price (× 100) using the OHLCV data.\nDeliverable: A JSON report structured by network and pool, including pool address, token metadata, detailed pool info, OHLCV summary, computed volatility percentage, transaction summary, token secondary pool listings, and cross-network availability flags.",
      "category": "single_server",
      "ground_truth_tool": "DEX Paprika",
      "servers": [
        "DEX Paprika"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "FruityVice",
        "Google Maps",
        "Huge Icons",
        "Metropolitan Museum",
        "National Parks",
        "OKX Exchange",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Unit Converter"
      ],
      "combination_name": "Single Server: DEX Paprika"
    },
    {
      "query_id": "dex_paprika_001",
      "query": "I’ve got this project where my team needs a clear picture of what’s been happening on the biggest DeFi venues over the last six months—specifically Uniswap V3 on Ethereum and QuickSwap on Polygon. I’m trying to figure out which pools have been doing the heaviest trading (let’s say the top five by volume on each chain), then dig into how those pools have behaved day-to-day: price swings, rough volatility, number of trades, that kind of thing. \n\nOn top of that, I’d like to know what tokens are sitting in each of those pools, and whether those same tokens show up in any major pools on the other network. Ultimately, I want a side-by-side look at each pool’s address, token info, volume stats, daily price history (so we can calculate a volatility percentage), plus a quick snapshot of transaction counts and where else those tokens are getting traded cross-chain. \n\nSounds like a lot, I know—but I really need actual figures and solid data to back this up. Can you help me pull all that together? Whatever you find, please make sure it’s backed up by real numbers or reliable sources, okay?\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "DEX Paprika",
      "servers": [
        "DEX Paprika"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "FruityVice",
        "Google Maps",
        "Huge Icons",
        "Metropolitan Museum",
        "National Parks",
        "OKX Exchange",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Unit Converter"
      ],
      "combination_name": "Single Server: DEX Paprika"
    },
    {
      "query_id": "fruityvice_000",
      "query": "You are a nutrition scientist designing a high-fiber, moderate-sugar fruit salad mix. Follow these steps:\n1. Call FruityVice:get_fruit_nutrition with fruit_name=\"apple\". Record the returned family.\n2. If the family is \"Rosaceae\", set your second fruit to \"strawberry\"; otherwise set it to \"pineapple\". Call FruityVice:get_fruit_nutrition with that chosen fruit.\n3. Examine the sugar content (grams per 100 g) from step 2. If sugar > 5 g, set your third fruit to \"orange\"; otherwise set it to \"banana\". Call FruityVice:get_fruit_nutrition with that fruit.\n4. You now have nutrition per 100 g for three fruits. Design a 500-calorie fruit salad mix using these three fruits. Your goals:\n   • Maximize total dietary fiber.\n   • Keep total sugar below 30 g.\n   • Provide exact weights (grams) of each fruit in the mix.\n5. Calculate and report the final nutritional profile (total calories, total fiber, total sugar) of your proposed mix.\n\nExpected output format:\n{\n  \"selected_fruits\": [\"fruit1\", \"fruit2\", \"fruit3\"],\n  \"nutrition_per_100g\": {\n    \"fruit1\": {\"calories\": X, \"fiber\": Y, \"sugar\": Z},\n    \"fruit2\": {…},\n    \"fruit3\": {…}\n  },\n  \"mix_weights_grams\": {\"fruit1\": a, \"fruit2\": b, \"fruit3\": c},\n  \"mix_nutritional_summary\": {\"total_calories\": C, \"total_fiber\": F, \"total_sugar\": S}\n}",
      "category": "single_server",
      "ground_truth_tool": "FruityVice",
      "servers": [
        "FruityVice"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "Car Price Evaluator",
        "Context7",
        "DEX Paprika",
        "Game Trends",
        "Google Maps",
        "Medical Calculator",
        "NASA Data",
        "OKX Exchange"
      ],
      "combination_name": "Single Server: FruityVice"
    },
    {
      "query_id": "fruityvice_000",
      "query": "Hey, I’m tinkering with a new snack idea and could really use your help. I want to build a fruit salad that ends up at about 500 calories, loads of fiber but under roughly 30 g of sugar total. My rough plan is to kick things off with an apple, then—depending on whether it falls into the Rosaceae family—go with either a strawberry or switch to pineapple. Next, based on how sweet that second pick is (I’m eyeballing about 5 g sugar per 100 g as my cutoff), I’d add either an orange or a banana. \n\nCan you grab the real nutrition facts for each of those fruits, help me decide which ones to use, figure out exactly how many grams of each to hit the 500 calories, maximize fiber, and stay under 30 g of sugar? I’d need:\n\n- The calories, fiber, and sugar per 100 g for each selected fruit\n- The precise weights of each fruit in the mix\n- A final tally of total calories, fiber, and sugar\n\nI really need hard numbers backed by genuine data—no guessing—so I can show the results to my team. Thanks!",
      "category": "single_server",
      "ground_truth_tool": "FruityVice",
      "servers": [
        "FruityVice"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "Car Price Evaluator",
        "Context7",
        "DEX Paprika",
        "Game Trends",
        "Google Maps",
        "Medical Calculator",
        "NASA Data",
        "OKX Exchange"
      ],
      "combination_name": "Single Server: FruityVice"
    },
    {
      "query_id": "fruityvice_001",
      "query": "Design a 7-day fruit smoothie plan for a client who needs each 300 mL smoothie to contain exactly 200 grams of fruit, with no more than 30 grams of sugar per serving and at least 8 grams of dietary fiber. Start with the candidate fruits: apple, banana, orange, strawberry, kiwi, mango, pineapple, and blueberry.\n\nSteps:\n1. For each of the eight initial fruits, call FruityVice:get_fruit_nutrition to retrieve the nutritional data per 100 g (specifically sugar and fiber grams).\n2. Scale each fruit’s sugar and fiber values to a 200 g serving.\n3. Filter out any fruit that in 200 g alone would exceed 30 g sugar or provide less than 8 g fiber.\n4. If fewer than three fruits remain after filtering, add “pear” and “grape” to the candidate list and call FruityVice:get_fruit_nutrition on each new fruit, then reapply the scale-and-filter step until at least three fruits pass.\n5. From the final filtered set, identify all unique combinations of three different fruits (200 g total means ~66.7 g of each fruit per combination). For each combination, compute the exact sugar and fiber by scaling the per-100 g data. Discard any combination that violates the sugar or fiber constraints.\n6. Assign one valid three-fruit combination to each day of the 7-day plan such that every valid combination is used at least once. If there are more days than combinations, cycle through the combinations.\n7. Provide a table listing, for each day, the three fruits, their individual weights (in grams), the total sugar and total fiber of the smoothie.\n\nExpected output: A 7-row breakdown showing day number, fruit names, gram allocation for each, total sugar (g), and total fiber (g).",
      "category": "single_server",
      "ground_truth_tool": "FruityVice",
      "servers": [
        "FruityVice"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "Hugging Face",
        "Medical Calculator",
        "NASA Data",
        "National Parks",
        "NixOS",
        "Paper Search",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "Single Server: FruityVice"
    },
    {
      "query_id": "fruityvice_001",
      "query": "Hey, I’ve got a bit of a smoothie challenge for next week and could use your brain on it. My coach wants each 300 mL drink to have exactly 200 g of fruit but stay under about 30 g of sugar and still hit at least 8 g of fiber. I’m thinking about using things like apple, banana, orange, strawberry, kiwi, mango, pineapple, blueberry—and if that doesn’t give me enough options, maybe throw in pear or grape. \n\nWhat I really need is a handful of three-fruit blends (so roughly 66–67 g of each fruit) that meet those sugar and fiber limits, and then a 7-day lineup cycling through all the valid combos. Could you break down each day’s smoothie with exactly which fruits, how many grams of each, plus the total sugar and fiber? I can’t just wing this—I need real numbers to show my coach.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "FruityVice",
      "servers": [
        "FruityVice"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "Hugging Face",
        "Medical Calculator",
        "NASA Data",
        "National Parks",
        "NixOS",
        "Paper Search",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "Single Server: FruityVice"
    },
    {
      "query_id": "game_trends_000",
      "query": "You are a market analyst for an indie game publisher. Over the next 7 days, identify high-potential indie titles by cross-platform performance on Steam and the Epic Games Store. Perform the following steps:\n\n1. API HEALTH CHECK\n   • Call get_api_health.  \n   • If the Steam sub-API is unhealthy, skip direct Steam calls and use get_all_trending instead to extract Steam data.  \n   • If the Epic sub-API is unhealthy, skip direct Epic calls and use get_all_trending instead to extract Epic data.\n\n2. STEAM DATA GATHERING (or fallback)\n   a. If Steam API is healthy:\n      1) Call get_steam_trending to get the current trending games and their Steam ranking positions.\n      2) From that list, call get_steam_top_sellers and filter to only trending games whose Steam sales rank is worse (numerically greater) than 5 (i.e., trending but not top 5 sellers).\n      3) Call get_steam_most_played and intersect with the filtered list, retaining only games with a current concurrent player count of at least 5,000.\n   b. If Steam API was unhealthy, call get_all_trending once and filter its output for platform == \"Steam\" and apply the same top-sellers and player thresholds above (sales rank > 5, concurrent ≥ 5,000).\n\n3. EPIC GAMES STORE DATA GATHERING (or fallback)\n   a. If Epic API is healthy:\n      1) Call get_epic_trending to get current trending titles and their Epic ranking positions.\n      2) Call get_epic_free to get the list of free games available or upcoming within the next 7 days.\n   b. If Epic API was unhealthy, call get_all_trending once and filter its output for platform == \"Epic Games Store\" and trending status, then separately call get_epic_free for free/upcoming games as above.\n\n4. CROSS-PLATFORM ANALYSIS\n   • From the Steam filtered list and the Epic trending list, identify games present in both lists (cross-platform trending).  \n     – For each, compute average rank = (Steam trending rank + Epic trending rank) / 2.  \n     – Retain only those with average rank ≤ 10.\n   • From the Steam filtered list, identify Steam-only trending games (not in Epic trending).  \n     – For each Steam-only title, verify it is not in the Epic free/upcoming list.  \n   • From the Epic trending list, identify Epic-only trending games (not in Steam filtered list).  \n     – For each Epic-only title, check if it appears in the Epic free/upcoming list.\n\n5. OUTPUT\n   Prepare a JSON report with three arrays:\n   {\n     \"cross_platform_hits\": [ { \"name\": string, \"steam_rank\": int, \"epic_rank\": int, \"average_rank\": float } ],\n     \"steam_only_opportunities\": [ { \"name\": string, \"steam_rank\": int } ],\n     \"epic_only_opportunities\": [ { \"name\": string, \"epic_rank\": int, \"is_free_next_7_days\": boolean } ]\n   }\n\nEnsure every step is automated via the specified tools and that no external data or vague parameters are used.",
      "category": "single_server",
      "ground_truth_tool": "Game Trends",
      "servers": [
        "Game Trends"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "FruityVice",
        "Google Maps",
        "Huge Icons",
        "Metropolitan Museum",
        "OKX Exchange",
        "OpenAPI Explorer",
        "Paper Search",
        "Unit Converter"
      ],
      "combination_name": "Single Server: Game Trends"
    },
    {
      "query_id": "game_trends_000",
      "query": "I’m working at a small indie game publisher and my boss wants me to spot any breakout titles over the next week. What I’m really after are those under-the-radar games that haven’t cracked the top five bestsellers but are still pulling in roughly 5,000 concurrent players. If any of those are buzzing on both Steam and Epic, I’d love to see their individual ranks and an averaged ranking—only if that average comes out to ten or below. For games that only pop on Steam, make sure they’re not quietly heading into any Epic free-to-play or upcoming giveaways. And for anything only trending on Epic, flag if it’s set to go free in the next seven days. Could you put together a shortlist laid out like that, with real rank numbers, player counts, and free-status notes? I really need hard data to bring back to my team, not just gut feelings.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Game Trends",
      "servers": [
        "Game Trends"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "FruityVice",
        "Google Maps",
        "Huge Icons",
        "Metropolitan Museum",
        "OKX Exchange",
        "OpenAPI Explorer",
        "Paper Search",
        "Unit Converter"
      ],
      "combination_name": "Single Server: Game Trends"
    },
    {
      "query_id": "game_trends_001",
      "query": "You are a gaming market analyst preparing a cross‐platform trend report for an upcoming week’s marketing campaign. Perform the following steps in order, using only the provided Game Trends tools:\n\n1. Verify the health of the analytics API to ensure all endpoints are operational.\n2. Fetch the comprehensive list of trending games across Steam and Epic for the past week.\n3. From that combined list, extract the top 10 unique game titles.\n4. For each of those 10 titles, query Steam to determine:\n   a. Real‐time peak concurrent players (using get_steam_most_played).\n   b. Live sales rank (using get_steam_top_sellers).\n5. In parallel, for those same 10 titles, check Epic’s store:\n   a. Whether each title is currently free or will become free in the upcoming week (using get_epic_free_games).\n   b. Its trending score on Epic (using get_epic_trending_games).\n6. Combine and cross‐validate data:\n   - If a title has peak players above 50,000 on Steam and is free on Epic, flag it as a “High‐Impact Free Play.”\n   - If a title ranks in Steam’s top 20 sellers but has fewer than 10,000 peak players, flag it as “Sales‐Driven.”\n   - All other titles should be categorized as “Standard Trend.”\n7. Generate a final report listing the 10 titles, their Steam peak players, Steam sales rank, Epic free status, Epic trending score, and assigned category. Output the report as a JSON array of objects.",
      "category": "single_server",
      "ground_truth_tool": "Game Trends",
      "servers": [
        "Game Trends"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Context7",
        "DEX Paprika",
        "FruityVice",
        "Google Maps",
        "Medical Calculator",
        "NixOS",
        "OSINT Intelligence",
        "Reddit"
      ],
      "combination_name": "Single Server: Game Trends"
    },
    {
      "query_id": "game_trends_001",
      "query": "Hey, I’m putting together a pitch for next week’s gaming campaign and I need a clear picture of what’s really popping on both Steam and Epic over the past seven days. I’m curious which titles made it into the top tier of buzz on each store, and then for the ten biggest hitters I’d like to know two things on Steam: what their peak live player counts looked like and where they sit in the sales charts right now. At the same time, I want to check on Epic whether those same games are free today or dropping free soon, and how hot they are on Epic’s trending list.\n\nI’m trying to spot the sweet spots—like games that have huge Steam crowds (say above about fifty-thousand at peak) but are free on Epic, which I’d flag as “High-Impact Free Play,” or stuff that’s selling really well on Steam (top twenty sellers) but isn’t filling its lobbies (under around ten-thousand peak) that I’d call “Sales-Driven.” Everything else would just be “Standard Trend.” \n\nCould you pull together real numbers for those ten games—Steam peak player counts, Steam sales ranks, Epic free status, Epic trending scores—and label each one with the category above? I’d really appreciate a neat, data-driven rundown (ideally something I can drop straight into a JSON-style report) because I need hard evidence to show the team, not just gut feelings.",
      "category": "single_server",
      "ground_truth_tool": "Game Trends",
      "servers": [
        "Game Trends"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Context7",
        "DEX Paprika",
        "FruityVice",
        "Google Maps",
        "Medical Calculator",
        "NixOS",
        "OSINT Intelligence",
        "Reddit"
      ],
      "combination_name": "Single Server: Game Trends"
    },
    {
      "query_id": "huge_icons_000",
      "query": "You are building a cross-platform design system that must include five core icons: home, settings, notification, search, and user-profile.  \n1. Use Huge Icons:list_icons to retrieve the complete master list of available icons.  \n2. From that list, determine which of the five core icons already exist exactly as named.  \n3. For each core icon not found, perform an alternative search using Huge Icons:search_icons with the following fallback synonyms:  \n   • user-profile → person, account  \n   • notification → alert, bell  \n   • settings → gear, cog  \n4. If an icon is still not found after synonyms, mark it as missing and stop further searches for it.  \n5. For every icon you have successfully located (exact name or via synonyms), retrieve platform-specific usage instructions by calling Huge Icons:get_platform_usage for each of the six platforms in this sequence: react, vue, angular, svelte, react-native, flutter.  \n6. Cross-validate that each icon has valid usage instructions on all six platforms. If any platform returns an error or empty instructions for a given icon, log that platform as unsupported for that icon.  \n\nExpected output: A JSON object listing each core icon with these fields:  \n• icon_name: the exact icon name used (original or synonym)  \n• found_by: “exact” or “synonym:<term>”  \n• platforms_supported: list of platforms with valid usage instructions  \n• platforms_missing: list of platforms that returned no instructions  \n• final_status: “complete” if supported on all six, otherwise “partial” or “missing” if no search result at all",
      "category": "single_server",
      "ground_truth_tool": "Huge Icons",
      "servers": [
        "Huge Icons"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "Car Price Evaluator",
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Medical Calculator",
        "NixOS",
        "Paper Search",
        "Unit Converter"
      ],
      "combination_name": "Single Server: Huge Icons"
    },
    {
      "query_id": "huge_icons_000",
      "query": "Hey, I’m wrapping up a new UI kit for my app and there are five icons I absolutely need—home, search, user-profile, notification and settings—but I’m not sure they all show up under those exact names in the library I’m using. For example, I’ve seen “user-profile” turned into “person” or “account,” notifications sneak in as “bell” or “alert,” and settings sometimes go by “gear” or “cog.” Could you dig in and see which ones are available under the exact or fallback names, then give me the actual import or usage snippets for React, Vue, Angular, Svelte, React Native and Flutter? If any icon doesn’t exist at all or a framework can’t handle one, just flag it so I know what’s missing or only partially supported. I really need real code examples, not just guesses, so I can hand it straight to my team.",
      "category": "single_server",
      "ground_truth_tool": "Huge Icons",
      "servers": [
        "Huge Icons"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "Car Price Evaluator",
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Medical Calculator",
        "NixOS",
        "Paper Search",
        "Unit Converter"
      ],
      "combination_name": "Single Server: Huge Icons"
    },
    {
      "query_id": "huge_icons_001",
      "query": "You are preparing a comprehensive cross-platform usage guide for the Hugeicons icon set covering React, Vue, Angular, Svelte, React-Native, and Flutter. Follow these steps exactly:\n\n1. Call Huge Icons:list_icons to retrieve the full inventory of icons. Record the total count as `universe_count`.\n2. Verify that the inventory contains at least 10 distinct icons. If fewer than 10 icons exist, stop and report an error.\n3. For each of these six functional categories: “home”, “search”, “notification”, “settings”, “user”, and “logout,” perform:  \n   a. Call Huge Icons:search_icons with `query` set to the category name (e.g., “home”).  \n   b. If no results are returned, immediately retry with the fallback query “<category>, outline” (e.g. “home, outline”).  \n   c. From the search results, select the very first icon name as the chosen icon for that category.  \n   d. Check that the chosen icon appears in the original universe list. If it does not, log a warning and mark `fallback_used: true`.  Otherwise, set `fallback_used: false`.\n4. For each of the six platforms: react, vue, angular, svelte, react-native, flutter, call Huge Icons:get_platform_usage to fetch usage instructions.  \n5. Assemble a mapping table (`icon_mapping`), one entry per category, containing:\n   - `category`: the functional category name\n   - `icon_name`: the chosen Hugeicons icon name\n   - `fallback_used`: true or false\n   - `platforms`: an object mapping each platform to the corresponding usage instructions returned in step 4\n6. Using the React instructions from step 4, craft a sample import and JSX usage snippet for the “home” icon and store it as `react_home_example`.  \n7. Return a JSON object with these keys:\n   - `universe_count` (integer)\n   - `icon_mapping` (array of objects as described above)\n   - `react_home_example` (string containing copy-paste ready code)\n\nYour output must be fully self-contained and formatted as valid JSON exactly as specified.",
      "category": "single_server",
      "ground_truth_tool": "Huge Icons",
      "servers": [
        "Huge Icons"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "DEX Paprika",
        "Google Maps",
        "Math MCP",
        "Medical Calculator",
        "Movie Recommender",
        "OKX Exchange",
        "OSINT Intelligence",
        "OpenAPI Explorer"
      ],
      "combination_name": "Single Server: Huge Icons"
    },
    {
      "query_id": "huge_icons_001",
      "query": "I’m putting together docs for the Hugeicons set in my cross-platform component library and could really use some hard numbers and copy-and-paste code. First off, how many icons are in the entire collection? I need at least ten to make this guide worthwhile—if it’s under ten, let me know so I can rethink my approach. \n\nThen for the six core UI bits—home, search, notifications, settings, user, and logout—I’d like you to pick the very first icon that matches each name, but if it doesn’t show up try the “outline” version instead. Once you’ve chosen those, could you walk me through exactly how to import and use each one in React, Vue, Angular, Svelte, React Native, and Flutter? Finally, I need a ready-to-go React snippet for the home icon. \n\nIt would be amazing if you could bundle the whole thing—total icon count, a mapping of category to icon name (noting if you had to fall back), plus the usage instructions for each platform, and the React home example—in one JSON object I can drop straight into my docs. I really need concrete data and real code, not just general advice.",
      "category": "single_server",
      "ground_truth_tool": "Huge Icons",
      "servers": [
        "Huge Icons"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "DEX Paprika",
        "Google Maps",
        "Math MCP",
        "Medical Calculator",
        "Movie Recommender",
        "OKX Exchange",
        "OSINT Intelligence",
        "OpenAPI Explorer"
      ],
      "combination_name": "Single Server: Huge Icons"
    },
    {
      "query_id": "hugging_face_000",
      "query": "You are building a research pipeline to select and evaluate a pre-trained English text-classification model for fine-tuning on a spam-detection task. Execute the following steps without asking for further parameters:\n\n1. Call Hugging Face:search-models with {\"query\": \"text-classification\", \"author\": \"distilbert\", \"tags\": \"text-classification\", \"limit\": 5}.\n2. For each model_id returned, call Hugging Face:get-model-info with {\"model_id\": <model_id>}.\n3. From those, keep only models with \"parameters\" ≤ 700 million AND \"license\" == \"apache-2.0\". If none match, repeat step 1 with author=\"bert\" instead of \"distilbert\" and reapply step 2–3.\n4. Parallel to step 3, call Hugging Face:search-datasets with {\"query\": \"spam\", \"tags\": \"text-classification\", \"limit\": 3} and Hugging Face:search-spaces with {\"query\": \"spam-classification\", \"tags\": \"text-classification\", \"sdk\": \"gradio\", \"limit\": 2}.\n5. For each dataset_id from step 4, call Hugging Face:get-dataset-info with {\"dataset_id\": <dataset_id>}. Keep datasets with \"train.num_rows\" ≥ 20000.\n6. For each space_id from step 4, call Hugging Face:get-space-info with {\"space_id\": <space_id>}. Discard any space whose \"sdk\" ≠ \"gradio\" or that reports no live demo metrics.\n7. Call Hugging Face:get-daily-papers (no parameters) to retrieve the list of today’s papers. From that list, select any paper whose title or abstract mentions “spam classification” and note its arxiv_id; if none, pick the first five papers.\n8. For each selected arxiv_id, call Hugging Face:get-paper-info with {\"arxiv_id\": <arxiv_id>} and extract reported dataset names and benchmark scores.\n9. Call Hugging Face:search-collections with {\"query\": \"spam classification\", \"limit\": 2}. For each returned {\"namespace\",\"collection_id\"}, call Hugging Face:get-collection-info with those two fields.\n\nDeliverable: A JSON report listing (a) chosen model_id, parameters, license; (b) chosen dataset_id and train.num_rows; (c) vetted space_ids with demo metrics; (d) paper arxiv_ids with extracted scores; (e) collection namespaces and collection_ids with their descriptions; and (f) your final recommendation of the best model+dataset pairing for fine-tuning.",
      "category": "single_server",
      "ground_truth_tool": "Hugging Face",
      "servers": [
        "Hugging Face"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "DEX Paprika",
        "FruityVice",
        "NASA Data",
        "NixOS",
        "OKX Exchange",
        "Paper Search",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Hugging Face"
    },
    {
      "query_id": "hugging_face_000",
      "query": "Hey, I’m knee-deep in setting up a spam filter for a side project and could really use some hard data to make a solid call. I’ve been poking around for a pre-trained English classifier that’s not too huge (ideally something under roughly 700 million parameters) and is released under an Apache-2.0-style license. I first checked out a few DistilBERT-ish models, but if none fit the bill, I guess I could fall back to something BERT-based. \n\nAt the same time, I need a dataset with at least around 20 k training examples so it doesn’t feel too flimsy, and I’d love to trial a couple of live demos—preferably built with something like Gradio—just to see how they actually perform on spammy text. \n\nAlso, since keeping up with the latest is crucial, I want to skim today’s fresh papers and see if any mention spam classification; if nothing jumps out, I’m okay with looking at the first handful for any useful benchmark scores or datasets they report. Oh, and if there are any community collections focusing on spam classification, I’d like a peek at those too.\n\nCould you pull together:\n- Details on the best fitting model (name, parameter count, license)\n- Dataset info (ID, train-size)\n- Any live demo spaces you find (with actual performance metrics)\n- A few of today’s papers that talk about spam filtering, with their datasets and scores\n- And any relevant collections or curated sets around spam classification\n\nThen, based on all that evidence—numbers, links, whatever—I’d love a recommendation for which model+dataset pairing seems strongest for fine-tuning. I really need concrete figures and sources so I can walk my boss through it with confidence, not just guesses. Thanks!",
      "category": "single_server",
      "ground_truth_tool": "Hugging Face",
      "servers": [
        "Hugging Face"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "DEX Paprika",
        "FruityVice",
        "NASA Data",
        "NixOS",
        "OKX Exchange",
        "Paper Search",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Hugging Face"
    },
    {
      "query_id": "hugging_face_001",
      "query": "Design a robust end-to-end English-to-French translation pipeline using only Hugging Face Hub resources. Perform the following steps without any external data: 1) Search for the top 5 pretrained models authored by \"google\" tagged \"translation\"; 2) For each model, fetch detailed info and filter to those under 1 billion parameters; 3) Search for open-source datasets authored by \"opus\" tagged \"translation_en_to_fr\" and fetch their info, keeping only those with at least 10 000 examples; 4) From the filtered models and datasets, form the top 3 model–dataset pairs ranked by dataset size; 5) For each of these 3 pairs, search for Spaces demonstrating inference (query by model_id and dataset_id) and fetch detailed Space info; 6) Retrieve the daily curated papers, filter to those mentioning any chosen model_id or dataset_id, and fetch full paper info for up to 3 relevant papers; 7) Search for Collections owned by \"google\" or \"opus\" that include items matching your model_ids or dataset_ids and fetch their collection info; 8) Compile a final JSON report with one entry per model–dataset pair containing: { \"model_id\", \"model_size\", \"dataset_id\", \"dataset_size\", \"space_url\", \"paper_list\": [ {\"arxiv_id\",\"title\",\"summary\"}, … ], \"collection_links\" } and rank entries by descending dataset_size.",
      "category": "single_server",
      "ground_truth_tool": "Hugging Face",
      "servers": [
        "Hugging Face"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "Context7",
        "Game Trends",
        "Google Maps",
        "Metropolitan Museum",
        "National Parks",
        "Paper Search",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Hugging Face"
    },
    {
      "query_id": "hugging_face_001",
      "query": "I’ve got this side project where I need to set up an English-to-French translation workflow, but I’m only allowed to use what’s already on Hugging Face. I’m a bit stuck figuring out which of Google’s translation models are both top quality and still on the lean side (maybe under a billion parameters?), and which of the OPUS English-to-French datasets have enough examples to actually work well (I’m thinking at least around ten thousand). \n\nIdeally I’d love to land on the three strongest model-dataset pairings, ranked by the dataset’s size—so I can show my boss some concrete options. And once those are picked, I’d also like to see if there are any live demos or Spaces where I can test them out, plus any recent papers that actually mention those exact models or datasets. Oh, and if Google or OPUS have bundled any of these into collections, point me to those too. \n\nI really need hard numbers, precise model sizes and dataset counts, direct links to demos, papers or collections—nothing vague. Can you dig up all that evidence for me?",
      "category": "single_server",
      "ground_truth_tool": "Hugging Face",
      "servers": [
        "Hugging Face"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "Context7",
        "Game Trends",
        "Google Maps",
        "Metropolitan Museum",
        "National Parks",
        "Paper Search",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Hugging Face"
    },
    {
      "query_id": "math_mcp_000",
      "query": "You are provided with quarterly yield data (in tons) from 10 farms for the past quarter: [120, 150, 150, 200, 180, 170, 160, 140, 130, 155]. Perform the following calculations using the Math MCP tools in sequence:\n\n1. Compute total yield using Math MCP:sum.\n2. Compute average yield using Math MCP:mean.\n3. Compute median yield using Math MCP:median.\n4. Compute mode yield using Math MCP:mode.\n5. Determine minimum yield using Math MCP:min.\n6. Determine maximum yield using Math MCP:max.\n7. Calculate yield range (max minus min) using Math MCP:subtract.\n8. Calculate total revenue by multiplying total yield by a fixed price of $30 per ton using Math MCP:multiply.\n9. Calculate total fixed cost by multiplying the number of farms (10) by a fixed cost of $2,000 per farm using Math MCP:multiply.\n10. Compute net profit by subtracting total fixed cost from total revenue using Math MCP:subtract.\n11. Compute profit margin ratio by dividing net profit by total revenue using Math MCP:division.\n12. Convert the profit margin ratio to a percentage by multiplying by 100 using Math MCP:multiply, then round to the nearest integer using Math MCP:round.\n13. Compute deviation between maximum yield and average yield using Math MCP:subtract. If this deviation exceeds 30 tons, compute an extra fertilizer budget by multiplying the deviation by $10 per ton using Math MCP:multiply and then rounding up with Math MCP:ceiling. If the deviation is 30 tons or less, set the extra fertilizer budget to $500 and round down to the nearest integer using Math MCP:floor.\n\nProvide a final report listing: total yield, average yield, median yield, mode yield, min yield, max yield, yield range, total revenue, total fixed cost, net profit, profit margin percentage, deviation, and final fertilizer budget.",
      "category": "single_server",
      "ground_truth_tool": "Math MCP",
      "servers": [
        "Math MCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "Car Price Evaluator",
        "Context7",
        "DEX Paprika",
        "Google Maps",
        "Huge Icons",
        "Hugging Face",
        "Movie Recommender",
        "Paper Search"
      ],
      "combination_name": "Single Server: Math MCP"
    },
    {
      "query_id": "math_mcp_000",
      "query": "I’m pulling together a report on last quarter’s harvest from our 10 farms, and honestly I need some hard numbers. We recorded yields of 120, 150, 150, 200, 180, 170, 160, 140, 130, and 155 tons. \n\nHere’s what I’m trying to nail down:\n- What’s our total output, average yield per farm, the median and the most common harvest size, plus our lowest and highest yields and the overall spread?\n- Then, at $30 a ton, what does that translate to in revenue?\n- After covering $2,000 in fixed costs per farm (so 10 farms total), what’s left as net profit and what’s our profit margin when you express it as a percentage (rounded to the nearest whole number)?\n- Finally, I’m curious about the gap between our top-performing farm (200 tons) and the average yield—if that difference is more than 30 tons, I want to budget extra fertilizer at $10 per ton of that gap (and round up); if it’s 30 or less, I’ll stick with a $500 allowance (and round down).\n\nCould you crunch all those figures? I really need solid data—can’t go to my boss with just guesses. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Math MCP",
      "servers": [
        "Math MCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "BioMCP",
        "Car Price Evaluator",
        "Context7",
        "DEX Paprika",
        "Google Maps",
        "Huge Icons",
        "Hugging Face",
        "Movie Recommender",
        "Paper Search"
      ],
      "combination_name": "Single Server: Math MCP"
    },
    {
      "query_id": "math_mcp_001",
      "query": "You are given the monthly sales figures (number of units sold) for a product over the past 6 months: [120, 150, 130, 170, 150, 160]. Perform the following analyses in sequence using the Math MCP tools:\n\n1. Compute the total sales for these 6 months.  (Math MCP:sum)\n2. Calculate the arithmetic mean of the 6 monthly figures.  (Math MCP:mean)\n3. Find the median sales value.  (Math MCP:median)\n4. Determine the mode (most frequent sales value).  (Math MCP:mode)\n5. Identify the maximum and minimum sales values.  (Math MCP:max and Math MCP:min)\n6. Compute the ratio of the highest month to the lowest month (max divided by min).  (Math MCP:division)\n7. Calculate the skewness of the distribution as (mean minus median).  (Math MCP:subtract)\n8. If the skewness is positive, round it up using ceiling; if skewness is zero or negative, round its absolute value down using floor.  (Math MCP:ceiling or Math MCP:floor)\n9. Assume the business wants an average of 180 units per month over the upcoming 7 months. Compute the total units required to meet this target.  (Math MCP:multiply)\n10. Determine how many additional units are needed next month by subtracting the already achieved total sales (from step 1) from the 7-month target total.  (Math MCP:subtract)\n11. Round the additional units needed next month to the nearest integer.  (Math MCP:round)\n\nFinally, present an executive summary in JSON format containing these fields: total_sales, average_sales, median_sales, mode_sales, max_sales, min_sales, max_to_min_ratio, skewness, adjusted_skewness, target_total_7_months, additional_needed_next_month_exact, additional_needed_next_month_rounded.",
      "category": "single_server",
      "ground_truth_tool": "Math MCP",
      "servers": [
        "Math MCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "DEX Paprika",
        "FruityVice",
        "Metropolitan Museum",
        "OKX Exchange",
        "OSINT Intelligence",
        "Paper Search",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "Single Server: Math MCP"
    },
    {
      "query_id": "math_mcp_001",
      "query": "Hey, I’ve been digging into my sales over the last six months—120, 150, 130, 170, 150 and 160 units—and I’m honestly a bit lost on how to pull it all together for my boss. Could you help me figure out where I stand overall (like total sales, average, median, and which month number appeared most often), spot the best and worst months, and even see how the top month compares to the bottom as a ratio? \n\nAlso, I heard it’s useful to look at how skewed things are by subtracting the median from the mean, and then rounding that skewness differently depending on whether it’s positive or not. On top of that, we’re aiming for an average of 180 units over the next seven months—so I need to know the total target for those seven months and exactly how many extra units I’d have to push next month to hit it (rounded to a whole number). \n\nCould you put all of that into a clean JSON summary (with fields like total_sales, average_sales, median_sales, mode_sales, max_sales, min_sales, max_to_min_ratio, skewness, adjusted_skewness, target_total_7_months, additional_needed_next_month_exact, additional_needed_next_month_rounded)? I really need real numbers for every piece so I can back it up properly—no guesses, just solid calculations.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Math MCP",
      "servers": [
        "Math MCP"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "DEX Paprika",
        "FruityVice",
        "Metropolitan Museum",
        "OKX Exchange",
        "OSINT Intelligence",
        "Paper Search",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "Single Server: Math MCP"
    },
    {
      "query_id": "nixos_000",
      "query": "Assess the viability of deploying “neovim” on NixOS stable channel with reproducible builds, Home Manager configuration, nix-darwin support, and community flakes. The agent must:\n\n1. List all available NixOS channels.\n2. Search the stable NixOS channel for “neovim” in the packages category.\n3. Fetch detailed info about the “neovim” package from the stable channel.\n4. Retrieve NixOS statistics for the stable channel.\n5. Attempt to find the specific version “0.9.2” of “neovim” in NixHub. If not found, fall back to retrieving the latest 5 versions of “neovim” from NixHub.\n6. Search the NixOS flakes index for “neovim” and retrieve flake statistics.\n7. Search Home Manager options for “programs.neovim”. If found:\n   a. Get detailed info on the exact Home Manager option.\n   b. List all Home Manager options under the “programs.neovim” prefix.\n8. Retrieve overall Home Manager statistics.\n9. Search nix-darwin configuration options for “neovim”. If found:\n   a. Get detailed info on the exact nix-darwin option.\n   b. List all nix-darwin options under the prefix that includes “neovim”.\n10. Retrieve overall nix-darwin statistics.\n\nProduce a consolidated report that includes:\n- NixOS channel availability and package details.\n- Channel package/option counts.\n- Version reproducibility data from NixHub (commit hashes or fallback list).\n- Number and metadata of community flakes providing “neovim”.\n- Home Manager support depth (option details, sub-options, overall stats).\n- nix-darwin support (option details, sub-options, overall stats).",
      "category": "single_server",
      "ground_truth_tool": "NixOS",
      "servers": [
        "NixOS"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "Context7",
        "FruityVice",
        "Game Trends",
        "Google Maps",
        "Huge Icons",
        "Movie Recommender",
        "National Parks",
        "OSINT Intelligence",
        "Scientific Computing"
      ],
      "combination_name": "Single Server: NixOS"
    },
    {
      "query_id": "nixos_000",
      "query": "I’ve been wrestling with setting up Neovim in a truly rock-solid NixOS environment and could really use a clear snapshot of where things stand. Here’s the deal: I’m on the stable NixOS channel, but I’m not even sure which channels are still alive or where Neovim lives in each. I’d love to know if the specific 0.9.2 release is packaged there—if it isn’t, what are the last few Neovim versions I could grab reproducibly? On top of that, I’m dabbling with flakes and want to see how many community flakes actually offer Neovim and what the download stats look like. Then there’s Home Manager and nix-darwin—does “programs.neovim” show up in their option trees, what does its entry look like, and how deep does the support go? Basically, I need hard numbers and real metadata—channel names, package counts, version hashes or fallback lists, flake counts, option paths, anything that proves this is actually supported end to end. I can’t go forward on gut feelings alone, so whatever you find, make sure it’s backed up by concrete data.",
      "category": "single_server",
      "ground_truth_tool": "NixOS",
      "servers": [
        "NixOS"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "Context7",
        "FruityVice",
        "Game Trends",
        "Google Maps",
        "Huge Icons",
        "Movie Recommender",
        "National Parks",
        "OSINT Intelligence",
        "Scientific Computing"
      ],
      "combination_name": "Single Server: NixOS"
    },
    {
      "query_id": "nixos_001",
      "query": "You are a DevOps engineer tasked with designing a fully reproducible, cross-platform Python 3.10 data-analysis environment that works on both NixOS (using flakes on the unstable channel) and macOS (using nix-darwin). Your deliverables:\n1) Identify the exact NixOS package name for Python 3.10 on channel “unstable.”\n2) Retrieve its detailed package info.\n3) Obtain its version history and find the exact commit hash for version 3.10.8.\n4) Search the Nix flakes index for a community flake that provides Python 3.10.8 or later, and select the best candidate.\n5) From Home Manager, discover and configure the option group that manages Python packages and Jupyter Notebook integration.\n6) From nix-darwin, discover and configure the parallel option group to enable the same Python/Jupyter support on macOS.\n7) Produce a final flake.nix snippet that pins the chosen Python 3.10.8 commit hash, imports the selected flake, and sets up home-manager and darwin modules with the discovered options.\nYour output must include:\n  • The NixOS package name and details.\n  • The commit hash for Python 3.10.8.\n  • The name and metadata of the chosen flake.\n  • The Home Manager option path and configuration block for Python and Jupyter.\n  • The nix-darwin option path and configuration block for Python and Jupyter.\n  • The complete flake.nix snippet.\nAll steps are mandatory and must be executed in sequence without skipping.",
      "category": "single_server",
      "ground_truth_tool": "NixOS",
      "servers": [
        "NixOS"
      ],
      "distraction_servers": [
        "DEX Paprika",
        "Game Trends",
        "Hugging Face",
        "Metropolitan Museum",
        "Movie Recommender",
        "OKX Exchange",
        "OpenAPI Explorer",
        "Reddit",
        "Scientific Computing",
        "Wikipedia"
      ],
      "combination_name": "Single Server: NixOS"
    },
    {
      "query_id": "nixos_001",
      "query": "I’ve been banging my head against getting a rock-solid Python 3.10 setup that works exactly the same on NixOS (with flakes on unstable) and on my Mac via nix-darwin. What I really need is to pin down the precise Python 3.10 package from unstable—ideally lock in version 3.10.8 by its commit hash—then find out if there’s a community flake out there bundling that (or a later) release and pick the best one. On top of that, I want to wire it up in home-manager and nix-darwin so Jupyter and all my usual Python packages just land in my user environment without me juggling things by hand.\n\nCould you help me track down:\n\n• The exact NixOS package name for Python 3.10 on the unstable channel and its full package metadata?  \n• The version history so I can grab the commit hash for 3.10.8?  \n• A good community flake that already includes Python 3.10.8 or above (with name and any relevant metadata)?  \n• The right option path and example config block in home-manager to enable Python packages plus Jupyter Notebook?  \n• The parallel option group and config snippet for nix-darwin to get the same Python/Jupyter support on macOS?  \n• Finally, a complete flake.nix snippet that pins that exact commit, imports the chosen flake, and sets up both home-manager and darwin modules with those options?\n\nI really need the actual values—package names, commit hashes, option names/paths, config blocks, etc.—so I can hand this over to my team and prove it’s rock solid. Thanks!",
      "category": "single_server",
      "ground_truth_tool": "NixOS",
      "servers": [
        "NixOS"
      ],
      "distraction_servers": [
        "DEX Paprika",
        "Game Trends",
        "Hugging Face",
        "Metropolitan Museum",
        "Movie Recommender",
        "OKX Exchange",
        "OpenAPI Explorer",
        "Reddit",
        "Scientific Computing",
        "Wikipedia"
      ],
      "combination_name": "Single Server: NixOS"
    },
    {
      "query_id": "osint_intelligence_000",
      "query": "You are tasked with building a comprehensive threat profile for the domain example-inc.com. Perform the following steps without human intervention:\n1. Run a DNSTwist analysis on example-inc.com to generate all typographical variants.\n2. If DNSTwist returns more than five variants, pick the five highest-risk variants by edit distance. Otherwise, use them all.\n3. For example-inc.com and for each selected variant:\n   a. Perform a DNSRecon lookup to enumerate subdomains and NS/MX records.\n   b. Perform a Dig lookup to retrieve A, AAAA, and CNAME records.\n   c. Cross-validate the records from DNSRecon and Dig; flag any record present in one but missing in the other.\n   d. For each hostname discovered, perform a Host lookup to resolve to IP addresses. Consolidate unique IPs.\n   e. For each unique IP:\n      i. Run Nmap scan targeting ports 22, 80, and 443.\n      ii. If port 22 is open, note potential SSH exposure; if ports 80/443 are open, note HTTP/S service.\n      iii. Perform a Whois lookup on the IP to retrieve the network owner and geolocation.\n   f. Perform a Whois lookup on the domain itself (example-inc.com or variant) to retrieve registrar and registration dates.\n4. After processing all domains and IPs:\n   a. Identify any variant domain whose IP ranges overlap with example-inc.com’s IP space; tag as “sibling domain.”\n   b. Compare registrar from domain Whois with network owner from IP Whois; flag discrepancies.\n   c. Summarize all open-port findings and registrar/network mismatches in a JSON report.\n\nExpected output format:\n{\n  \"domain_profile\": [\n    {\n      \"domain\": \"example-inc.com\",          // or variant\n      \"whois_registrar\": \"...\",\n      \"variant_tag\": \"primary|typo-squat\",\n      \"subdomains_count\":  ...,\n      \"dns_record_discrepancies\": [...],\n      \"hosts\": [\n        {\n          \"hostname\": \"api.example-inc.com\",\n          \"ip\": \"203.0.113.45\",\n          \"nmap_open_ports\": [22, 443],\n          \"ip_whois_owner\": \"Example Hosting LLC\",\n          \"ssh_exposure\": true\n        },\n        ...\n      ]\n    }\n  ],\n  \"sibling_domains\": [...],\n  \"registrar_network_mismatches\": [...]\n}",
      "category": "single_server",
      "ground_truth_tool": "OSINT Intelligence",
      "servers": [
        "OSINT Intelligence"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "Context7",
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Hugging Face",
        "National Parks",
        "Scientific Computing",
        "Wikipedia"
      ],
      "combination_name": "Single Server: OSINT Intelligence"
    },
    {
      "query_id": "osint_intelligence_000",
      "query": "I’ve got a bit of a situation with the domain example-inc.com. My boss wants a full picture of any look-alike sites, subdomains and exposed services so we can see if someone’s squatting on typos or even hosting malicious stuff on the same network. I’m not even sure how many variants there might be if you fuzz it a bit, and I don’t want to miss a single suspicious spelling error or clone that ends up pointing back to our own servers. \n\nCould you help me track down all the possible typo-style domains that resemble example-inc.com, figure out which ones pose the biggest risk, and then map out what subdomains they’ve got? I also need to know every IP they resolve to, what ports are open (especially SSH or web ports), and who technically “owns” each IP and domain from a registration standpoint. And if any of those look-alikes share IP ranges with the real example-inc.com, flag that as a potential “sibling” setup. \n\nAt the end, I really need solid numbers or output—like actual DNS/DNS record details, open-port findings, and registrar versus network-owner info—so I can show my team real evidence. Does that make sense?",
      "category": "single_server",
      "ground_truth_tool": "OSINT Intelligence",
      "servers": [
        "OSINT Intelligence"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "Context7",
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Hugging Face",
        "National Parks",
        "Scientific Computing",
        "Wikipedia"
      ],
      "combination_name": "Single Server: OSINT Intelligence"
    },
    {
      "query_id": "osint_intelligence_001",
      "query": "You are a security analyst investigating a recently registered domain fakeshoponline.com that has appeared in potential phishing reports over the past 3 months. Your objective is to map its DNS and hosting infrastructure, enumerate subdomains, discover typosquatting variants, scan for active services, and cross-validate ownership data to classify high-risk hosts and variants. Execute the following steps in order:\n\n1. Run dig_lookup with target=\"fakeshoponline.com\" to retrieve A, MX, NS, and TXT records.  \n2. Parse the NS records from step 1. Then run dnsrecon_lookup with target=\"fakeshoponline.com\" to enumerate subdomains over the past 3 months.  \n3. For each subdomain discovered in step 2, run host_lookup to resolve it to one or more IP addresses.  \n4. For each unique IP from step 3, run nmap_scan to identify open ports and services.  \n   • If port 25 is open on any IP, flag that host as an email relay candidate.  \n   • If ports 80 or 443 are open, flag that host as a web server candidate.  \n5. Run whois_lookup with target=\"fakeshoponline.com\" to retrieve the registrant organization and email.  \n6. Run dnstwist_lookup with domain=\"fakeshoponline.com\" to generate typosquatting and homoglyph variants.  \n7. Filter dnstwist results to keep only variants with Levenshtein distance ≤ 2. For each variant:\n   a. Run host_lookup to resolve the variant to IP(s).  \n   b. If the variant resolves, run nmap_scan on its IP(s).  \n   c. Run whois_lookup on the variant domain.  \n   d. Compare the variant’s registrant organization/email to the original domain’s registrant from step 5:\n      – If both match exactly, mark the variant as “likely related”.  \n      – If they differ, mark the variant as “likely unrelated.”  \n8. Produce a structured report in JSON with these sections:\n   • dns_records: output from step 1  \n   • subdomain_list: names from step 2  \n   • host_resolutions: mapping of each subdomain/variant to IP(s)  \n   • nmap_results: list of hosts with open ports and flagged roles  \n   • original_registrant: whois data from step 5  \n   • typosquat_variants: list of variants with resolution status, whois match status, and risk classification  \n   • risk_summary: classify each host/variant as High (resolves + whois match + port 80/443/25 open), Medium (resolves + only one indicator), or Low (no resolve or no indicators).",
      "category": "single_server",
      "ground_truth_tool": "OSINT Intelligence",
      "servers": [
        "OSINT Intelligence"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Context7",
        "Game Trends",
        "Google Maps",
        "Metropolitan Museum",
        "Movie Recommender",
        "OKX Exchange",
        "Scientific Computing",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: OSINT Intelligence"
    },
    {
      "query_id": "osint_intelligence_001",
      "query": "Hey, I’ve got this sketchy domain, fakeshoponline.com, that only popped up roughly three months ago and now keeps showing up in phishing reports. I’m trying to piece together who’s really behind it—what their name servers and mail servers look like, any subdomains they’ve spun up recently, and where all those endpoints actually live. On top of that, I’m worried about look-alike tricks—domains with just a letter or two changed—that might resolve to the same IP space and even run a web server or open mail relay. Can you help me trace all of that back to the registrant’s info so I can see which ones share the same owner and which are red herrings? I really need everything backed by concrete DNS records, IP mappings, port/service checks, and ownership details—so I can show my team hard evidence, not just theories.",
      "category": "single_server",
      "ground_truth_tool": "OSINT Intelligence",
      "servers": [
        "OSINT Intelligence"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Context7",
        "Game Trends",
        "Google Maps",
        "Metropolitan Museum",
        "Movie Recommender",
        "OKX Exchange",
        "Scientific Computing",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: OSINT Intelligence"
    },
    {
      "query_id": "reddit_000",
      "query": "Your team needs to compare community engagement and discussion depth on AI research topics in r/MachineLearning and r/artificial over the past week. Execute the following steps using the provided Reddit tools without any external calls:\n\n1. In parallel, call Reddit:fetch_reddit_hot_threads for subreddit=\"MachineLearning\" and subreddit=\"artificial\", each with limit=10.  \n2. Parse each tool’s output to extract the post_id and initial comment count for every thread returned.  \n3. For each post_id, call Reddit:fetch_reddit_post_content with comment_limit=20 and comment_depth=2. Record the actual number of comments retrieved per post.  \n4. Identify threads where comment count > 50. For each of these, call Reddit:fetch_reddit_post_content again with comment_limit=50 and comment_depth=3 to capture deeper engagement.  \n5. Across all threads from both subreddits, detect those whose title or content includes any of the keywords: “GPT”, “Transformer”, “LLaMA”. For each matching post_id, call Reddit:fetch_reddit_post_content with comment_limit=50 and comment_depth=4.  \n6. Find any exact title matches between the two subreddits’ thread lists. For each matching pair of post_ids, call Reddit:fetch_reddit_post_content with comment_limit=5 and comment_depth=1 to directly compare top-level reactions.  \n7. Produce a JSON report with three sections:\n   a) \"subreddit_analysis\": For each subreddit, list all fetched threads sorted by the increase in comment count from step 3 to step 4, include average comment_depth, and highlight the top 3 keyword-related threads with their final comment counts.\n   b) \"cross_subreddit_pairs\": For each exact-title match, show both post_ids, their top 5 comments side by side, and a brief note on differences in tone or key concerns.\n   c) \"action_items\": Five concrete recommendations on which AI topics to monitor further, based on comment growth, keyword prevalence, and cross-community divergences.\n\nDeliver the report as a single JSON object with those three fields.",
      "category": "single_server",
      "ground_truth_tool": "Reddit",
      "servers": [
        "Reddit"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Context7",
        "DEX Paprika",
        "Google Maps",
        "NASA Data",
        "OpenAPI Explorer",
        "Paper Search",
        "Scientific Computing",
        "Unit Converter",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Reddit"
    },
    {
      "query_id": "reddit_000",
      "query": "Hey, I’m putting together a quick rundown of how active conversations have been in r/MachineLearning versus r/artificial over the past week. I’d love to know which of the hottest posts in each community really took off—how many comments they started with and how much they grew when you dig into the deeper threads. If any threads jumped past around fifty comments, could you take a closer look at how the discussion branches out there?\n\nI’m also really curious about anything mentioning GPT, Transformer, or LLaMA—how those keyword-driven talks compare in volume and depth to everything else. And then, for an extra comparison, if any exact same titles showed up in both subreddits, can you pull the first handful of comments from each and highlight any difference in tone or main concerns?\n\nAt the end, I need a sense of which discussions saw the biggest surge in engagement, the top three most-talked-about GPT/Transformer/LLaMA threads, and five solid recommendations on which AI topics are worth keeping an eye on next. I really need real comment counts and clear evidence behind it—no wild guesses. Thanks!",
      "category": "single_server",
      "ground_truth_tool": "Reddit",
      "servers": [
        "Reddit"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Context7",
        "DEX Paprika",
        "Google Maps",
        "NASA Data",
        "OpenAPI Explorer",
        "Paper Search",
        "Scientific Computing",
        "Unit Converter",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Reddit"
    },
    {
      "query_id": "reddit_001",
      "query": "You are a community engagement analyst for the subreddit r/MachineLearning. Your objectives:\n\n1. Use the Reddit:fetch_reddit_hot_threads tool to retrieve the top 5 hot threads from r/MachineLearning (limit=5).\n2. Parse the returned list to identify:\n   a. The thread with the highest comment_count (call this Thread A).\n   b. The thread with the highest score (upvotes) among the remaining four (call this Thread B).\n3. Sequential workflow for Thread A:\n   a. Use Reddit:fetch_reddit_post_content with post_id of Thread A, comment_limit=15, comment_depth=3.\n   b. From the fetched comments, count how many of the top 15 comments have at least one reply. If that count exceeds 10, re-fetch Thread A with comment_limit=15 and comment_depth=5 to capture deeper discussion.\n4. Parallel workflow for Thread B:\n   a. In parallel with the above, use Reddit:fetch_reddit_post_content for Thread B with comment_limit=10, comment_depth=2.\n5. After all fetch calls complete, produce a JSON report containing an array named “threads” with two objects (for Thread A and Thread B). Each object must include:\n   - id: the Reddit post ID\n   - title: the thread title\n   - score: the thread’s score from step 1\n   - comment_count: the thread’s comment_count from step 1\n   - fetched_depth: the final comment_depth used\n   - top_comment_snippet: the text of the single most upvoted top-level comment fetched\n   - deeper_refetch_performed: true/false (true only if Thread A was re-fetched at depth 5)\n\nEnsure you do not request any extra information beyond what the two tools provide. The task is executable immediately without further clarification.",
      "category": "single_server",
      "ground_truth_tool": "Reddit",
      "servers": [
        "Reddit"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Context7",
        "Huge Icons",
        "Hugging Face",
        "Math MCP",
        "Metropolitan Museum",
        "NixOS",
        "OpenAPI Explorer",
        "Paper Search",
        "Weather Data"
      ],
      "combination_name": "Single Server: Reddit"
    },
    {
      "query_id": "reddit_001",
      "query": "Hey, I’m putting together a quick highlight for our ML community newsletter and I want to focus on two posts: the one that’s getting the most chatter right now and the next biggest by upvotes. Could you:\n\n• Grab the current top 5 hot threads from r/MachineLearning  \n• Figure out which one has the highest comment count and call that our “main” thread  \n• Skim its first 15 top-level comments (down to three replies deep) and check how many of those 15 actually sparked at least one reply—if more than 10 did, dig two more levels deep instead  \n• At the same time, pull the runner-up by score from the remaining four, read its first 10 comments up to two levels deep  \n• Finally, give me a JSON array of two objects (main and runner-up) where each object has:  \n  – id (post ID)  \n  – title  \n  – score  \n  – comment_count  \n  – fetched_depth (the depth you ended up using)  \n  – top_comment_snippet (the text of its single most upvoted top-level comment)  \n  – deeper_refetch_performed (true only if you had to go deeper on the main thread)\n\nI really need the real numbers and snippets so I can drop this straight into our newsletter—no guesses, just hard data. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Reddit",
      "servers": [
        "Reddit"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Context7",
        "Huge Icons",
        "Hugging Face",
        "Math MCP",
        "Metropolitan Museum",
        "NixOS",
        "OpenAPI Explorer",
        "Paper Search",
        "Weather Data"
      ],
      "combination_name": "Single Server: Reddit"
    },
    {
      "query_id": "national_parks_000",
      "query": "You are planning a 7-day multi-park camping and hiking adventure that visits national parks in California and Oregon. First, identify up to 10 parks in CA and OR offering both hiking and camping. Then, for each park found, gather detailed park information, current alerts, visitor center operating hours, campground amenities, and upcoming events over the next 7 days. Exclude any park that has active closure or hazard alerts, whose visitor centers are not open at least 9 AM–5 PM every day, or whose campgrounds do not list showers. Finally, from the remaining parks, select those with at least one event starting after 6 PM and build a day-by-day itinerary showing for each park: park name, summary from details, list of open visitor centers and hours, campsite names with showers, and scheduled evening events. Present your result as a JSON itinerary array with one entry per day and park.",
      "category": "single_server",
      "ground_truth_tool": "National Parks",
      "servers": [
        "National Parks"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "DEX Paprika",
        "FruityVice",
        "Math MCP",
        "Medical Calculator",
        "Movie Recommender",
        "OKX Exchange",
        "OSINT Intelligence",
        "Weather Data"
      ],
      "combination_name": "Single Server: National Parks"
    },
    {
      "query_id": "national_parks_000",
      "query": "I’ve been plotting a week-long road trip through California and Oregon, bouncing between parks where I can both hike and camp. I’d love to avoid anywhere that’s under closure alerts or has serious hazards, and I really need campgrounds that actually have showers—plus I’d like the visitor centers to be open every day from about 9 AM to 5 PM so I’m not showing up at a ghost town. On top of that, I’d be thrilled if there’s something cool going on each evening after 6 PM—like ranger talks, stargazing programs, live music, whatever. \n\nCould you help me figure out which parks fit all those criteria over the next seven days and then sketch out a day-by-day plan? I’m imagining something that tells me each day: where I’m headed, a quick park overview, which visitor centers are open with their hours, which campsites have showers, and any evening events I shouldn’t miss. \n\nI really need the details—current alerts, official hours, amenity lists, event schedules—so I can actually book and not just rely on hearsay. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "National Parks",
      "servers": [
        "National Parks"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "DEX Paprika",
        "FruityVice",
        "Math MCP",
        "Medical Calculator",
        "Movie Recommender",
        "OKX Exchange",
        "OSINT Intelligence",
        "Weather Data"
      ],
      "combination_name": "Single Server: National Parks"
    },
    {
      "query_id": "national_parks_001",
      "query": "You are planning a series of backpacking trips over the next 7 days and need to recommend the top 3 California national parks that offer both hiking and camping, have minimal safety closures, visitor services open daily, adequate campground amenities, and at least one public event scheduled. Perform the following steps:\n\n1. Use National Parks:findParks with stateCode=\"CA\", activities=\"hiking,camping\", limit=10 to retrieve candidate parks.\n2. For each returned park, call National Parks:getParkDetails to obtain the annual visitor count and parkCode.\n3. Select the 3 parks with the highest annual visitor counts.\n4. For each of these 3 parks:\n   a. Call National Parks:getAlerts with parkCode and limit=10 to fetch current alerts. Exclude any park with 3 or more active alerts (closures or hazards).\n   b. Call National Parks:getVisitorCenters with parkCode and limit=10 to retrieve operating hours. Verify that at least one visitor center is open every day over the next 7 days. Exclude any park that fails this requirement.\n   c. Call National Parks:getCampgrounds with parkCode and limit=10 to get campground details. Filter for campgrounds offering both potable water and toilets. Record how many such campgrounds are available; if fewer than 2, mark the park as having limited campground capacity.\n   d. Call National Parks:getEvents with parkCode, dateStart=\"today\", dateEnd=\"7 days from today\", limit=5 to list upcoming events. Record how many events are scheduled.\n5. For each park that passes the alert and visitor-center checks, produce a recommendation entry including:\n   • parkCode and full park name\n   • number of active alerts\n   • count of days covered by visitor-center hours\n   • number of campgrounds with water and toilets (and note if limited)\n   • number of upcoming events\n   • overall recommendation: “Highly Recommended” if at least 2 campgrounds and ≥1 event, otherwise “Recommended with Caveats.”\n\nOutput a JSON array of recommendation entries for all qualifying parks.",
      "category": "single_server",
      "ground_truth_tool": "National Parks",
      "servers": [
        "National Parks"
      ],
      "distraction_servers": [
        "Call for Papers",
        "DEX Paprika",
        "Google Maps",
        "Huge Icons",
        "Metropolitan Museum",
        "NixOS",
        "OpenAPI Explorer",
        "Paper Search",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "Single Server: National Parks"
    },
    {
      "query_id": "national_parks_001",
      "query": "I’m planning a week of backpacking in California and trying to pick the three best national parks that won’t let me down. Ideally they’d offer solid hiking and camping, have almost no current closures or safety alerts, keep their visitor centers or services open every day for the next seven days, and have at least a couple of campgrounds with real potable water and toilets. It’d be even better if there’s some kind of event happening—like a ranger talk or guided walk—sometime in the upcoming week. Can you help me narrow it down to the top three spots and show me the proof—how many alerts they each have, their daily service coverage, how many campgrounds meet the water-and-toilet requirement, and what events they’ve got lined up? I need actual numbers and details so I can book with confidence.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "National Parks",
      "servers": [
        "National Parks"
      ],
      "distraction_servers": [
        "Call for Papers",
        "DEX Paprika",
        "Google Maps",
        "Huge Icons",
        "Metropolitan Museum",
        "NixOS",
        "OpenAPI Explorer",
        "Paper Search",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "Single Server: National Parks"
    },
    {
      "query_id": "metropolitan_museum_000",
      "query": "Generate a comparative visual catalog of five ‘sword’ objects from two distinct Met Museum departments (“Arms and Armor” and “Medieval Art”) for a research presentation. Steps: 1) List all Met departments to identify the numeric IDs for “Arms and Armor” and “Medieval Art.” 2) For each of these two departments, search for objects with “sword” in the title that have images, retrieving all matching Object IDs. 3) If a department returns fewer than five sword objects with images, perform a fallback search in that same department for “sword” without requiring images to reach five objects. 4) From the resulting IDs in each department, select the first five unique Object IDs. 5) Fetch full details and images for each selected Object ID. 6) Compile a side-by-side catalog listing, for each object: Department Name, Object Title, Object Date, Artist or Culture, and Image URL. Present the final catalog as a JSON array with two entries (one per department), each containing its five object records.",
      "category": "single_server",
      "ground_truth_tool": "Metropolitan Museum",
      "servers": [
        "Metropolitan Museum"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "DEX Paprika",
        "Google Maps",
        "Hugging Face",
        "Medical Calculator",
        "Movie Recommender",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Unit Converter",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Metropolitan Museum"
    },
    {
      "query_id": "metropolitan_museum_000",
      "query": "I’m putting together a research talk on medieval swords and I want to pick out five examples from two different corners of the Met—the Arms and Armor collection and the Medieval Art galleries. Ideally each sword would have a nice photo for my slides, but if one section only has a few with images, it’s okay to include some without so I still end up with five. For each piece, could you pull together its name, the date or era it comes from, the artist or cultural origin, and a link to its image (if there is one)? I really need concrete details and real links so I can plug them straight into my presentation without any guesses.",
      "category": "single_server",
      "ground_truth_tool": "Metropolitan Museum",
      "servers": [
        "Metropolitan Museum"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "DEX Paprika",
        "Google Maps",
        "Hugging Face",
        "Medical Calculator",
        "Movie Recommender",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Unit Converter",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Metropolitan Museum"
    },
    {
      "query_id": "metropolitan_museum_001",
      "query": "Compile a catalog of the five earliest-dated landscape-themed paintings in the “European Paintings” department of the Metropolitan Museum of Art. First, list all departments to identify the numeric ID for “European Paintings.” Next, perform a search for objects with the query “landscape” scoped to that department. If the initial search returns more than 100 results, repeat the search limiting results to objects with images only. From the final list of object IDs, select the five objects with the earliest documented object dates. For each selected object, retrieve full details—including title, artist, object date, medium, and primary image URL—and present them in a summary table with columns: Title, Artist, Date, Medium, Image URL.",
      "category": "single_server",
      "ground_truth_tool": "Metropolitan Museum",
      "servers": [
        "Metropolitan Museum"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "FruityVice",
        "Hugging Face",
        "Medical Calculator",
        "Movie Recommender",
        "NASA Data",
        "OKX Exchange",
        "OSINT Intelligence",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Metropolitan Museum"
    },
    {
      "query_id": "metropolitan_museum_001",
      "query": "I’ve gotten myself into a bit of an art-history deep dive: my prof wants a quick reference on the absolute earliest landscape paintings in the Met’s European collection—like, the ones that kicked off the whole genre over there. But I’m kind of lost on where to even start in their database. I think there’s a “European Paintings” section, and I only really want works tagged as “landscape,” ideally with actual images so I can drop them into my slides. Could you help me figure out which five pieces have the oldest documented dates, and then pull together each painting’s title, artist, date, medium, and a link to its main image? I really need solid, real-data details and URLs—nothing hand-wavy—so I can back up my little presentation. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Metropolitan Museum",
      "servers": [
        "Metropolitan Museum"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "FruityVice",
        "Hugging Face",
        "Medical Calculator",
        "Movie Recommender",
        "NASA Data",
        "OKX Exchange",
        "OSINT Intelligence",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Metropolitan Museum"
    },
    {
      "query_id": "movie_recommender_000",
      "query": "You are curating a two-day thematic movie marathon around three distinct themes: “space exploration”, “post-apocalyptic”, and “steampunk”.\n\n1. For each theme, call Movie Recommender:get_movies with keyword exactly “space exploration”, “post-apocalyptic”, and “steampunk” (in parallel) to retrieve 10 suggestions each.\n2. Parse the three returned lists and identify any movie title that appears in at least two of the lists.\n   • If you find one or more overlapping titles, designate those as your core_movies.\n   • If there are no overlaps, for each theme take the first two movies (by list order) as core_movies (total of six).\n3. For each core_movie title, call Movie Recommender:get_movies again with keyword exactly “movies like <core_movie>” to retrieve 5 similar suggestions per core movie.\n4. Aggregate all the newly returned lists (parallel expansion calls), deduplicate titles, and compute a frequency count of how many expansion lists each title appeared in.\n5. Produce the final output as a JSON object with these fields:\n   • \"thematic_lists\": an object with keys \"space_exploration\", \"post_apocalyptic\", \"steampunk\", each mapped to its list of 10 titles from step 1.\n   • \"core_movies\": the list of titles chosen in step 2.\n   • \"expanded_recommendations\": an object mapping each core_movie to its 5 similar titles from step 3.\n   • \"final_recommendations\": the list of unique titles from step 4, sorted descending by frequency count (titles appearing in more expansion lists come first; ties broken alphabetically).",
      "category": "single_server",
      "ground_truth_tool": "Movie Recommender",
      "servers": [
        "Movie Recommender"
      ],
      "distraction_servers": [
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Huge Icons",
        "Hugging Face",
        "Medical Calculator",
        "NASA Data",
        "NixOS",
        "OSINT Intelligence",
        "Paper Search"
      ],
      "combination_name": "Single Server: Movie Recommender"
    },
    {
      "query_id": "movie_recommender_000",
      "query": "Hey, I’m putting together a two-day movie marathon for my film club and I want three very different vibes: space exploration, post-apocalyptic survival, and that quirky steampunk flair. I’m thinking about ten go-to films for each vibe, but I also want to see if any titles show up in more than one category—that way those overlapping movies become the marquee picks. If nothing overlaps, I’ll just pick the top couple from each list. Then, for each of those headliners, I’d love around five more “movies like” them to really flesh out the lineup. Finally, I need a big master list of all those extra suggestions, sorted so the films that pop up most often float to the top. Can you pull together the original vibe lists, highlight the core picks, share all the expansion titles, and wrap up with that final ranked recommendation list? I really need actual movie names and how frequently they appear—no vague gut feelings—because I have to show this to the group.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Movie Recommender",
      "servers": [
        "Movie Recommender"
      ],
      "distraction_servers": [
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Huge Icons",
        "Hugging Face",
        "Medical Calculator",
        "NASA Data",
        "NixOS",
        "OSINT Intelligence",
        "Paper Search"
      ],
      "combination_name": "Single Server: Movie Recommender"
    },
    {
      "query_id": "movie_recommender_001",
      "query": "You are the curator for an upcoming week-long sci-fi film showcase. Your goal is to assemble a final slate of 5 distinct movies that best match both core ‘science fiction’ themes and narrower ‘space exploration’ themes, with a fallback to ‘upcoming week’ releases if necessary. Execute the following steps without asking for additional information:\n\n1. In parallel, call get_movies twice:\n   • get_movies(keyword: \"science fiction\") → SciFiList (a list of 10 sci-fi movie titles)\n   • get_movies(keyword: \"space exploration\") → SpaceList (a list of 10 space-exploration titles)\n2. Compute CommonList = intersection of SciFiList and SpaceList.\n3. Decision point:\n   • If CommonList contains 5 or more titles, set FinalList to the first 5 titles in CommonList.\n   • If CommonList contains fewer than 5 titles, call get_movies(keyword: \"science fiction upcoming week\") → UpcomingSciFi (a list of 10 upcoming sci-fi releases). Remove any titles already in SciFiList or SpaceList, then set FinalList = all titles in CommonList plus the first (5 – |CommonList|) titles from the filtered UpcomingSciFi list.\n4. Return a JSON object with keys:\n   {\n     \"final_movies\": [array of 5 selected titles],\n     \"source_breakdown\": {\n       \"common_list\": [titles from CommonList included],\n       \"upcoming_recommendations\": [titles added from UpcomingSciFi]\n     },\n     \"selection_rationale\": \"Brief explanation of how many came from overlap vs. upcoming releases.\"\n   }",
      "category": "single_server",
      "ground_truth_tool": "Movie Recommender",
      "servers": [
        "Movie Recommender"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Car Price Evaluator",
        "DEX Paprika",
        "FruityVice",
        "NixOS",
        "OSINT Intelligence",
        "Reddit",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Movie Recommender"
    },
    {
      "query_id": "movie_recommender_001",
      "query": "Hey, I’m putting together a week-long sci-fi film showcase at my local theater next week and need to nail down a slate of five movies. Ideally, they’d all be solid science-fiction picks that really lean into space exploration—starship voyages, alien worlds, that kind of epic adventure. I’m not sure there are five titles that hit both “pure sci-fi” and “deep space” perfectly, so if we can’t find enough classics crossing both, I’d top up the list with the best new sci-fi releases opening in the next seven days. Could you help me choose those five, highlight which ones come from that overlap of space-heavy sci-fi and which are the fresh upcoming flicks, and give me a quick note on why each made the cut? I really need actual titles with solid reasons so I can pitch it to our crowd.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Movie Recommender",
      "servers": [
        "Movie Recommender"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Car Price Evaluator",
        "DEX Paprika",
        "FruityVice",
        "NixOS",
        "OSINT Intelligence",
        "Reddit",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Movie Recommender"
    },
    {
      "query_id": "nasa_data_000",
      "query": "Perform a comprehensive Solar System situational awareness report for decision-makers, integrating near-Earth object hazards, current space weather, NASA imagery, exoplanet research, and active Mars rover operations. Specifically:\n1. Asteroid Hazard Assessment: Identify all asteroids making a close approach to Earth over the next 7 days. For each asteroid, retrieve detailed data and flag any object designated as potentially hazardous.\n2. Space Weather Summary: Gather space weather events from the past 7 days, including solar flares, coronal mass ejections (CMEs), solar energetic particles (SEPs), geomagnetic storms (GSTs), magnetopause crossings (MPCs), radiation belt enhancements (RBEs), and high-speed streams (HSSs). Summarize each event with date, type, and intensity. If any solar flare of class M1.0 or higher occurred, retrieve the WSA+Enlil simulation for the upcoming week.\n3. Cross-Validation with DONKI Notifications: Fetch all DONKI notifications for the past 7 days to ensure no critical events were missed.\n4. Earth Imagery for New York City: For latitude 40.7128 and longitude -74.0060, first retrieve available Landsat 8 imagery assets for the most recent date, then fetch the corresponding image.\n5. EPIC Imagery: Obtain the list of available dates for the EPIC natural collection, select the latest date, and retrieve all EPIC images for that date.\n6. Astronomy Picture of the Day (APOD): Retrieve today’s APOD title, media type, and URL.\n7. Exoplanet Research: Query confirmed exoplanets with orbital periods greater than 300 days and radii less than 2 Earth radii; return the top 5 results in JSON format with their names, orbital periods, and radii.\n8. Mars Rover Curiosity Operations: Retrieve the mission manifest, identify the most recent martian sol and its corresponding Earth date, and fetch Mast camera photos for that sol.\n\nOutput a single structured JSON object with sections: AsteroidHazards, SpaceWeatherSummary, WSAEnlilSimulation (if retrieved), DONKINotifications, EarthImagery, EPICImagery, APOD, ExoplanetData, and MarsRoverPhotos.",
      "category": "single_server",
      "ground_truth_tool": "NASA Data",
      "servers": [
        "NASA Data"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "Game Trends",
        "Math MCP",
        "Medical Calculator",
        "National Parks",
        "NixOS",
        "OpenAPI Explorer",
        "Paper Search",
        "Reddit"
      ],
      "combination_name": "Single Server: NASA Data"
    },
    {
      "query_id": "nasa_data_000",
      "query": "I’m putting together a high-level solar system briefing for some senior folks, and juggling all the pieces is giving me a headache. I need to know if any near-Earth asteroids are swinging by in the next week—especially the ones that might be flagged as potentially hazardous. At the same time, I’d love a snapshot of the Sun’s recent activity: flares, coronal mass ejections, particle storms, geomagnetic disturbances—everything from the past seven days. And if there’s been at least an M-class flare, could you grab that week-ahead solar wind forecast we usually lean on? I also want to double-check that no critical alerts slipped through, so please cross-check any space weather notifications from the past week.\n\nOn the imagery side, I could really use a fresh satellite shot of New York City (around 40.7128, –74.0060) plus the latest batch of EPIC Earth photos from deep space. Oh, and don’t forget today’s astronomy picture of the day—title, media type, and URL.\n\nFor the exoplanet section, show me the top 5 confirmed worlds that take more than about 300 days to orbit but are under twice Earth’s size. A small JSON snippet for that would be perfect so I can paste it straight into our system.\n\nFinally, I need the latest from Curiosity on Mars: what’s the most recent Martian sol and its Earth date, plus any new Mastcam shots from that sol?\n\nCould you bundle all of this into one neat report I can drop into our dashboard? I really need actual numbers and solid sources—no hand-waving, please.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "NASA Data",
      "servers": [
        "NASA Data"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "Game Trends",
        "Math MCP",
        "Medical Calculator",
        "National Parks",
        "NixOS",
        "OpenAPI Explorer",
        "Paper Search",
        "Reddit"
      ],
      "combination_name": "Single Server: NASA Data"
    },
    {
      "query_id": "nasa_data_001",
      "query": "Perform a comprehensive NASA Data integration and analysis report that includes the following subtasks:\n\n1. Near-Earth Asteroid Risk Assessment (Sequential & Dependent)\n   a. Fetch all asteroids with Earth close-approach dates over the next 7 days.\n   b. From that feed, identify the three asteroids with the largest estimated diameter.\n   c. For each of those three, look up their detailed NASA JPL data (including absolute magnitude, velocity, miss distance).\n\n2. Space Weather Monitoring (Parallel Monitoring)\n   a. For the same next-7-day window, retrieve DONKI notifications of all types.\n   b. In parallel, fetch coronal mass ejection, geomagnetic storm, solar flare, solar energetic particle, magnetopause crossing, radiation belt enhancement, and high speed stream data for that 7-day window.\n   c. Run a WSA+Enlil simulation for the upcoming week to model solar wind conditions.\n\n3. Earth Observation for Urban Expansion in San Francisco (Dependency Chain)\n   a. Get the list of available EPIC image dates, then select the latest available date.\n   b. Retrieve EPIC natural-collection imagery for that date.\n   c. For latitude 37.7749 and longitude -122.4194 on that same date, list all available Landsat 8 imagery assets.\n   d. Fetch the most recent Landsat 8 image for that location with a 0.1°×0.1° footprint and cloud_score enabled.\n\n4. Mars Rover Photo Retrieval (Iterative Refinement)\n   a. Get the mission manifest for rover “curiosity,” determine its maximum sol.\n   b. Retrieve Curiosity’s MAST camera photos for one sol before its maximum sol (page 1).\n   c. Retrieve Curiosity’s NAVCAM photos for the maximum sol (page 1).\n\n5. Exoplanet Candidate Identification (Filter & Sort)\n   a. Query the Exoplanet Archive for confirmed exoplanets with orbital period > 1000 days and planet radius < 2 Earth radii.\n   b. From the returned list, identify the exoplanet with the longest orbital period.\n\n6. Daily Astronomy Picture (Final Context)\n   a. Fetch today’s Astronomy Picture of the Day with video thumbnail if applicable.\n\nDeliverables:\n• A consolidated report containing:\n  - The three largest near-Earth asteroids and their JPL details.\n  - A summary of DONKI notifications and all space weather indices for the upcoming week, plus the Enlil simulation overview.\n  - Visual links and metadata for the selected EPIC and Landsat-8 images over San Francisco.\n  - Metadata and sample URLs for the two sets of Curiosity rover photos.\n  - The name and key properties of the exoplanet with the longest orbital period matching the filter.\n  - Title, description, and URL (and thumbnail if video) of today’s APOD.",
      "category": "single_server",
      "ground_truth_tool": "NASA Data",
      "servers": [
        "NASA Data"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Context7",
        "Game Trends",
        "Google Maps",
        "Huge Icons",
        "Metropolitan Museum",
        "Movie Recommender",
        "OKX Exchange",
        "Scientific Computing"
      ],
      "combination_name": "Single Server: NASA Data"
    },
    {
      "query_id": "nasa_data_001",
      "query": "Hey, I’ve got a bit of a space‐heavy request that’s been bugging me—my boss wants a one‐stop update covering a bunch of NASA goodies for the coming week, and I’m totally drowning in where to start. \n\nFirst off, can you see if any asteroids swing by Earth over the next seven days and then flag the three biggest ones? I’m talking diameter, so once you’ve got those, I’d love their JPL stats—like how bright they seem, how fast they’re moving, and just how close they actually get. \n\nWhile you’re at it, I also need a breakdown of everything going on with space weather during that same period. You know, all the flare alerts, geomagnetic storms, CMEs, radiation belt changes—any of those daily notifications—and a quick sense of how the solar wind might behave over the next week (if there’s a way to simulate it roughly, that’d be fantastic). \n\nOn top of that, I’m digging into urban growth around San Francisco. Could you grab the very latest satellite picture of the Bay Area and then zoom right in on 37.7749, –122.4194 with about a 0.1°×0.1° patch, checking the cloud cover too? \n\nAlso, Curiosity’s been snapping away—would you pull its mastcam shots from one sol before its most recent day and some navcam pics from that final sol? \n\nAnd just for fun (and science), I want to see which confirmed exoplanets out there take more than roughly 1,000 days to orbit but are under twice Earth’s radius—and from that group, which one has the looooongest year. \n\nOh, and before I forget: today’s Astronomy Picture of the Day (with a thumbnail if it’s a video) needs to be in there as well. \n\nI really can’t bring a bunch of vague opinions to my boss—everything should come with real numbers, dates, image links or data sources so I can back it all up. Thanks a ton!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "NASA Data",
      "servers": [
        "NASA Data"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Context7",
        "Game Trends",
        "Google Maps",
        "Huge Icons",
        "Metropolitan Museum",
        "Movie Recommender",
        "OKX Exchange",
        "Scientific Computing"
      ],
      "combination_name": "Single Server: NASA Data"
    },
    {
      "query_id": "okx_exchange_000",
      "query": "You are building a crypto breakout detection report for three OKX instruments: BTC-USDT, ETH-USDT, and ADA-USDT. Perform the following steps in sequence and output a JSON summary for each instrument with fields: instrument, current_price, avg_24h_price, deviation_pct, trend_15m, volume_change_5m, breakout_signal.\n\n1. For each instrument (BTC-USDT, ETH-USDT, ADA-USDT):\n   a. Call OKX Exchange:get_price to fetch the latest price as current_price.\n   b. Call OKX Exchange:get_candlesticks with bar=\"1H\" and limit=24 to fetch the past 24 one-hour candlesticks. Compute avg_24h_price (the arithmetic mean of each candlestick’s close).\n   c. Compute deviation_pct = (current_price - avg_24h_price) / avg_24h_price × 100. If |deviation_pct| ≤ 2.0, set breakout_signal = false and skip to the next instrument; otherwise proceed.\n\n2. For each instrument where |deviation_pct| > 2.0:\n   a. Call OKX Exchange:get_candlesticks with bar=\"15m\" and limit=50 to fetch the past 50 fifteen-minute candlesticks. Compute trend_15m as “up” if the simple moving average of the last 5 closes is greater than that of the first 5 closes; otherwise “down.” If trend_15m is “down,” set breakout_signal = false and skip further analysis for this instrument.\n\n   b. For instruments with trend_15m = “up,” call OKX Exchange:get_candlesticks with bar=\"5m\" and limit=60 to fetch the past 60 five-minute candlesticks. Compute average_volume_5m over all 60 volumes, and let last_volume be the volume of the most recent candlestick. Compute volume_change_5m = (last_volume - average_volume_5m) / average_volume_5m × 100.\n\n   c. If volume_change_5m ≥ 10.0, set breakout_signal = true; otherwise set breakout_signal = false.\n\n3. Output a JSON array named \"report\" with one object per instrument containing: instrument, current_price, avg_24h_price, deviation_pct, trend_15m (or null if skipped), volume_change_5m (or null), breakout_signal.\n\nEnsure all calculations use the fetched tool outputs directly, and follow the exact procedure without requesting additional inputs.",
      "category": "single_server",
      "ground_truth_tool": "OKX Exchange",
      "servers": [
        "OKX Exchange"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "Context7",
        "Hugging Face",
        "Medical Calculator",
        "Metropolitan Museum",
        "National Parks",
        "OSINT Intelligence",
        "Scientific Computing",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: OKX Exchange"
    },
    {
      "query_id": "okx_exchange_000",
      "query": "So, here’s the deal: I’m putting together a quick “breakout radar” for BTC-USDT, ETH-USDT, and ADA-USDT, and I really need hard numbers to back any call. What I’m wondering is:\n\n– What’s the current price vs. its average over roughly the past day?  \n– How far off is that in percentage terms?  \n– In the last 15 minutes, does it look like the coin’s on an upswing or heading down?  \n– And over the last 5 minutes, has volume shot up or tanked compared to its recent average?  \n– Finally—based on all that—are any of these really cracking out into a breakout right now?\n\nCould you pull the live data, run those calculations, and give me a concise summary (JSON, table, whatever) for each pair? I can’t walk into my team meeting with gut feels—I need real, data-driven answers. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "OKX Exchange",
      "servers": [
        "OKX Exchange"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "Context7",
        "Hugging Face",
        "Medical Calculator",
        "Metropolitan Museum",
        "National Parks",
        "OSINT Intelligence",
        "Scientific Computing",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: OKX Exchange"
    },
    {
      "query_id": "okx_exchange_001",
      "query": "You are an AI trading-analysis agent using the OKX Exchange API. Perform the following workflow in one run:\n\n1. In parallel, fetch 1-minute candlestick data for the past 30 minutes for both BTC-USDT and ETH-USDT:\n   • Call get_candlesticks with instrument='BTC-USDT', bar='1m', limit=30\n   • Call get_candlesticks with instrument='ETH-USDT', bar='1m', limit=30\n\n2. For each instrument, compute 1-minute momentum percentage:\n   momentum1m_pct = (last_close – first_close) / first_close × 100\n\n3. If momentum1m_pct > 1.0% for an instrument, fetch its current market price:\n   • Call get_price with instrument set to that symbol\n\n4. For each instrument where you fetched a price, determine whether the current price continues the momentum direction:\n   direction_continues = (current_price – last_close) has the same sign as momentum1m_pct\n\n5. Identify which instrument has the higher absolute value of momentum1m_pct. On that top instrument, perform 5-minute candlestick analysis for the past hour:\n   • Call get_candlesticks with instrument set to top symbol, bar='5m', limit=12\n   • Compute trend5m_volatility = standard deviation of the 12 closing prices\n\n6. If trend5m_volatility > 0.5%, trigger a deeper short-term review:\n   • Call get_candlesticks with the same top instrument, bar='1m', limit=60\n   • Label this step “deep_analysis_executed”\n\n7. Produce a JSON report with an array field named “instrument_data” containing one object per symbol with these keys:\n   • instrument: 'BTC-USDT' or 'ETH-USDT'\n   • momentum1m_pct: number\n   • current_price: number (if fetched; otherwise null)\n   • direction_continues: boolean (if price fetched; otherwise null)\n   • trend5m_volatility: number (for top instrument; null for the other)\n   • deep_analysis_executed: boolean\n\nEnsure you call get_price only when momentum1m_pct > 1.0% and get the deep-dive 1m candles only when volatility > 0.5%.",
      "category": "single_server",
      "ground_truth_tool": "OKX Exchange",
      "servers": [
        "OKX Exchange"
      ],
      "distraction_servers": [
        "Call for Papers",
        "DEX Paprika",
        "FruityVice",
        "Google Maps",
        "Medical Calculator",
        "NASA Data",
        "Paper Search",
        "Unit Converter",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: OKX Exchange"
    },
    {
      "query_id": "okx_exchange_001",
      "query": "I’ve been tinkering with a quick crypto check for BTC and ETH – basically looking at the last half-hour of one-minute candles to see who’s been really moving. If either coin has jumped more than about 1% over those 30 minutes, I want to know its latest price and whether it’s still pushing in the same direction. Then, whichever one shows the bigger burst, could you peek at roughly the past hour of five-minute bars and give me a sense of how choppy its closes have been (like the % volatility)? And if that volatility turns out to be north of about 0.5%, I’d love a deeper look into the last 60 one-minute bars to see exactly what’s going on. In the end, I need a clear breakdown for each coin: the one-minute momentum %, the current price (if it qualified), a yes/no on whether it’s still trending, the five-minute volatility % for the stronger coin, and a flag saying if you did that extra minute-by-minute deep dive. I really need real numbers here – can’t just wing it in my presentation. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "OKX Exchange",
      "servers": [
        "OKX Exchange"
      ],
      "distraction_servers": [
        "Call for Papers",
        "DEX Paprika",
        "FruityVice",
        "Google Maps",
        "Medical Calculator",
        "NASA Data",
        "Paper Search",
        "Unit Converter",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: OKX Exchange"
    },
    {
      "query_id": "paper_search_002",
      "query": "Compile a comparative review of the latest deep learning applications in genomics and proteomics published in the past 3 months across arXiv, bioRxiv, medRxiv, PubMed, and Google Scholar. Execute the following steps without asking for additional information:\n\n1. In parallel, call each of the five search tools with query='deep learning genomics proteomics' and max_results=10:\n   - Paper Search:search_arxiv\n   - Paper Search:search_biorxiv\n   - Paper Search:search_medrxiv\n   - Paper Search:search_pubmed\n   - Paper Search:search_google_scholar\n2. For any server that returns fewer than 5 papers whose title contains “genomic” or “proteomic,” rerun that server’s search with query='machine learning bioinformatics' and max_results=10.\n3. Merge all returned metadata, deduplicate by title, and select the 8 most recent papers (using the metadata’s publication date).\n4. For each selected paper, execute the appropriate download→read chain based on its source:\n   • arXiv: call download_arxiv(paper_id) then read_arxiv_paper(paper_id)\n   • bioRxiv: call download_biorxiv(paper_id) then read_biorxiv_paper(paper_id)\n   • medRxiv: call download_medrxiv(paper_id) then read_medrxiv_paper(paper_id)\n   • PubMed: call download_pubmed(paper_id) (expect unsupported download); set full_text = metadata['abstract']\n   • Google Scholar: set full_text = metadata.get('abstract', 'Abstract unavailable')\n5. From each paper’s full_text or abstract, extract:\n   - algorithm_type (e.g., CNN, RNN, Transformer)\n   - dataset (specify genomic or proteomic dataset name)\n   - primary_performance_metric (e.g., accuracy, AUC)\n   - main_conclusion (one-sentence summary)\n6. Perform cross‐server validation: verify that each algorithm_type appears in at least two papers from different servers; if an algorithm_type appears in only one server’s papers, flag it as ‘singleton algorithm.’\n7. Return a JSON array of eight objects, each with fields: {\"server\",\"title\",\"authors\",\"publication_date\",\"algorithm_type\",\"dataset\",\"primary_performance_metric\",\"main_conclusion\",\"validation_status\"}.  \n\nThe agent should directly invoke the specified tools in sequence, handle conditional branches and fallbacks, extract all required data, and produce the final structured JSON review.",
      "category": "single_server",
      "ground_truth_tool": "Paper Search",
      "servers": [
        "Paper Search"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "DEX Paprika",
        "FruityVice",
        "Medical Calculator",
        "NASA Data",
        "OKX Exchange",
        "OpenAPI Explorer",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "Single Server: Paper Search"
    },
    {
      "query_id": "paper_search_002",
      "query": "I’m wrapping up a project on how deep learning is being used in genomics and proteomics, and my manager has asked for a snapshot of what’s really new in the last three months. I’ve seen buzz about CNNs, RNNs, Transformers and such, but I’m not sure which models are actually gaining traction across different studies, or which datasets they’ve been tested on (like specific genome sequencing collections versus mass-spec proteomics sets). Could you dive into the recent preprints and journal articles, pick out roughly eight of the newest papers, and for each one tell me:\n\n- What type of algorithm they used (CNN, RNN, Transformer, etc.)\n- Which genomic or proteomic dataset they evaluated on\n- Their headline performance number (accuracy, AUC, whatever they highlight)\n- A one-sentence summary of the main takeaway\n\nAlso, if any algorithm only shows up in a single paper (i.e. a one-off), flag it so I know it might be a fringe idea. I really need concrete details and real numbers—no vague impressions—because I’m presenting this to my team and need solid evidence from the actual studies.",
      "category": "single_server",
      "ground_truth_tool": "Paper Search",
      "servers": [
        "Paper Search"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Call for Papers",
        "DEX Paprika",
        "FruityVice",
        "Medical Calculator",
        "NASA Data",
        "OKX Exchange",
        "OpenAPI Explorer",
        "Unit Converter",
        "Weather Data"
      ],
      "combination_name": "Single Server: Paper Search"
    },
    {
      "query_id": "paper_search_004",
      "query": "You are tasked with a comprehensive review of recent progress in “machine learning for protein folding” over the past 3 months. Follow these steps in sequence and leverage all five search servers:\n\n1. SEARCH PHASE  \n   a. Run search_arxiv with query = \"machine learning protein folding\" and max_results = 5.  \n   b. Run search_biorxiv with the same query and max_results = 5.  \n   c. Run search_medrxiv with the same query and max_results = 5.  \n   d. Run search_pubmed with the same query and max_results = 5.  \n   e. Run search_google_scholar with the same query and max_results = 5.\n\n2. QUERY REFINEMENT  \n   If any server returns fewer than 3 papers, re-run that server’s search with query = \"deep learning protein folding\" and max_results = 5.\n\n3. DOWNLOAD & EXTRACTION  \n   For each paper in arXiv, bioRxiv, and medRxiv result sets (top 3 each):  \n     • Invoke the appropriate download tool (download_arxiv / download_biorxiv / download_medrxiv) to save the PDF.  \n     • Invoke the corresponding read tool (read_arxiv_paper / read_biorxiv_paper / read_medrxiv_paper) to extract full text.  \n   For PubMed results: record metadata only (downloading unsupported).  \n   For Google Scholar results: record metadata only.\n\n4. KEYWORD ANALYSIS & CROSS-VALIDATION  \n   a. In each extracted text, count occurrences of “AlphaFold”.  \n   b. For each PubMed and Google Scholar paper, perform search_google_scholar using the exact paper title (max_results = 1) to confirm it appears and retrieve metadata.  \n   c. Build a combined table of all unique papers, listing: paper_id, title, source_servers (which of the five servers returned it), and AlphaFold_mention_count (zero for PubMed/Google entries if no full text).\n\n5. ITERATIVE FALLBACK  \n   If no paper in the combined table has AlphaFold_mention_count ≥ 1, repeat steps 1–4 replacing keyword “AlphaFold” with “RoseTTAFold”.\n\n6. REPORT  \n   Output a JSON report sorted by descending AlphaFold_mention_count (or RoseTTAFold_mention_count if you invoked fallback). For each paper include:  \n   • paper_id  \n   • title  \n   • source_servers (array)  \n   • mention_keyword (\"AlphaFold\" or \"RoseTTAFold\")  \n   • mention_count  \n   • one-sentence summary extracted from the first 200 characters of the paper’s text (or abstract placeholder for PubMed/Google if full text unavailable).",
      "category": "single_server",
      "ground_truth_tool": "Paper Search",
      "servers": [
        "Paper Search"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Car Price Evaluator",
        "Game Trends",
        "Medical Calculator",
        "Movie Recommender",
        "NASA Data",
        "OSINT Intelligence",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "Single Server: Paper Search"
    },
    {
      "query_id": "paper_search_004",
      "query": "Hey, I’m trying to put together a quick overview of what’s been happening with machine learning applied to protein folding over the past three months. My boss wants to know which approach is getting the most buzz – I’m betting AlphaFold still has the lead, but if papers aren’t talking about it, feel free to switch focus to RoseTTAFold. Could you pull together a set of recent studies from all the usual sources, tally how many times each one mentions the target method, note where you found each paper, and give me a one-sentence summary? For anything you can’t grab the full text on, just use the abstract. Then sort everything by the mention count so I can see at a glance who’s really driving the field. I really need actual counts and solid sources—no guesswork—so I can show the team the real numbers.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Paper Search",
      "servers": [
        "Paper Search"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "Car Price Evaluator",
        "Game Trends",
        "Medical Calculator",
        "Movie Recommender",
        "NASA Data",
        "OSINT Intelligence",
        "Reddit",
        "Unit Converter"
      ],
      "combination_name": "Single Server: Paper Search"
    },
    {
      "query_id": "scientific_computing_000",
      "query": "You are given two 3×3 matrices:\n  • M1 = [[4, 2, 1], [2, 3, 0], [1, 0, 2]]\n  • M2 = [[1, 0, 2], [0, 1, 1], [2, 1, 3]]\nand two 3-component vectors:\n  • v1 = [1, 2, 3]\n  • v2 = [3, 2, 1]\nAlso consider the scalar potential φ(x,y,z)=x²·y + y²·z + z²·x and the vector field F(x,y,z)=[x·y, y·z, z·x].\n\nStep-by-step tasks (all intermediate results must be stored under clear names):\n1. Create M1 and M2 in the tensor store.\n2. Compute M_sum = M1 + M2, store as “M_sum”.\n3. Compute M_diff = M1 − M2, store as “M_diff”.\n4. Compute M_prod = M1 × M2 (matrix-multiply), store as “M_prod”.\n5. Scale M_prod in place by 0.5; name this scaled matrix “M_scaled”.\n6. Compute det = determinant(M_scaled).\n   • If |det| > 0.1: compute M_inv = inverse(M_scaled) and store as “M_inv”.\n   • Otherwise: compute the SVD of M_scaled, store U as “U_svd”, S as “S_svd”, and Vᵀ as “Vt_svd”.\n7. Compute the eigenvalues and eigenvectors of the stored inverse (or, if you took the SVD branch, of U_svd·diag(S_svd)·Vt_svd); store the eigenvectors as “eigvecs”.\n8. Perform a QR decomposition of M_scaled; store Q as “Q_qr” and R as “R_qr”.\n9. Find an orthonormal basis for the column space of M_scaled; store it as “basis”.\n10. Change the basis of M_sum into the new orthonormal basis; store result as “M_in_new_basis”.\n11. Compute the rank of M_scaled.\n\nVector operations:\n12. Create v1 and v2 in the tensor store.\n13. Compute the dot product v1·v2.\n14. Compute the cross product v1×v2.\n15. Project v1 onto v2.\n\nSymbolic and field analysis:\n16. Compute the symbolic gradient ∇φ.\n17. Compute the directional derivative of φ along the vector [1,1,1].\n18. Compute the symbolic curl of F and evaluate it numerically at [1,1,1].\n19. Compute the symbolic divergence of F and evaluate it at [0,0,0].\n20. Compute the scalar Laplacian of φ.\n\nVisualization:\n21. Plot the 3D vector field F over the box x,y,z∈[−1,1].\n22. Plot the 2D function f(x,y)=sin(√(x²+y²)) over x,y∈[−5,5].\n\nCleanup:\n23. Delete all stored tensors (M1, M2, M_sum, M_diff, M_prod, M_scaled, M_inv or U_svd/S_svd/Vt_svd, eigvecs, Q_qr, R_qr, basis, M_in_new_basis, v1, v2).",
      "category": "single_server",
      "ground_truth_tool": "Scientific Computing",
      "servers": [
        "Scientific Computing"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Car Price Evaluator",
        "DEX Paprika",
        "FruityVice",
        "Hugging Face",
        "Movie Recommender",
        "NASA Data",
        "OKX Exchange",
        "Unit Converter",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Scientific Computing"
    },
    {
      "query_id": "scientific_computing_000",
      "query": "Hey, I’m wrestling with a pretty hefty bit of linear algebra and vector calculus for my project and could really use a hand. I’ve got two 3×3 matrices—one with rows [4, 2, 1], [2, 3, 0], [1, 0, 2] and the other [1, 0, 2], [0, 1, 1], [2, 1, 3]—and also two vectors [1, 2, 3] and [3, 2, 1]. On top of that there’s a scalar potential φ(x,y,z)=x²·y + y²·z + z²·x and a vector field F(x,y,z)=[x·y, y·z, z·x].  \n\nI need to see what happens when I add and subtract those matrices, multiply them, scale the product by 0.5 and then check its determinant. If the absolute value ends up over 0.1, I want the inverse; if not, we’ll have to dive into an SVD breakdown. After that I’d like to pull out eigenvalues and eigenvectors, get a QR decomposition, find an orthonormal basis for the scaled matrix’s column space, and then re-express the sum of the originals in that new basis—plus figure out the rank.  \n\nMeanwhile, for the vectors [1, 2, 3] and [3, 2, 1], I’d appreciate their dot product, cross product, and the projection of one onto the other. Then there’s the symbolic side: the gradient of φ, its directional derivative along [1, 1, 1], the curl of F at [1, 1, 1], the divergence of F at [0, 0, 0], and the scalar Laplacian of φ.  \n\nIf it’s not too much, could you also sketch a 3D plot of F over the cube x,y,z∈[–1, 1] and a 2D plot of f(x,y)=sin(√(x²+y²)) over x,y∈[–5, 5]? And once all that’s done, let’s wipe out every intermediate tensor or matrix so nothing’s left hanging.  \n\nI really need the exact numbers—my advisor wants concrete results, not just vague descriptions. Appreciate any help you can give!",
      "category": "single_server",
      "ground_truth_tool": "Scientific Computing",
      "servers": [
        "Scientific Computing"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Car Price Evaluator",
        "DEX Paprika",
        "FruityVice",
        "Hugging Face",
        "Movie Recommender",
        "NASA Data",
        "OKX Exchange",
        "Unit Converter",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Scientific Computing"
    },
    {
      "query_id": "scientific_computing_001",
      "query": "You are given a 3×3 covariance matrix C = [[2.0, 0.3, 0.5], [0.3, 1.5, 0.4], [0.5, 0.4, 1.0]] and a data vector v = [1.2, -0.8, 0.5]. Perform the following analysis in sequence, using the provided Scientific Computing tools:\n\n1. Create tensor \"C\" with shape [3,3] and values [2.0,0.3,0.5,0.3,1.5,0.4,0.5,0.4,1.0].\n2. Create tensor \"v\" with shape [3] and values [1.2,-0.8,0.5].\n3. Compute determinant of \"C\". If det==0, regularize by scaling \"C\" in place with factor 0.01 and recompute determinant. Proceed only if det≠0.\n4. Compute inverse of \"C\".\n5. Compute eigenvalues and eigenvectors of \"C\".\n6. Perform singular value decomposition of \"C\".\n7. Perform QR decomposition of \"C\".\n8. Find an orthonormal basis for the column space of \"C\".\n9. Change the basis of \"C\" to that orthonormal basis.\n10. Project vector \"v\" onto the first (principal) eigenvector of \"C\".\n11. Define the scalar function f(x,y,z) = exp(-0.5*(x**2 + y**2 + z**2)). Compute the directional derivative of f at point v along the first eigenvector (normalized).\n12. Compute the symbolic gradient of f(x,y,z); then compute the divergence and the curl of that gradient field.\n13. Plot the gradient vector field over x,y,z ∈ [−2,2] with resolution n=15.\n14. Plot the 2D function exp(-0.5*x**2) over x ∈ [−3,3] with y-range [−0.1,1.1] and grid resolution 200.\n15. Delete tensors \"C\" and \"v\" to clean up.\n\nReturn a structured report (JSON) containing all intermediate numeric results (determinant, inverse matrix, eigenvalues, eigenvectors, singular values, U, V^T, Q, R, orthonormal basis vectors, changed-basis matrix, projection value, directional derivative, symbolic gradient, divergence, curl) and include the two generated plots.",
      "category": "single_server",
      "ground_truth_tool": "Scientific Computing",
      "servers": [
        "Scientific Computing"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "DEX Paprika",
        "Hugging Face",
        "Movie Recommender",
        "NASA Data",
        "NixOS",
        "OpenAPI Explorer",
        "Reddit",
        "Weather Data"
      ],
      "combination_name": "Single Server: Scientific Computing"
    },
    {
      "query_id": "scientific_computing_001",
      "query": "Hey, I’m working on this 3D Gaussian model for my thesis and it’s been driving me nuts. I’ve defined a covariance matrix that looks like\n\n[2.0, 0.3, 0.5  \n 0.3, 1.5, 0.4  \n 0.5, 0.4, 1.0]\n\nand my sample vector is [1.2, –0.8, 0.5]. I need to know if that matrix is actually invertible (what’s its determinant? if it comes out zero, I might shrink it by a factor of 0.01 so I can invert it), then get the inverse so I can plug it into my Mahalanobis stuff. On top of that, I’d love to see its eigenvalues and eigenvectors—and even run an SVD or QR to get a feel for its geometry—grab an orthonormal basis for its column space, and re-express the matrix there. When I project my vector onto the first eigenvector, what number do I get? \n\nAs a side project, I’m also exploring the function f(x,y,z)=exp(–0.5*(x²+y²+z²)). Could you tell me its directional derivative at [1.2, –0.8, 0.5] along that leading eigenvector? It’d be great to have the full symbolic gradient of f, plus the divergence and curl of that gradient field. And because I learn best by seeing things, I need a 3D quiver plot of the gradient over x,y,z from –2 to 2 (about 15 points per axis) and a simple 2D curve of exp(–0.5 x²) from x=–3 to 3 with y going from –0.1 to 1.1 (200 samples). \n\nMy advisor wants everything—determinant, inverse matrix, eigenvalues/vectors, singular values, Q and R from QR, your orthonormal basis, the changed-basis form, the projection value, the directional derivative, the gradient expression, divergence, curl—and the two plots all wrapped up in a JSON report. I really need hard numbers and visuals to back it all up, not just a high-level summary. Can you help me pull all that together?\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Scientific Computing",
      "servers": [
        "Scientific Computing"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Car Price Evaluator",
        "DEX Paprika",
        "Hugging Face",
        "Movie Recommender",
        "NASA Data",
        "NixOS",
        "OpenAPI Explorer",
        "Reddit",
        "Weather Data"
      ],
      "combination_name": "Single Server: Scientific Computing"
    },
    {
      "query_id": "weather_data_000",
      "query": "You are a meteorological analyst tasked with determining the single best Springfield (from any country) and date in the upcoming 7 days to hold a large outdoor event, based on consistency between legacy and detailed temperature readings and favorable forecast conditions.\n\nSteps:\n1. Use search_locations_tool with query=\"Springfield\" to retrieve all matching locations (city name, region, country).\n2. For each returned Springfield:\n   a. Call get_current_weather_tool with city=<city> to fetch detailed current weather (including temperature in °C).\n   b. Call get_live_temp with city=<city> to fetch legacy current temperature (°C).\n   c. Calculate the absolute difference between detailed temperature and legacy temperature. If the difference > 2°C, mark this location as an anomaly and exclude it from further analysis.\n3. For each non-anomalous Springfield, call get_weather_forecast_tool with city=<city> and days=7 to retrieve a 7-day forecast.\n4. For each forecast, compute:\n   - average_daily_temperature = average of high and low temperatures over the 7 days.\n   - max_precipitation_probability = highest day’s precipitation probability.\n5. Selection logic:\n   - Identify all location-day pairs where precipitation probability ≤ 30%.\n   - If one or more pairs exist, choose the pair with the highest average_daily_temperature.\n   - If none ≤ 30%, choose the pair (across all days and locations) with the lowest precipitation probability, regardless of temperature.\n6. Prepare final JSON output containing:\n   {\n     \"chosen_location\": {\"city\":...,\"region\":...,\"country\":...},\n     \"chosen_date\": \"<YYYY-MM-DD>\",\n     \"forecast_summary\": {\"temperature_high\":...,\"temperature_low\":...,\"precipitation_probability\":...,\"humidity\":...},\n     \"anomalies\": [ {\"city\":...,\"region\":...,\"country\":...,\"temp_detailed\":...,\"temp_live\":...,\"difference\":...}, ... ]\n   }\n\nThis task requires using all four tools in a dependent chain and performing cross-validation, filtering, iterative loops, decision branches, and data calculations to arrive at a single optimal solution.",
      "category": "single_server",
      "ground_truth_tool": "Weather Data",
      "servers": [
        "Weather Data"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "FruityVice",
        "Hugging Face",
        "Math MCP",
        "Medical Calculator",
        "Movie Recommender",
        "National Parks",
        "NixOS",
        "OSINT Intelligence"
      ],
      "combination_name": "Single Server: Weather Data"
    },
    {
      "query_id": "weather_data_000",
      "query": "I’m organizing a big outdoor festival and I’ve hit a bit of a snag: every time I check “Springfield” I get a dozen or more possibilities around the world, and the quick temperature readings I see online don’t always match the more detailed reports—sometimes by over two degrees, which makes me uneasy. \n\nWhat I’d really love is your help figuring out which Springfield and which day in the next week would give me the best shot at a warm, mostly dry day—ideally with the chance of rain at or under about 30%. If none of them can stay under that threshold, then just find me the day with the lowest chance of showers, no matter how it ranks on warmth. \n\nAlso, if you notice any of those Springfields where the “fast” temp and the official temp are more than 2 °C apart, just flag them for me so I know which cities to cross off. \n\nIn the end, I need a clear answer: which city, what date, and what the high/low temps, chance of rain and humidity look like that day. Plus a short note on any locations you tossed out because of weird temp mismatches. I’ve got to show my team real numbers, not just guesses, so please back everything up with solid data. Thanks!\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Weather Data",
      "servers": [
        "Weather Data"
      ],
      "distraction_servers": [
        "BioMCP",
        "Call for Papers",
        "FruityVice",
        "Hugging Face",
        "Math MCP",
        "Medical Calculator",
        "Movie Recommender",
        "National Parks",
        "NixOS",
        "OSINT Intelligence"
      ],
      "combination_name": "Single Server: Weather Data"
    },
    {
      "query_id": "weather_data_001",
      "query": "You are planning an outdoor promotional event in “Springfield” next week and need to identify the best days based on weather. Perform the following steps:\n1. Use search_locations_tool with query=\"Springfield\" to get all matching U.S. locations named Springfield.\n2. From the search results, select the Springfield with the largest population (must be >100,000). Record its exact city name as selected_city.\n3. Call get_current_weather_tool for selected_city to fetch detailed current weather (temperature, conditions, humidity, wind).\n4. Call get_live_temp for selected_city to fetch the legacy current temperature. Compare it to the detailed temperature from step 3. If the difference exceeds 2°C, set discrepancy_flag=true, otherwise false.\n5. Request a 3-day forecast via get_weather_forecast_tool(city=selected_city, days=3).\n6. Inspect the 3-day forecast: if more than 1 day has precipitation probability >50%, set extended_forecast_used=true and then fetch a 7-day forecast instead (get_weather_forecast_tool(city=selected_city, days=7)). Otherwise set extended_forecast_used=false and stick with the 3-day data.\n7. From the forecast data in use (3-day or 7-day), identify all days where:\n   • average temperature is between 20°C and 25°C inclusive\n   • precipitation probability is below 30%\n   Compile these into recommended_days, up to a maximum of three days, each with date (relative, e.g., “Day 2”), avg_temp, precipitation_chance, and summary of conditions.\n8. Produce a final JSON report containing:\n   {\n     \"selected_city\": string,\n     \"current_weather\": {temperature, conditions, humidity, wind},\n     \"legacy_temperature\": number,\n     \"temperature_discrepancy\": boolean,\n     \"extended_forecast_used\": boolean,\n     \"forecast_days\": integer,\n     \"forecast_details\": [ …full forecast entries… ],\n     \"recommended_days\": [ …up to 3 day objects… ]\n   }\nAll dates are relative (e.g., Day 1 = tomorrow). Use only the provided tools; do not ask for any additional information.",
      "category": "single_server",
      "ground_truth_tool": "Weather Data",
      "servers": [
        "Weather Data"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Context7",
        "FruityVice",
        "Google Maps",
        "Huge Icons",
        "Math MCP",
        "NASA Data",
        "NixOS",
        "Reddit",
        "Scientific Computing"
      ],
      "combination_name": "Single Server: Weather Data"
    },
    {
      "query_id": "weather_data_001",
      "query": "I’m putting together an outdoor promo in Springfield next week and, to be honest, I’m not even sure which Springfield is the right one—there are so many! I’d like to zero in on the biggest city (somewhere over 100 K folks) and get a clear picture of what’s happening weather-wise right now. Also, if you could grab a quick temperature check and flag it if it’s off by more than a couple of degrees, that’d be great. Then, can you scan the forecast for the next three days and, if more than one day looks too rainy, stretch it out to the full seven-day outlook? What I really need is up to three days that sit around 20–25 °C with less than a 30 percent chance of rain. I need solid numbers and a detailed rundown so I can sell this plan to my boss—with real data, not just vibes.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Weather Data",
      "servers": [
        "Weather Data"
      ],
      "distraction_servers": [
        "Bibliomantic",
        "Context7",
        "FruityVice",
        "Google Maps",
        "Huge Icons",
        "Math MCP",
        "NASA Data",
        "NixOS",
        "Reddit",
        "Scientific Computing"
      ],
      "combination_name": "Single Server: Weather Data"
    },
    {
      "query_id": "time_mcp_000",
      "query": "A global strategy team needs to decide the best 1-hour call slot that maximizes attendance during local business hours (09:00–17:00) in three offices: New York (America/New_York), London (Europe/London), and Tokyo (Asia/Tokyo). They have three candidate UTC slots next week: 09:00 UTC, 15:00 UTC, and 20:00 UTC. \n\nSteps to execute:\n1. Fetch the current local time in each office’s timezone (America/New_York, Europe/London, Asia/Tokyo) to confirm no misconfiguration in timezone identifiers.  \n2. For each UTC candidate slot (\"09:00\", \"15:00\", \"20:00\"), convert that time into each office’s local time.  \n3. Determine for each office whether the converted local time falls within its business hours (09:00–17:00).  \n4. Count how many offices can attend within business hours for each UTC slot.  \n5. Select the UTC slot that yields the highest number of offices in business hours. If two slots tie, pick the earlier UTC slot.  \n6. Provide a summary table in JSON with fields:  \n   • utc_slot  \n   • new_york_time  \n   • london_time  \n   • tokyo_time  \n   • offices_within_business_hours  \n   • recommendation (yes/no for best slot)",
      "category": "single_server",
      "ground_truth_tool": "Time MCP",
      "servers": [
        "Time MCP"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Hugging Face",
        "Medical Calculator",
        "Metropolitan Museum",
        "OKX Exchange",
        "OSINT Intelligence",
        "OpenAPI Explorer"
      ],
      "combination_name": "Single Server: Time MCP"
    },
    {
      "query_id": "time_mcp_000",
      "query": "Hey, I’m trying to schedule a one-hour global strategy call next week with our teams in New York, London and Tokyo. The only windows I’ve got are 09:00 UTC, 15:00 UTC or 20:00 UTC, and I’d love to pick the slot that keeps as many people as possible within their 9 am–5 pm workday. Could you work out what those UTC times look like locally in New York (America/New_York), London (Europe/London) and Tokyo (Asia/Tokyo), count how many offices fall into normal business hours for each option, and then recommend the best slot (going with the earlier one if there’s a tie)? It’d be awesome if you could drop all the details—local times, office counts and the final pick—in a simple JSON snippet, since I really need hard numbers to show my boss, not just guesses.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Time MCP",
      "servers": [
        "Time MCP"
      ],
      "distraction_servers": [
        "Car Price Evaluator",
        "DEX Paprika",
        "FruityVice",
        "Game Trends",
        "Hugging Face",
        "Medical Calculator",
        "Metropolitan Museum",
        "OKX Exchange",
        "OSINT Intelligence",
        "OpenAPI Explorer"
      ],
      "combination_name": "Single Server: Time MCP"
    },
    {
      "query_id": "time_mcp_001",
      "query": "You need to schedule a one-hour meeting during the upcoming week for four offices in different timezones: Los Angeles (America/Los_Angeles), New York (America/New_York), London (Europe/London), and Tokyo (Asia/Tokyo).\n\nRequirements:\n1. Retrieve the current time in Los Angeles.\n2. Determine the next full hour from now that falls within Los Angeles business hours (09:00–17:00). Call this the “candidate start.”\n3. For each candidate start, convert that time to each participant’s local timezone.\n4. Check if the converted time is between 09:00 and 17:00 inclusive for New York, London, and Tokyo offices.\n5. If all four offices have the candidate start within their business hours, finalize this slot and output the meeting schedule. The schedule must list the start and end times (one-hour duration) in each office’s local timezone.\n6. If any office falls outside business hours, increment the candidate start in Los Angeles by one hour. If the incremented time goes past 17:00 in Los Angeles, roll over to the next day at 09:00. Repeat steps 3–5 until you find a slot within the upcoming 7 days.\n7. If no common slot is found within the upcoming week, report that scheduling failed.\n\nExpected Output Format:\n{\n  \"meeting_slot_los_angeles\": {\"start\": \"HH:MM\",\"end\": \"HH:MM\"},\n  \"meeting_slot_new_york\": {\"start\": \"HH:MM\",\"end\": \"HH:MM\"},\n  \"meeting_slot_london\": {\"start\": \"HH:MM\",\"end\": \"HH:MM\"},\n  \"meeting_slot_tokyo\": {\"start\": \"HH:MM\",\"end\": \"HH:MM\"}\n}",
      "category": "single_server",
      "ground_truth_tool": "Time MCP",
      "servers": [
        "Time MCP"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Car Price Evaluator",
        "DEX Paprika",
        "Hugging Face",
        "Medical Calculator",
        "Movie Recommender",
        "NASA Data",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Reddit"
      ],
      "combination_name": "Single Server: Time MCP"
    },
    {
      "query_id": "time_mcp_001",
      "query": "I’m juggling a global team spread across Los Angeles, New York, London and Tokyo, and I need to lock down a one-hour meeting sometime during everyone’s 09:00–17:00 local workday in the upcoming week. Could you start by looking at the next full hour here in LA and then convert that slot into each office’s local time? If any of them fall outside 09:00–17:00, bump it an hour forward in LA and keep checking—rolling over to the next day at 09:00 if we hit 17:00—and keep going until we find a time that works for all four offices within the next seven days. If nothing lines up, just let me know it’s impossible. I really need the exact start and end times for each city so I can send the invites.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Time MCP",
      "servers": [
        "Time MCP"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Car Price Evaluator",
        "DEX Paprika",
        "Hugging Face",
        "Medical Calculator",
        "Movie Recommender",
        "NASA Data",
        "OSINT Intelligence",
        "OpenAPI Explorer",
        "Reddit"
      ],
      "combination_name": "Single Server: Time MCP"
    },
    {
      "query_id": "medical_calculator_000",
      "query": "Conduct an integrated clinical assessment for three patients using the Medical Calculator suite.  \n\nPatient A (Adult Surgical Candidate):  \n• Age: 65 years; Sex: male  \n• Weight: 95 kg; Height: 170 cm (convert to 67 inches)  \n• Serum creatinine (Scr): 1.8 mg/dL; Serum cystatin C (Scys): 1.5 mg/L  \n• Fasting insulin: 20 uIU/mL; Fasting glucose: 150 mg/dL  \n• Serum calcium: 8.0 mg/dL; Albumin: 3.0 g/dL  \n• Measured sodium: 130 mEq/L; Serum glucose: 200 mg/dL  \n• Total cholesterol: 5.2 mmol/L; HDL cholesterol: 1.0 mmol/L  \n• Systolic BP: 150 mmHg; Diastolic BP: 90 mmHg; Heart rate: 80 bpm; QT interval: 380 ms  \n• History: diabetes mellitus (yes), hypertension (yes), congestive heart failure (yes), prior MI (yes), atrial fibrillation (yes), no prior stroke/TIA, non-smoker, on antihypertensive and statin therapy  \n• Hepatic labs: total bilirubin 3.0 mg/dL; albumin 2.5 g/dL; INR 1.8; ascites: slight; encephalopathy grade: 1  \n• Dialysis in last 7 days: no  \n• Current opioids: oxycodone 5 mg every 6 hours (4 doses/day) and fentanyl patch 25 mcg/hr  \n• Chronic steroid: prednisone 10 mg orally daily  \n• Scheduled for elective suprainguinal vascular surgery (high risk)  \n\nPatient B (Pediatric Hypertension Workup):  \n• Age: 12 years 6 months; Sex: female  \n• Weight: 50 kg; Height: 150 cm  \n• Systolic BP: 120 mmHg; Diastolic BP: 80 mmHg  \n• Fasting insulin: 15 uIU/mL; Fasting glucose: 100 mg/dL  \n\nPatient C (Pregnant Wellness Visit):  \n• Age: 30 years; Sex: female; Last menstrual period (LMP): 2024-02-15; Cycle length: 30 days  \n\nRequired outputs (for each patient where applicable):  \n1. BMI and BSA  \n2. Ideal Body Weight (IBW) and Adjusted Body Weight (ABW)  \n3. Maintenance IV fluid rate (4-2-1 rule)  \n4. Cockcroft-Gault creatinine clearance (use ABW if actual weight >120% IBW)  \n5. eGFR (2021 CKD-EPI creatinine formula); if eGFR <60, also run CKD-EPI creatinine-cystatin C equation  \n6. Mean arterial pressure (MAP)  \n7. HOMA-IR score; classify insulin resistance if >2.5 and use to set diabetic flag  \n8. Corrected calcium for hypoalbuminemia  \n9. Corrected sodium for hyperglycemia  \n10. QTc using Bazett’s formula  \n11. CHA₂DS₂-VASc score  \n12. Wells’ PE score  \n13. Revised Cardiac Risk Index  \n14. Framingham 10-year CHD risk  \n15. PREVENT 10-year CVD risk (requires eGFR, SBP, diabetic flag, smoker flag, antihypertensive/statin use)  \n16. Child-Pugh score  \n17. MELD 3.0 score  \n18. Pregnancy due date estimation (EDD, EDC, EGA from LMP)  \n19. Equivalent dose of prednisone 10 mg to hydrocortisone  \n20. Total daily MME for oxycodone and fentanyl patch  \n\nProduce a structured report listing each tool call with input parameters, its result, interpretive classification, and final clinical recommendation per patient.  \n\nUse the Medical Calculator tools in the sequence and conditional logic outlined. No external data sources—only the values and calculators specified above.",
      "category": "single_server",
      "ground_truth_tool": "Medical Calculator",
      "servers": [
        "Medical Calculator"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Context7",
        "FruityVice",
        "Huge Icons",
        "Math MCP",
        "Metropolitan Museum",
        "NASA Data",
        "NixOS",
        "OKX Exchange",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Medical Calculator"
    },
    {
      "query_id": "medical_calculator_000",
      "query": "Hey, I’m gearing up for tomorrow’s multidisciplinary rounds and I’ve got three patients that are driving me nuts with all the numbers. First is Mr. A, a 65-year-old guy who’s about 95 kg and 170 cm (so roughly 67″). He’s diabetic, hypertensive, has CHF, prior MI and AF, and he’s headed for a high-risk suprainguinal vascular case. Labs show creatinine 1.8 mg/dL, cystatin C 1.5 mg/L, fasting insulin 20 µIU/mL, fasting glucose 150 mg/dL (but his electrolytes panel spiked his glucose to 200 mg/dL), calcium 8.0 mg/dL with albumin at 3.0 g/dL, sodium 130 mEq/L, total cholesterol 5.2 mmol/L and HDL 1.0 mmol/L. Vitals are 150/90 mmHg, HR 80, QT interval around 380 ms. He’s on oxycodone 5 mg q6h plus a 25 µg/h fentanyl patch, chronic prednisone 10 mg daily, plus standard antihypertensives and a statin. On top of that, his liver numbers—bilirubin 3.0 mg/dL, albumin 2.5 g/dL, INR 1.8—with slight ascites and grade 1 encephalopathy—have me wondering about his Child-Pugh and MELD.  \n\nThen there’s a 12-year-6-month-old girl, 50 kg, 150 cm, BP around 120/80, fasting insulin 15 µIU/mL, glucose 100 mg/dL.  \n\nAnd finally a 30-year-old pregnant woman who had her LMP on 2024-02-15 with a 30-day cycle.  \n\nI need to pull together their body metrics (BMI, BSA, ideal vs. adjusted weight), IV fluid rates, creatinine clearance vs. eGFR (and switch to the cystatin‐C equation if it’s under 60), MAP, HOMA-IR, corrected calcium and sodium, QTc, CHA₂DS₂-VASc, Wells’ PE probability, RCRI, Framingham and PREVENT 10-year risk, plus that liver scoring and her obstetric dates. Oh, and converting prednisone 10 mg to hydrocortisone and tallying his daily MME. Can you walk me through the actual calculations with those exact numbers and then tell me what you’d recommend for each? I really need hard data—no loose guesses—so I can confidently present to the team.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Medical Calculator",
      "servers": [
        "Medical Calculator"
      ],
      "distraction_servers": [
        "Call for Papers",
        "Context7",
        "FruityVice",
        "Huge Icons",
        "Math MCP",
        "Metropolitan Museum",
        "NASA Data",
        "NixOS",
        "OKX Exchange",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Medical Calculator"
    },
    {
      "query_id": "medical_calculator_001",
      "query": "Perform a comprehensive cardiometabolic and perioperative risk assessment for a 65-year-old male patient (weight 95 kg, height 175 cm) with type 2 diabetes mellitus, hypertension, hyperlipidemia, chronic kidney disease (serum creatinine 1.4 mg/dL, cystatin C 1.0 mg/L), peripheral arterial disease, insulin treatment, current smoker status, on antihypertensive therapy and statins, scheduled for elective hip replacement. The agent must:\n\n1. Calculate BMI and BSA using bmi_bsa_calculator.\n2. Convert height to inches, then compute IBW and ABW via ibw_abw_calculator.\n3. Decide which weight to use for Cockcroft–Gault: if actual weight >1.2×IBW use ABW, otherwise use actual weight; calculate creatinine clearance with crcl_cockcroft_gault.\n4. Compute two eGFRs: one with egfr_epi (scr only) and one with egfr_epi_cr_cys (scr + scys). Compare them; if they differ by more than 5 mL/min/1.73 m² select the lower eGFR, else select the scr-only eGFR for downstream models.\n5. Correct measured sodium (130 mEq/L) for hyperglycemia (serum glucose 280 mg/dL) with corrected_sodium.\n6. Correct serum calcium (8.2 mg/dL) for hypoalbuminemia (albumin 3.3 g/dL) with corrected_calcium.\n7. Calculate mean arterial pressure from SBP 150 mmHg and DBP 95 mmHg using map_calculator.\n8. Compute 10-year cardiovascular event risk using prevent_cvd_risk with age 65, male, TC 5.0 mmol/L, HDL 1.0 mmol/L, SBP 150, diabetes true, smoker true, chosen eGFR, antihypertensive therapy true, statins true.\n9. Assess Revised Cardiac Risk Index for the planned noncardiac surgery with appropriate boolean risk factors.\n10. Calculate CHA₂DS₂-VASc score given atrial fibrillation, hypertension, diabetes, peripheral vascular disease and age components via chads2_vasc_score.\n11. Evaluate insulin resistance by computing HOMA-IR with fasting insulin 20 uIU/mL and fasting glucose 140 mg/dL via homa_ir.\n\nReturn a structured summary of all intermediate results, the decision logic for weight selection and eGFR choice, and final risk estimates.",
      "category": "single_server",
      "ground_truth_tool": "Medical Calculator",
      "servers": [
        "Medical Calculator"
      ],
      "distraction_servers": [
        "BioMCP",
        "Math MCP",
        "Metropolitan Museum",
        "National Parks",
        "OpenAPI Explorer",
        "Paper Search",
        "Reddit",
        "Scientific Computing",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Medical Calculator"
    },
    {
      "query_id": "medical_calculator_001",
      "query": "Hey, I’m looking at a 65-year-old guy who’s booked for an elective hip replacement and I’m a bit overwhelmed by all his numbers. He’s 95 kg, 175 cm tall, type 2 diabetic on insulin, hypertensive on meds, on a statin for high lipids, has chronic kidney disease (creatinine 1.4 mg/dL, cystatin C 1.0 mg/L), peripheral arterial disease, atrial fibrillation—and he still smokes. His latest labs show sodium 130 mEq/L with glucose at 280 mg/dL, calcium 8.2 mg/dL with albumin 3.3 g/dL, blood pressure about 150/95, total cholesterol 5.0 mmol/L and HDL 1.0 mmol/L. \n\nI’m trying to pull together:\n• a sense of his BMI and body surface area  \n• which weight to use for creatinine clearance (actual vs ideal vs adjusted)  \n• whether to trust a creatinine-only eGFR or the one that adds cystatin C (and what to do if they differ)  \n• corrected sodium for his high glucose and corrected calcium for low albumin  \n• his mean arterial pressure  \n• his 10-year cardiovascular event risk given age 65, male, TC 5.0, HDL 1.0, SBP 150, diabetes, smoking, on blood pressure meds and statin  \n• his revised cardiac risk index for non-cardiac surgery  \n• his CHA₂DS₂-VASc with AF, HTN, diabetes, peripheral vascular disease and age  \n• and even an idea of his insulin resistance via HOMA-IR using fasting insulin 20 µIU/mL and fasting glucose 140 mg/dL\n\nCan you walk me through all of that with every calculation, how you decided between weights or eGFRs, and the final risk estimates? I need all the intermediate figures and the reasoning—real numbers, no guesswork—so I can feel confident about the recommendations.\n\nPlease ensure all findings are supported by concrete data and verifiable sources. I need specific numbers and evidence, not generalizations.",
      "category": "single_server",
      "ground_truth_tool": "Medical Calculator",
      "servers": [
        "Medical Calculator"
      ],
      "distraction_servers": [
        "BioMCP",
        "Math MCP",
        "Metropolitan Museum",
        "National Parks",
        "OpenAPI Explorer",
        "Paper Search",
        "Reddit",
        "Scientific Computing",
        "Weather Data",
        "Wikipedia"
      ],
      "combination_name": "Single Server: Medical Calculator"
    }
  ]
}